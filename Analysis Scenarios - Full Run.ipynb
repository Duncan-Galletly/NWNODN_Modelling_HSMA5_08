{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "azdata_cell_guid": "2696d9d4-76d6-4d08-8db1-b1ae507e0023",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "'numpy' is already installed.\n",
                        "'pandas' is already installed.\n",
                        "'matplotlib' is already installed.\n",
                        "'seaborn' is already installed.\n",
                        "'folium' is already installed.\n",
                        "'geopandas' is already installed.\n",
                        "'networkx' is already installed.\n",
                        "'deap' is already installed.\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import importlib\n",
                "import subprocess\n",
                "import gc\n",
                "\n",
                "def install_and_check_libraries(lib_list):\n",
                "    for lib in lib_list:\n",
                "        try:\n",
                "            # Try to import library\n",
                "            importlib.import_module(lib)\n",
                "            print(f\"'{lib}' is already installed.\")\n",
                "        except ImportError:\n",
                "            # If not installed, install library\n",
                "            try:\n",
                "                print(f\">>>> Installing {lib}.\")\n",
                "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", lib])\n",
                "            except Exception as e:\n",
                "                print(f\"Error installing {lib}:\", str(e))\n",
                "\n",
                "libraries = [\"numpy\", \"pandas\", \"matplotlib\", \"seaborn\", \"folium\",\"geopandas\", \"networkx\", \"deap\"]\n",
                "install_and_check_libraries(libraries)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "azdata_cell_guid": "5bf896b3-fd2f-458c-a943-f0ae671530f0",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from deap import base, creator, tools, algorithms\n",
                "from deap.benchmarks.tools import igd\n",
                "from math import factorial\n",
                "import warnings\n",
                "from collections import defaultdict\n",
                "import seaborn as sns\n",
                "import folium\n",
                "import random\n",
                "import geopandas as gpd\n",
                "from geopandas import GeoSeries\n",
                "from functools import partial\n",
                "from statistics import mean\n",
                "from datetime import datetime\n",
                "import requests\n",
                "import itertools\n",
                "from itertools import product\n",
                "import time\n",
                "import os\n",
                "import networkx as nx\n",
                "from branca.element import Figure\n",
                "\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "55165ff9-1679-4df6-9b53-7d0076ee561c"
            },
            "source": [
                "Lets set up some initial parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "azdata_cell_guid": "c6d16bbd-56f7-40fa-acb1-3133ed26f019",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "nsga3 = False\n",
                "weighted_mutation = False\n",
                "restricted_mutation = False\n",
                "restricted_mutation_depth = 10 # nearest x number of sites by travel times\n",
                "elite_pop = 10\n",
                "include_extreme_individual = False\n",
                "include_original_sites = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "activities = pd.read_csv('./Badgernet_Activity_3.csv', encoding='ISO-8859-1')\n",
                "sites = pd.read_csv('./Sites.csv', encoding='ISO-8859-1')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "01648c2c-656a-4e70-bb5c-437f09e2ac04"
            },
            "source": [
                "We'll use a function to turn our financial year into useable dates"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "azdata_cell_guid": "6ff314f0-4412-4503-a91d-da4285360b2e",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "def get_fin_year_dates(financial_year):\n",
                "    start_year_part, end_year_part = financial_year.split('/')\n",
                "\n",
                "    start_year = int(\"20\" + start_year_part)  \n",
                "    end_year = int(\"20\" + end_year_part)\n",
                "\n",
                "    start_date = pd.Timestamp(f\"{start_year}-04-01\")\n",
                "    end_date = pd.Timestamp(f\"{end_year}-03-31\")\n",
                "\n",
                "    return start_date, end_date"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "55e21c36-ce7b-4c2b-b942-3ec03bf1d47f"
            },
            "source": [
                "We need to load our data, firstly our travel times which we have in a pre-calculated table"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "azdata_cell_guid": "f8102f3b-5522-49e7-9add-5020a4443550",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Adding file Missing_travel_times_20231117_153542.csv\n",
                        "Adding file Missing_travel_times_20231117_163729.csv\n",
                        "Adding file Missing_travel_times_20231120_002542.csv\n",
                        "Adding file Missing_travel_times_20231120_083042.csv\n",
                        "Adding file Missing_travel_times_20231120_083043.csv\n",
                        "Adding file Missing_travel_times_20231120_143912.csv\n",
                        "Adding file Missing_travel_times_20231121_025324.csv\n",
                        "Adding file Missing_travel_times_20231121_075202.csv\n",
                        "Adding file Missing_travel_times_20231121_075206.csv\n",
                        "Adding file Missing_travel_times_20231122_132507.csv\n",
                        "Adding file Missing_travel_times_20231122_151104.csv\n",
                        "Adding file Missing_travel_times_20240116_161859.csv\n",
                        "Adding file Missing_travel_times_20240117_094839.csv\n",
                        "Adding file Missing_travel_times_20240117_102403.csv\n",
                        "Adding file Missing_travel_times_20240117_104236.csv\n",
                        "Adding file Missing_travel_times_20240117_124704.csv\n",
                        "Adding file Missing_travel_times_20240118_111610.csv\n",
                        "Adding file Missing_travel_times_20240130_132413.csv\n"
                    ]
                }
            ],
            "source": [
                "# Read in the travel times data\n",
                "travel_times = pd.read_csv('./LSOA_Travel_Times.csv')\n",
                "travel_times = travel_times.dropna()\n",
                "\n",
                "# Initialize an empty DataFrame to hold the new travel times from CSV files\n",
                "new_travel_times_df = pd.DataFrame()\n",
                "\n",
                "directory = \"./\"\n",
                "\n",
                "# Loop through the files in the directory\n",
                "for filename in os.listdir(directory):\n",
                "    if filename.startswith(\"Missing_travel_times\") and filename.endswith(\".csv\"):\n",
                "        # Construct the full file path\n",
                "        file_path = os.path.join(directory, filename)\n",
                "        # Read the CSV file into a DataFrame\n",
                "        current_df = pd.read_csv(file_path)\n",
                "        \n",
                "        print(f\"Adding file {filename}\")\n",
                "        # Append the current DataFrame to the new travel times DataFrame\n",
                "        new_travel_times_df = pd.concat([new_travel_times_df, current_df], ignore_index=True)\n",
                "\n",
                "\n",
                "# Drop any rows with NaN values that may have appeared in the new DataFrame\n",
                "new_travel_times_df = new_travel_times_df.dropna()\n",
                "\n",
                "new_travel_times_df = new_travel_times_df.rename(columns={'Travel_Time': 'TT', 'home_LSOA': 'Home_LSOA'})\n",
                "\n",
                "# Concatenate the existing and new travel times DataFrames\n",
                "combined_travel_times_df = pd.concat([travel_times, new_travel_times_df], ignore_index=True)\n",
                "\n",
                "# Drop duplicates in case some entries are in both DataFrames\n",
                "combined_travel_times_df = combined_travel_times_df.drop_duplicates(subset=['Home_LSOA', 'Site_LSOA'])\n",
                "\n",
                "# Convert the combined DataFrame into a dictionary\n",
                "travel_times_dict = {(row[\"Home_LSOA\"], row[\"Site_LSOA\"]): row[\"TT\"] for _, row in combined_travel_times_df.iterrows()}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "combined_travel_times_df.to_csv('./Data_Output/Full Data/combined_travel_times_df.csv', index=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "8a0ff233-e64a-4c82-a532-07170ec4ec61"
            },
            "source": [
                "Let us load and process our data about our sites"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "azdata_cell_guid": "1122bf65-6cb5-4f89-8e8b-bebf9087191c",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "#remove unnecessary columns\n",
                "sites = sites.loc[:, ['UnitCode', 'LSOA','NICU','LCU','SCBU']]\n",
                "\n",
                "#Apply data cleansing\n",
                "sites = sites.replace('', np.nan)\n",
                "sites = sites.dropna()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "256896f8-b975-4e3f-a923-17a3dce94c57"
            },
            "source": [
                "And our activities data "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "azdata_cell_guid": "0957a236-138a-49ae-a819-d26a2a3e2963",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Data ranges from 2020-11-14 00:00:00 to 2023-03-31 00:00:00\n"
                    ]
                }
            ],
            "source": [
                "# #Remove unecessary data and columns\n",
                "# activities = activities.dropna(subset=['Sustainability_And_Transformation_Partnership'])\n",
                "# values_to_keep = ['QE1', 'QOP', 'QYG']\n",
                "# activities = activities[activities['Sustainability_And_Transformation_Partnership'].isin(values_to_keep)]\n",
                "\n",
                "activities = activities.dropna(subset=['SiteLSOA'])\n",
                "values_to_exclude = ['E01006570']\n",
                "activities = activities[~activities['SiteLSOA'].isin(values_to_exclude)]\n",
                "\n",
                "# activities = activities.dropna(subset=['CC_Level'])\n",
                "# values_to_exclude = ['NICU']\n",
                "# activities = activities[activities['CC_Level'].isin(values_to_exclude)]\n",
                "\n",
                "activities_orig = activities.loc[:, ['Der_Postcode_LSOA_Code','CC_Activity_Date','SiteLSOA', 'CC_Level']]\n",
                "activities = activities.loc[:, ['Der_Postcode_LSOA_Code','CC_Activity_Date','SiteLSOA', 'CC_Level']]\n",
                "\n",
                "#Apply data cleansing\n",
                "activities = activities.replace('', np.nan)\n",
                "activities = activities.dropna()\n",
                "\n",
                "# Ensure the date is a date\n",
                "activities['CC_Activity_Date'] = pd.to_datetime(activities['CC_Activity_Date'], format='%d/%m/%Y')\n",
                "activities_indexed = activities.set_index('Der_Postcode_LSOA_Code')\n",
                "\n",
                "# time_periods = pd.date_range(start_date, end_date, freq='D')\n",
                "\n",
                "int_to_activity = {i: activity for i, activity in enumerate(activities['CC_Level'].unique())}\n",
                "\n",
                "home_lsoas = []\n",
                "most_frequent_sites = []\n",
                "home_activities = []\n",
                "home_populations = []\n",
                "all_sites = []\n",
                "\n",
                "def data_prep(activities, start_date, end_date):\n",
                "    filtered_activities = activities.loc[(activities['CC_Activity_Date'] >= start_date) & (activities['CC_Activity_Date'] <= end_date)]\n",
                "    filtered_activities = filtered_activities.set_index('Der_Postcode_LSOA_Code')\n",
                "    home_lsoas = sorted(filtered_activities.index.unique().tolist())\n",
                "    num_homes = len(home_lsoas)\n",
                "    num_sites = len(site_codes)# Group by DER_Postcode_LSOA_Code and count the occurrences\n",
                "    home_populations_dict = filtered_activities.groupby('Der_Postcode_LSOA_Code').size().to_dict()\n",
                "    home_activities_dict = filtered_activities.groupby('Der_Postcode_LSOA_Code')['CC_Level'].value_counts().unstack(fill_value=0).to_dict(orient='index')\n",
                "    home_activities = [[home_activities_dict[home][int_to_activity[i]] for i in range(3)] for home in home_lsoas]\n",
                "    # Convert it to list matching the order of home_lsoas\n",
                "    home_populations = [home_populations_dict.get(home, 0) for home in home_lsoas]\n",
                "    site_frequencies = filtered_activities.groupby(['Der_Postcode_LSOA_Code', 'SiteLSOA']).size().reset_index(name='counts')\n",
                "    most_frequent_sites = site_frequencies.loc[site_frequencies.groupby('Der_Postcode_LSOA_Code')['counts'].idxmax()]\n",
                "    return filtered_activities, num_homes, num_sites, most_frequent_sites, home_lsoas, home_activities, home_populations\n",
                "\n",
                "def data_prep(activities, start_date, end_date, site_codes, int_to_activity):\n",
                "    # Filtering activities within the date range\n",
                "    filtered_activities = activities.loc[(activities['CC_Activity_Date'] >= start_date) & (activities['CC_Activity_Date'] <= end_date)]\n",
                "    # Setting the index to 'Der_Postcode_LSOA_Code'\n",
                "    filtered_activities = filtered_activities.set_index('Der_Postcode_LSOA_Code')\n",
                "    # Getting unique LSOA codes\n",
                "    home_lsoas = sorted(filtered_activities.index.unique().tolist())\n",
                "    # Calculating the number of homes and sites\n",
                "    num_homes = len(home_lsoas)\n",
                "    num_sites = len(site_codes)\n",
                "    # Grouping by LSOA code to get populations\n",
                "    home_populations_dict = filtered_activities.groupby('Der_Postcode_LSOA_Code').size().to_dict()\n",
                "    # Grouping by LSOA code and CC_Level to get activities\n",
                "    home_activities_dict = filtered_activities.groupby('Der_Postcode_LSOA_Code')['CC_Level'].value_counts().unstack(fill_value=0).to_dict(orient='index')\n",
                "    # Dynamically determining the range based on unique CC_Level values\n",
                "    unique_cc_levels = sorted(filtered_activities['CC_Level'].unique())\n",
                "    home_activities = [[home_activities_dict[home].get(int_to_activity[i], 0) for i in range(len(unique_cc_levels))] for home in home_lsoas]\n",
                "    # Getting populations matching the order of home_lsoas\n",
                "    home_populations = [home_populations_dict.get(home, 0) for home in home_lsoas]\n",
                "    # Calculating site frequencies\n",
                "    site_frequencies = filtered_activities.groupby(['Der_Postcode_LSOA_Code', 'SiteLSOA']).size().reset_index(name='counts')\n",
                "    most_frequent_sites = site_frequencies.loc[site_frequencies.groupby('Der_Postcode_LSOA_Code')['counts'].idxmax()]\n",
                "    return filtered_activities, num_homes, num_sites, most_frequent_sites, home_lsoas, home_activities, home_populations\n",
                "\n",
                "\n",
                "#Add site code to our df\n",
                "activities = pd.merge(activities, sites[['LSOA','UnitCode']], left_on='SiteLSOA', right_on='LSOA', how='left')\n",
                "activities = activities.drop('LSOA', axis=1)\n",
                "activities.rename(columns={'UnitCode': 'SiteCode'}, inplace=True)\n",
                "\n",
                "\n",
                "# Make a list of all our homes and sites\n",
                "site_codes = sites['LSOA'].unique().tolist()\n",
                "home_codes =  activities_indexed.index.unique().tolist()\n",
                "\n",
                "# print (f\"filtered_activities row count: {len(filtered_activities)}\")\n",
                "endrange = activities['CC_Activity_Date'].max()\n",
                "startrange = activities['CC_Activity_Date'].min()\n",
                "\n",
                "print(f\"Data ranges from {startrange} to {endrange}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "80cc050c-c9d4-411b-9a8e-384273b06833"
            },
            "source": [
                "We also want to look up any travel times that might be missing from our data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "azdata_cell_guid": "b3c2944d-3cd3-4f05-8423-c366c80a4303",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "class OutOfAPICallsException(Exception):\n",
                "    \"\"\"Exception raised when the API returns a 403 status code indicating the quota has been exceeded.\"\"\"\n",
                "    pass\n",
                "\n",
                "class NoSuchLocationException(Exception):\n",
                "    \"\"\"Exception raised when the API returns a 404 status code indicating the location hasnt been found.\"\"\"\n",
                "    pass \n",
                "\n",
                "class RateLimitException(Exception):\n",
                "    \"\"\"Exception raised when the API returns a 429 status code indicating too many requests.\"\"\"\n",
                "    pass\n",
                "\n",
                "\n",
                "def calculate_travel_time_openrouteservice(api_key, start_coords, end_coords, api_request_no, transport_mode='driving-car'):\n",
                "    \"\"\" Calculate travel time using the Openrouteservice API. \"\"\"\n",
                "    \n",
                "    url = \"https://api.openrouteservice.org/v2/directions/{}/geojson\".format(transport_mode)\n",
                "    \n",
                "    # Set up the headers with the API key\n",
                "    headers = {\n",
                "        'Authorization': api_key,\n",
                "        'Content-Type': 'application/json'\n",
                "    }\n",
                "    \n",
                "    # Set up the parameters with the start and end coordinates\n",
                "    body = {\n",
                "        'coordinates': [start_coords, end_coords]\n",
                "    }\n",
                "    \n",
                "    # Make the request \n",
                "    response = requests.post(url, headers=headers, json=body)\n",
                "    \n",
                "    # Check response\n",
                "    if response.status_code == 200:\n",
                "        # Parse the response\n",
                "        directions = response.json()\n",
                "        try:\n",
                "            # Travel time in seconds is nested in the 'features' list, under the 'properties' dictionary\n",
                "            duration_seconds = directions['features'][0]['properties']['segments'][0]['duration']\n",
                "            return duration_seconds\n",
                "        except (IndexError, KeyError):\n",
                "            print(\"Error parsing the response.\")\n",
                "            return None\n",
                "    elif response.status_code == 403:  # Out of API calls\n",
                "        print(f\"API request {api_request_no} failed with status code {response.status_code}\")\n",
                "        raise OutOfAPICallsException(\"API quota exceeded\")\n",
                "    elif response.status_code == 404:  # Out of API calls\n",
                "        print(f\"API request {api_request_no} failed with location not found {response.status_code}\")\n",
                "        raise NoSuchLocationException(\"No location found\")\n",
                "    elif response.status_code == 429:  # Rate limited by the API\n",
                "        print(f\"API request {api_request_no} has been rate-limited with status code {response.status_code}\")\n",
                "        raise RateLimitException(\"Rate limit exceeded\")\n",
                "    else:\n",
                "        print(f\"API request {api_request_no} failed with status code {response.status_code}\")\n",
                "        return None\n",
                "\n",
                "# Example usage\n",
                "api_key = '5b3ce3597851110001cf62486f4bed53db4c47b7a841e3da98655493'\n",
                "start_coordinates = (8.681495, 49.41461)  # Example coordinates (longitude, latitude)\n",
                "end_coordinates = (8.687872, 49.420318)  # Example coordinates (longitude, latitude)\n",
                "transport_mode = 'driving-car'  # Mode of transportation\n",
                "\n",
                "# Calculate travel time\n",
                "# travel_time_seconds = calculate_travel_time_openrouteservice(api_key, start_coordinates, end_coordinates, transport_mode)\n",
                "# print(f\"Estimated travel time: {travel_time_seconds / 60:.2f} minutes\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "azdata_cell_guid": "3bc1f1a6-d6d6-46c0-8866-c63cf0fcb32c",
                "language": "python"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "LSOA_LL_df = pd.read_csv('./LSOA_to_LL.csv')\n",
                "\n",
                "# Create the Cartesian product of home_codes and site_codes\n",
                "combination_product = list(product(home_codes, site_codes + ['E01012632']))\n",
                "\n",
                "# List to store lat/lng details\n",
                "lat_lng_details = list()\n",
                "\n",
                "LSOA_LL_df\n",
                "\n",
                "# Loop through each combination\n",
                "for home, site in combination_product:\n",
                "    # Check if we have the travel time for this home and site\n",
                "    if (home, site) not in travel_times_dict and home != 'M99999999':\n",
                "        # Filter the DataFrame for the home and check if it's empty\n",
                "        home_rows = LSOA_LL_df[LSOA_LL_df['LSOA'] == home][['Latitude_1m', 'Longitude_1m']]\n",
                "        if not home_rows.empty:\n",
                "            home_lat_lng = home_rows.iloc[0]\n",
                "        else:\n",
                "            # Handle the case where no match is found, for example by continuing to the next iteration\n",
                "            continue\n",
                "\n",
                "        # Filter the DataFrame for the site and check if it's empty\n",
                "        site_rows = LSOA_LL_df[LSOA_LL_df['LSOA'] == site][['Latitude_1m', 'Longitude_1m']]\n",
                "        if not site_rows.empty:\n",
                "            site_lat_lng = site_rows.iloc[0]\n",
                "        else:\n",
                "            # Handle the case where no match is found, for example by continuing to the next iteration\n",
                "            continue\n",
                "        \n",
                "        # Store the details in a dictionary\n",
                "        lat_lng_detail = {\n",
                "            'home_code': home,\n",
                "            'home_latitude': home_lat_lng['Latitude_1m'],\n",
                "            'home_longitude': home_lat_lng['Longitude_1m'],\n",
                "            'site_code': site,\n",
                "            'site_latitude': site_lat_lng['Latitude_1m'],\n",
                "            'site_longitude': site_lat_lng['Longitude_1m']\n",
                "        }\n",
                "        \n",
                "        # Add the dictionary to our list\n",
                "        lat_lng_details.append(lat_lng_detail)\n",
                "        \n",
                "len(lat_lng_details)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "azdata_cell_guid": "820ca576-5028-41b6-903b-c3ab7a1cf4c8",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "# Initialize an empty DataFrame to store home_LSOA, Site_LSOA, and Travel Time\n",
                "missing_travel_times_df = pd.DataFrame(columns=['home_LSOA', 'Site_LSOA', 'Travel_Time'])\n",
                "\n",
                "# API rate limiting parameters\n",
                "api_request_count = 0\n",
                "api_limit = 2000\n",
                "api_per_minute_limit = 40  # Adjust to per-minute limit\n",
                "delay_between_requests = 60 / api_per_minute_limit  # Delay to adhere to per-minute limit\n",
                "\n",
                "not_found_details = []\n",
                "\n",
                "max_retries = 5\n",
                "\n",
                "# Function to handle API requests and retries\n",
                "def fetch_travel_time(detail, max_retries):\n",
                "    retries = 0\n",
                "    while retries < max_retries:\n",
                "        try:\n",
                "            travel_time_seconds = calculate_travel_time_openrouteservice(\n",
                "                api_key, \n",
                "                (detail['home_longitude'], detail['home_latitude']), \n",
                "                (detail['site_longitude'], detail['site_latitude']), \n",
                "                api_request_count, \n",
                "                transport_mode\n",
                "            )\n",
                "\n",
                "            if travel_time_seconds is not None:\n",
                "                return round(travel_time_seconds / 60, 1)  # Convert to minutes\n",
                "\n",
                "        except OutOfAPICallsException:\n",
                "            # If out of API calls, halt further processing\n",
                "            print(\"API quota exceeded. Halting process.\")\n",
                "            return \"quota_exceeded\"\n",
                "        except NoSuchLocationException:\n",
                "            # Log and break for locations not found\n",
                "            not_found_details.append((detail['home_code'], detail['site_code']))\n",
                "            print(f\"Location not found for {detail['home_code']} - {detail['site_code']}\")\n",
                "            break\n",
                "        except RateLimitException:\n",
                "            # If rate limit is hit, wait and retry\n",
                "            time.sleep(delay_between_requests)\n",
                "        except Exception as e:\n",
                "            # Log unexpected exceptions and retry\n",
                "            print(f\"An unexpected exception occurred: {e}\")\n",
                "\n",
                "        retries += 1\n",
                "        if retries < max_retries:\n",
                "            print(f\"Retrying request for {detail['home_code']} to {detail['site_code']}. Attempt {retries}/{max_retries}\")\n",
                "            time.sleep(delay_between_requests)\n",
                "\n",
                "    # Return None if unsuccessful after all retries\n",
                "    return None\n",
                "\n",
                "# List to collect results\n",
                "results = []\n",
                "\n",
                "# Loop through each home-site pair\n",
                "for detail in lat_lng_details:\n",
                "    if api_request_count >= api_limit:\n",
                "        print(\"Stopped due to API quota being exceeded.\")\n",
                "        break\n",
                "\n",
                "    # Fetch travel time with retries\n",
                "    travel_time_minutes = fetch_travel_time(detail, max_retries)\n",
                "\n",
                "    if travel_time_minutes is not None and travel_time_minutes != \"quota_exceeded\":\n",
                "        # Append successful results\n",
                "        results.append({\n",
                "            'home_LSOA': detail['home_code'],\n",
                "            'Site_LSOA': detail['site_code'],\n",
                "            'Travel_Time': travel_time_minutes\n",
                "        })\n",
                "        api_request_count += 1\n",
                "\n",
                "    if travel_time_minutes == \"quota_exceeded\":\n",
                "        # Break the loop if API quota is exceeded\n",
                "        break\n",
                "\n",
                "    if travel_time_minutes is None and isinstance(travel_time_minutes, OutOfAPICallsException):\n",
                "        # If out of API calls, break from the loop\n",
                "        break\n",
                "    \n",
                "    print(f\"Home: {detail['home_code']} Site: {detail['site_code']} >>> {travel_time_minutes} minutes\")\n",
                "    \n",
                "    time.sleep(delay_between_requests)\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "azdata_cell_guid": "4007c1df-336b-4b1a-a63d-24cda74868b4",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Empty DataFrame\n",
                        "Columns: []\n",
                        "Index: []\n"
                    ]
                }
            ],
            "source": [
                "# Convert results to DataFrame\n",
                "missing_travel_times_df = pd.DataFrame(results)\n",
                "\n",
                "print(missing_travel_times_df)\n",
                "\n",
                "def export_travel_times(df):\n",
                "    if len(df) > 0:\n",
                "        now = datetime.now()\n",
                "        timestamp = now.strftime(\"%Y%m%d_%H%M%S\") \n",
                "        # Specify the file name\n",
                "        log_name = f\"./Missing_travel_times_{timestamp}.csv\"\n",
                "        # Save the DataFrame to CSV\n",
                "        df.to_csv(log_name, index=False)\n",
                "        print(f\"Exported file {log_name}\")\n",
                "    \n",
                "# Call the export function\n",
                "export_travel_times(missing_travel_times_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "f427e402-bff6-4990-bdad-7e5aecfb2d9c"
            },
            "source": [
                "Load and organise our geographic information "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "azdata_cell_guid": "1f9613b3-cc6d-4dfe-8bd1-c160cf8dd419",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "# Load the LSOA shape file\n",
                "lsoas = gpd.read_file('./LSOA_Dec_2011_PWC_in_England_and_Wales/LSOA_Dec_2011_PWC_in_England_and_Wales.shp')\n",
                "\n",
                "# Make a sites GeoDF\n",
                "sites_geo_df = lsoas[lsoas['lsoa11cd'].isin(site_codes)]\n",
                "sites_geo_df = sites_geo_df.set_index('lsoa11cd')\n",
                "sites_geo_df['centroid'] = sites_geo_df.geometry.centroid\n",
                "\n",
                "# Make a homes GeoDF\n",
                "homes_geo_df = lsoas[lsoas['lsoa11cd'].isin(home_codes)]\n",
                "homes_geo_df = homes_geo_df.set_index('lsoa11cd')\n",
                "homes_geo_df['centroid'] = homes_geo_df.geometry.centroid\n",
                "\n",
                "# Extract the centroids as a GeoSeries\n",
                "homes_centroids = GeoSeries(homes_geo_df['centroid'])\n",
                "sites_centroids = GeoSeries(sites_geo_df['centroid'])\n",
                "\n",
                "# Set the CRS of the centroids to EPSG:27700\n",
                "homes_centroids.crs = \"EPSG:27700\"\n",
                "sites_centroids.crs = \"EPSG:27700\"\n",
                "\n",
                "# Convert the centroids to EPSG:4326 (latitude and longitude)\n",
                "homes_centroids_ll = homes_centroids.to_crs(epsg=4326)\n",
                "sites_centroids_ll = sites_centroids.to_crs(epsg=4326)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "8bcd0047-aa03-483a-873b-29a6e310dd5d"
            },
            "source": [
                "And we can then plot the assignments on a map"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "azdata_cell_guid": "0f93c526-b945-4ec7-98e5-39ec37f9cfa3",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "current_site_list = []\n",
                "\n",
                "def plot_assignments_folium(individual):\n",
                "\n",
                "    # Make a homes GeoDF\n",
                "    homes_geo_df = lsoas[lsoas['lsoa11cd'].isin(home_codes)]\n",
                "    homes_geo_df = homes_geo_df.set_index('lsoa11cd')\n",
                "    homes_geo_df['centroid'] = homes_geo_df.geometry.centroid\n",
                "\n",
                "    sites_geo_df = lsoas[lsoas['lsoa11cd'].isin(current_site_list)]\n",
                "    sites_geo_df = sites_geo_df.set_index('lsoa11cd')\n",
                "    sites_geo_df['centroid'] = sites_geo_df.geometry.centroid\n",
                "\n",
                "    # Convert centroids to latitude and longitude\n",
                "    homes_centroids_ll = homes_geo_df['centroid'].to_crs(epsg=4326)\n",
                "    sites_centroids_ll = sites_geo_df['centroid'].to_crs(epsg=4326)\n",
                "    \n",
                "    # Calculate the mean latitude and longitude\n",
                "    center_latitude = homes_centroids_ll.apply(lambda p: p.y).mean()\n",
                "    center_longitude = homes_centroids_ll.apply(lambda p: p.x).mean()\n",
                "\n",
                "    # Create a map centered at the mean coordinates\n",
                "\n",
                "    m = folium.Map(location=[center_latitude, center_longitude], zoom_start=7, width='100%', height='100%')\n",
                "\n",
                "    \n",
                "    # Add the site locations as blue markers\n",
                "    for point in sites_centroids_ll:\n",
                "        folium.Marker([point.y, point.x], icon=folium.Icon(color=\"blue\")).add_to(m)\n",
                "\n",
                "    # Add the home locations as small red markers\n",
                "    for point in homes_centroids_ll:\n",
                "        folium.CircleMarker([point.y, point.x], radius=2, color=\"red\").add_to(m)\n",
                "\n",
                "    # Plot lines from home to site\n",
                "    for home_idx, site_idx in enumerate(individual):\n",
                "        home_code = home_lsoas[home_idx]\n",
                "        site_code = current_site_list[site_idx]\n",
                "        \n",
                "        if home_code in homes_geo_df.index and site_code in sites_geo_df.index:\n",
                "            home_point = homes_centroids_ll.loc[home_code]\n",
                "            site_point = sites_centroids_ll.loc[site_code]\n",
                "            coords = [[home_point.y, home_point.x], [site_point.y, site_point.x]]\n",
                "            folium.PolyLine(coords, color=\"grey\", weight=1, opacity=0.8).add_to(m)\n",
                "            \n",
                "    fig = Figure(width=600, height=400)\n",
                "    fig.add_child(m)  \n",
                "         \n",
                "    return fig\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "azdata_cell_guid": "7a2a9c14-7da0-4ce1-855d-2207f9721964",
                "language": "python"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "Ranking\n",
                            "1.0     66.012695\n",
                            "2.0     14.918953\n",
                            "3.0      5.572313\n",
                            "4.0      4.450156\n",
                            "5.0      2.222455\n",
                            "6.0      1.578538\n",
                            "7.0      1.171554\n",
                            "8.0      0.504905\n",
                            "9.0      0.596269\n",
                            "10.0     0.430590\n",
                            "11.0     0.358461\n",
                            "12.0     0.228191\n",
                            "13.0     0.098795\n",
                            "14.0     0.396055\n",
                            "15.0     0.606760\n",
                            "16.0     0.375946\n",
                            "17.0     0.168302\n",
                            "18.0     0.164804\n",
                            "19.0     0.034972\n",
                            "20.0     0.037157\n",
                            "21.0     0.072129\n",
                            "Name: count, dtype: float64"
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# filtered_activities = pd.DataFrame()\n",
                "# sorted_sites = {}\n",
                "\n",
                "activities_to_rank = activities.copy()\n",
                "activities_to_rank.set_index('Der_Postcode_LSOA_Code', inplace=True)\n",
                "home_lsoa_codes = sorted(activities_to_rank.index.unique().tolist())\n",
                "\n",
                "def get_sites(home, num_sites=None):\n",
                "    # If num_sites is None, return all sites\n",
                "    num_sites = len(site_codes)\n",
                "    # Returns a list of Site_LSOA codes sorted by distance (nearest first)\n",
                "    sorted_sites = sorted(site_codes, key=lambda site: travel_times_dict.get((home, site), float('inf')))\n",
                "    return sorted_sites[:num_sites]\n",
                "\n",
                "def calculate_percentages(data):\n",
                "    ranking_counts = data['Ranking'].value_counts()\n",
                "    total_counts = data['Ranking'].count()  # Count only non-null rankings\n",
                "    percentages = (ranking_counts / total_counts) * 100\n",
                "    return percentages.sort_index()\n",
                "\n",
                "sorted_sites_by_distance = {home: get_sites(home) for home in home_lsoa_codes}\n",
                "\n",
                "def optimized_determine_site_ranking(data, sorted_sites_by_distance):\n",
                "    # Assuming 'SiteLSOA' is a column in 'data' and 'home_lsoa_codes' is a list of unique LSOAs\n",
                "    # Update 'Ranking' column based on pre-computed 'sorted_sites_by_distance'\n",
                "    for home_lsoa in home_lsoa_codes:\n",
                "        sites_list = sorted_sites_by_distance.get(home_lsoa, [])\n",
                "        # Use a vectorized operation or apply to update rankings\n",
                "        data.loc[data.index == home_lsoa, 'Ranking'] = data[data.index == home_lsoa]['SiteLSOA'].apply(lambda x: sites_list.index(x) + 1 if x in sites_list else None)\n",
                "\n",
                "optimized_determine_site_ranking(activities_to_rank, sorted_sites_by_distance)\n",
                "percentages = calculate_percentages(activities_to_rank)\n",
                "\n",
                "percentages \n",
                "\n",
                "# 1     65.824697\n",
                "# 2     14.283368\n",
                "# 3      6.887122\n",
                "# 4      3.835668\n",
                "# 5      2.270525\n",
                "# 6      2.157204\n",
                "# 7      0.976367\n",
                "# 8      0.979651\n",
                "# 9      0.283302\n",
                "# 10     0.194616\n",
                "# 11     0.463138\n",
                "# 12     0.156843\n",
                "# 13     0.068978\n",
                "# 14     0.394981\n",
                "# 15     0.918885\n",
                "# 16     0.128923\n",
                "# 17     0.135492\n",
                "# 18     0.000821\n",
                "# 19     0.003285\n",
                "# 20     0.026277\n",
                "# 22     0.009854"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[0.6601269474899017,\n",
                            " 0.8093164769448669,\n",
                            " 0.8650396055185436,\n",
                            " 0.9095411705048172,\n",
                            " 0.9317657241777263,\n",
                            " 0.9475511024847434,\n",
                            " 0.9592666421864343,\n",
                            " 0.9643156900802601,\n",
                            " 0.9702783752120162,\n",
                            " 0.9745842731993914,\n",
                            " 0.9781688786305058,\n",
                            " 0.9804507859903128,\n",
                            " 0.9814387382188882,\n",
                            " 0.9853992900732658,\n",
                            " 0.9914668904859325,\n",
                            " 0.9952263547185647,\n",
                            " 0.9969093706831733,\n",
                            " 0.9985574148874783,\n",
                            " 0.9989071324905139,\n",
                            " 0.9992787074437391,\n",
                            " 1.0]"
                        ]
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "def calculate_cumulative_probabilities(probabilities):\n",
                "    total = sum(probabilities)\n",
                "    normalized_probs = [p / total for p in probabilities]\n",
                "    cumulative_probs = []\n",
                "    cumulative_sum = 0\n",
                "    for p in normalized_probs:\n",
                "        cumulative_sum += p\n",
                "        cumulative_probs.append(cumulative_sum)\n",
                "    return cumulative_probs\n",
                "\n",
                "cumulative_probs = calculate_cumulative_probabilities(percentages)\n",
                "\n",
                "cumulative_probs"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "8e0c380f-b2d3-44a0-aecc-9787a72e575b"
            },
            "source": [
                "So a simple nearest assignment has distinct limitations and does not allow us to balance any other competing priorities. \n",
                "\n",
                "This kind of problem is a variant of the travelling salesman problem (https://en.wikipedia.org/wiki/Travelling_salesman_problem) which is NP-Hard meaning that it is too computationally expensive to compute all possible solutions and find the best one. Therefore needs to be approached and solved using a heuristic approach.\n",
                "\n",
                "For this purpose we are going to use a genetic algorithm to enable us to balance competing priorities and come up with a balanced good solution. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "0de44856-939a-4918-94ce-0d71bd197c55"
            },
            "source": [
                "We will need to define the parameters for our genetic algorithm\n",
                "\n",
                "    * Population Size\n",
                "    * Chance to cross breed\n",
                "    * Mutation Probabilities\n",
                "    * Max number of generations to run for\n",
                "\n",
                "The base mutation rate probability is adaptive and increased based on the stagnation of both the size and diversity of the pareto front, but we can also set the probability that elements of an individual will be mutated once the individual has been selected for mutation.\n",
                "\n",
                "Cessation of the process is also controlled by stagnation in the pareto front, once the mutation rate has been increased and yet still no improvements have been realised in both size and diversity of the pareto front for a number of generations then the evolution is stopped "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "azdata_cell_guid": "8f160298-8950-4f30-8a05-ceccf525f15b",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "# Let us set up sone of the parameters for the evolution of our solution\n",
                "# number of solutions in a population\n",
                "pop_num = 200\n",
                "# percentage chance to cross breed one solution with another\n",
                "cross_chance = 0.3\n",
                "# percentage chance to introduce random mutations into the solutions, % of selected individuals\n",
                "initial_mutation_prob = 0.05\n",
                "# maximum percentage chance to introduce random mutations into the solutions, % of selected individuals\n",
                "max_mutation_prob = 0.8\n",
                "# percentage chance to introduce random mutations into the individuals selected for mutation\n",
                "individual_mutation_prob = 0.2\n",
                "individual_mutation_max_prob = 0.8"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "5cd8bbf2-765a-43bb-a395-49ab003a8dfb"
            },
            "source": [
                "Now we will set up and run our evolutionary algorithm. The most important part is the custom evaluation function. Most of the population, generations, breeding and mutating is handled by the DEAP library, but we need to define our own custom function to assess the fitness of each solution. These scores are then used to find the best individual solutions in each generation to breed off and mutate in later generations to evelove the population towards a 'good' solution to our problem with competing priorities."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "95636d72-1499-44be-8232-9f86bde3dff8"
            },
            "source": [
                "Lets add all our competing priorities in to our evaluation function as per the source paper: https://www.journalslibrary.nihr.ac.uk/hsdr/hsdr06350/#/abstract\n",
                "\n",
                "We want to:\n",
                "\n",
                "    * Minimise the average travel time\n",
                "    * Maximise the proportion within 30 minutes\n",
                "    * Minimise the maximum distance for any assignment\n",
                "    * Maximise the number taking place in units with more than x admissions per year\n",
                "    * Maximise the smallest number of admissions per year  \n",
                "    * Minimise the largest number of admissions per year \n",
                "    * Maximise the proportion within 30 minutes and in units with more than x admissions per year\n",
                "\n",
                "The fourth and final of these are different in this approach as we are not working with admissions data but with critical care information, what we will model instead here is whether a NICU, LNU, and SCBU site meets the minimum required number of days as set out in the BAPM standards https://hubble-live-assets.s3.amazonaws.com/bapm/file_asset/file/1494/BAPM_Service_Quality_Standards_FINAL.pdf and we will look at the proiportion of activities taking place in the nicu sites as a general positive given these sites are the most specialised.\n",
                "\n",
                "So we have:\n",
                "\n",
                "    * Minimise the average travel time\n",
                "    * Maximise the proportion within 30 minutes\n",
                "    * Minimise the maximum distance for any assignment\n",
                "    * Maximise the number taking place in level 3 nicu units\n",
                "    * Maximise the smallest number of admissions per year  \n",
                "    * Minimise the largest number of admissions per year \n",
                "    * Maximise the proportion within 30 minutes and in in level 3 nicu units\n",
                "\n",
                "We can also adjust the weightings that we give to each of these should we want to."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "azdata_cell_guid": "19c2f4fd-7669-4b2a-be7e-50913684d62c",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "# Let us set up variables for the weightings\n",
                "min_travel_time         = -1.0\n",
                "max_in_30               = 1.0\n",
                "min_max_distance        = -1.0\n",
                "max_large_unit          = 1.0\n",
                "max_min_no              = 1.0\n",
                "min_max_no              = -1.0\n",
                "max_in_30_and_large     = 1.0\n",
                " # the following are a helper metric to aid the maximisation of activities taking place in larger units\n",
                "max_large_nicu          = 1.0 \n",
                "consolidation           = 1.0\n",
                "\n",
                "# Define the threshold for minimum admissions\n",
                "nicu_activities_threshold = 1000  # set to 1000 to make the algorithm reach the threshold of over lnu range and insentivise those solutions\n",
                "lnu_activities_threshold = 1000  \n",
                "scbu_activities_threshold = 500\n",
                "\n",
                "# Using this we can provide objectives to our evolutionary process\n",
                "# must be structured like this {\n",
                "#     'E01024897': {'NICU': {'min': 0, 'max': 500}}\n",
                "#     ,'E01005062': {'NICU': {'min': 4000}}\n",
                "#     }\n",
                "# can provide both minimums, maximums to any existing site and any activity level\n",
                "\n",
                "activity_limits = set()\n",
                "\n",
                "# Sites that should not be assigned to any home, for modelling full site closures\n",
                "restricted_sites = set()\n",
                "\n",
                "# Do we want to propose a new site, we can add the LSOA of the proposed site and run our process against it\n",
                "# E01012632 would be blackburn hospital\n",
                "proposed_additions = list()\n",
                "\n",
                "# Activity to focus on in the evolutionary assignment\n",
                "activity_focus = list()\n",
                "\n",
                "# We can also add an extreme individual to the population this is to ensure that the population space contains \n",
                "# the most optimal fitness for one of our evaluation metrics.. in this case the minimisation of travel time\n",
                "include_original_sites = False\n",
                "\n",
                "# Number of elite individuals to carry to the next generation\n",
                "num_elites = elite_pop\n",
                "\n",
                "# normalisation boundaries, these are based on known results, these could need further evaluation\n",
                "min_avg_time = 10\n",
                "max_avg_time = 70\n",
                "min_prop_within_30_mins = 0.1\n",
                "max_prop_within_30_mins = 0.9\n",
                "min_min_max_distance = 200\n",
                "max_min_max_distance = 350\n",
                "min_number_of_sites_over_nicu_threshold = 0.0\n",
                "max_number_of_sites_over_nicu_threshold = 0.4\n",
                "min_smallest_site = 900 \n",
                "max_smallest_site = 5000\n",
                "min_largest_site = 6000 \n",
                "max_largest_site = 13000\n",
                "min_constraint_adherence = 0 \n",
                "max_constraint_adherence = 3000\n",
                "min_prop_within_30_mins_and_large_NICU = 0.05 \n",
                "max_prop_within_30_mins_and_large_NICU = 0.20\n",
                "min_max_large_nicu = 1000\n",
                "max_max_large_nicu = 6000\n",
                "min_consolidation_metric = 100\n",
                "max_consolidation_metric = 1300"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "412297cc-e16c-4f8d-bd0a-4cc98d3de544"
            },
            "source": [
                "Let us add these priorities in to our evaluation function algorithm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {
                "azdata_cell_guid": "8f63c30e-3d13-40cb-a990-a14891860aa0",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "creator.create(\"FitnessMulti\", base.Fitness, weights=(min_travel_time\n",
                "                                                      , max_in_30\n",
                "                                                      , min_max_distance\n",
                "                                                      , max_large_unit\n",
                "                                                      , max_min_no\n",
                "                                                      , min_max_no\n",
                "                                                      , max_in_30_and_large\n",
                "                                                      , max_large_nicu\n",
                "                                                      , consolidation\n",
                "                                                      ))\n",
                "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
                "\n",
                "toolbox = base.Toolbox()\n",
                "\n",
                "num_sites = 0\n",
                "\n",
                "current_site_list = []\n",
                "\n",
                "def get_nearby_sites(home, mutation_depth_no=restricted_mutation_depth):\n",
                "    # Returns a list of site indices sorted by distance (nearest first)\n",
                "    sorted_sites = sorted(range(len(current_site_list)), key=lambda site_idx: travel_times_dict.get((home, current_site_list[site_idx]), float('inf')))\n",
                "    return sorted_sites[:mutation_depth_no]\n",
                "\n",
                "def prepare_site_list(site_codes, proposed_additions, restricted_sites):\n",
                "    combined_sites = site_codes + proposed_additions\n",
                "    filtered_sites = [site for site in combined_sites if site not in restricted_sites]\n",
                "    return filtered_sites\n",
                "\n",
                "# Function to asign a random site to each individual in the population but allow us to add or remove sites\n",
                "def restricted_random_site():\n",
                "    return random.choice(range(len(current_site_list)))\n",
                "\n",
                "restricted_site_indices = {site_codes.index(code) for code in restricted_sites if code in current_site_list}\n",
                "\n",
                "toolbox.register(\"random_site\", restricted_random_site)\n",
                "\n",
                "# This function allows us to create random individuals based on the weighted distribution of the patients travelling to their nearest x site\n",
                "def weighted_random_choice(cumulative_probs):\n",
                "    rnd = random.random()\n",
                "    for i, prob in enumerate(cumulative_probs):\n",
                "        if rnd <= prob:\n",
                "            return i\n",
                "    print(len(cumulative_probs) - 1)\n",
                "    return len(cumulative_probs) - 1  # Fallback in case of rounding errors\n",
                "\n",
                "def weighted_site(home, nearby_sites, cumulative_probs):\n",
                "    weighted_index = weighted_random_choice(cumulative_probs)\n",
                "    home_sites = nearby_sites[home]\n",
                "    # Safety check to ensure the index is within bounds\n",
                "    while weighted_index >= len(home_sites):\n",
                "        weighted_index = weighted_random_choice(cumulative_probs)\n",
                "    selected_site_index = home_sites[weighted_index]\n",
                "    return selected_site_index\n",
                "\n",
                "def create_weighted_individual(cumulative_probs, home_lsoas, nearby_sites):\n",
                "    individual = []\n",
                "    for home in home_lsoas:\n",
                "        weighted_site_code = weighted_site(home, nearby_sites, cumulative_probs)\n",
                "        individual.append(weighted_site_code)\n",
                "    return creator.Individual(individual)\n",
                "\n",
                "# Create an extreme individual based on the the data itself\n",
                "def nearest_restricted_site(home, restricted_site_indices, passed_sites, travel_times_dict):\n",
                "    # Find the nearest non-restricted site\n",
                "    valid_sites_indices = [i for i in range(len(passed_sites)) if i not in restricted_site_indices]\n",
                "    nearest_site_idx = min(valid_sites_indices, key=lambda site_idx: travel_times_dict.get((home, passed_sites[site_idx]), float('inf')))\n",
                "    return nearest_site_idx\n",
                "\n",
                "def create_individual_based_on_data(most_frequent_sites, home_lsoas, passed_sites, restricted_site_indices, nearby_sites):\n",
                "    site_code_indices = {code: idx for idx, code in enumerate(passed_sites)}\n",
                "    site_index_map = {}\n",
                "    for _, row in most_frequent_sites.iterrows():\n",
                "        home_code = row['Der_Postcode_LSOA_Code']\n",
                "        site_code = row['SiteLSOA']\n",
                "        site_idx = site_code_indices.get(site_code)\n",
                "        \n",
                "        # If the site is not restricted, use it; otherwise, use the first site from nearby_sites\n",
                "        if site_idx is not None and site_idx not in restricted_site_indices:\n",
                "            site_index_map[home_code] = site_idx\n",
                "        else:\n",
                "            # Assign the first site from nearby_sites, as it already accounts for restrictions\n",
                "            home_nearby_sites = nearby_sites[home_code]\n",
                "            site_index_map[home_code] = home_nearby_sites[0] if home_nearby_sites else None\n",
                "\n",
                "    # Build the individual based on the most frequented site index or the first site from nearby_sites\n",
                "    individual = [site_index_map.get(home, nearby_sites[home][0] if nearby_sites[home] else None) for home in home_lsoas]\n",
                "\n",
                "    return creator.Individual(individual)\n",
                "\n",
                "\n",
                "# Create an extreme individual based on the sites in the data using most frequent where more than one site\n",
                "\n",
                "def create_extreme_individual():\n",
                "    individual = []\n",
                "    for home_idx, home in enumerate(home_lsoas):\n",
                "        nearest_site_idx = nearest_restricted_site(home, restricted_site_indices, current_site_list, travel_times_dict)\n",
                "        individual.append(nearest_site_idx)\n",
                "    return creator.Individual(individual)\n",
                "\n",
                "proportion_weighted = 0.1\n",
                "\n",
                "def init_population(n, prop_weighted):\n",
                "    population = []\n",
                "    num_weighted = int(n * prop_weighted)\n",
                "    num_random = n - num_weighted - len(population)\n",
                "    z = 0\n",
                "    # Add the extreme individual if flagged to\n",
                "    if include_extreme_individual:\n",
                "        population.append(create_extreme_individual())\n",
                "        z += 1\n",
                "    # Add the individual based on the actual data if flagged to\n",
                "    if include_original_sites:\n",
                "        population.append(create_individual_based_on_data(most_frequent_sites, home_lsoas, current_site_list, restricted_site_indices, nearby_sites))\n",
                "        z += 1\n",
                "\n",
                "    # Add weighted individuals\n",
                "    for _ in range(num_weighted):\n",
                "        population.append(create_weighted_individual(cumulative_probs, home_lsoas, nearby_sites))\n",
                "\n",
                "    # Add random individuals\n",
                "    for _ in range(num_random - z):\n",
                "        population.append(toolbox.individual())\n",
                "    print (f\"        Added {num_weighted} weighted individuals and {num_random - z} random individuals\")\n",
                "\n",
                "    return population\n",
                "\n",
                "toolbox.register(\"population\", init_population, prop_weighted = proportion_weighted)\n",
                "\n",
                "def create_logs_df():\n",
                "    column_types = {'individual': 'str',\n",
                "                    'avg_time': 'float64'\n",
                "                    ,'prop_within_30_mins': 'float64'\n",
                "                    ,'max_distance': 'float64'\n",
                "                    ,'units_over_x': 'float64'\n",
                "                    ,'smallest_site': 'float64'\n",
                "                    ,'largest_site': 'float64'\n",
                "                    ,'max_in_30_and_large': 'float64'\n",
                "                    ,'totals': 'float64'\n",
                "                    ,'large_nicu': 'float64'\n",
                "                    ,'consolidation_stdev': 'float64'\n",
                "                    }\n",
                "\n",
                "    # Create a DataFrame with the specified columns and data types\n",
                "    logs_df = pd.DataFrame(columns=column_types.keys()).astype(column_types)\n",
                "    return logs_df\n",
                "\n",
                "inner_log_df = pd.DataFrame(columns=['site',\n",
                "                                    'home',\n",
                "                                    'activity_type',\n",
                "                                    'activity_counts'])\n",
                "\n",
                "activity_log_df = pd.DataFrame(columns=['Generation', 'Site', 'HDU', 'SCBU', 'NICU'])\n",
                "\n",
                "def calculate_activity_counts(individual):\n",
                "    activity_counts = defaultdict(lambda: [0, 0, 0])  # Initialize counts for each activity at each site\n",
                "    # used_sites = site_codes + proposed_additions  # Combine existing and proposed sites\n",
                "    # Iterate over each home-site pair\n",
                "    for home_idx, site_idx in enumerate(individual):\n",
                "        site = current_site_list[site_idx]  # Get the site assigned to this home\n",
                "        home_activity_counts = home_activities[home_idx]  # Get the activity counts for this home\n",
                "        # Aggregate activities at the assigned site\n",
                "        for i in range(len(home_activity_counts)):\n",
                "            activity_counts[site][i] += home_activity_counts[i]\n",
                "    return activity_counts\n",
                "\n",
                "# these function are to allow us to apply penalty objective to the evolution, \n",
                "# this will enable us to evaluate different proposed scenarios\n",
                "def is_feasible(individual):\n",
                "    activity_counts = calculate_activity_counts(individual)\n",
                "    for site, counts in activity_counts.items():\n",
                "        if site in activity_limits:\n",
                "            for i, activity in enumerate(['HDU', 'SCBU', 'NICU']):\n",
                "                limits = activity_limits[site].get(activity)\n",
                "                if limits:\n",
                "                    if counts[i] < limits.get('min', 0) or counts[i] > limits.get('max', float('inf')):\n",
                "                        return False\n",
                "    return True\n",
                "\n",
                "def distance_to_feasibility(individual):\n",
                "    distance = 0\n",
                "    activity_counts = calculate_activity_counts(individual)\n",
                "    for site, counts in activity_counts.items():\n",
                "        if site in activity_limits:\n",
                "            for i, activity in enumerate(['HDU', 'SCBU', 'NICU']):\n",
                "                limits = activity_limits[site].get(activity)\n",
                "                if limits:\n",
                "                    excess = max(0, counts[i] - limits.get('max', float('inf')))\n",
                "                    shortfall = max(0, limits.get('min', 0) - counts[i])\n",
                "                    distance += excess + shortfall\n",
                "    return distance\n",
                "\n",
                "base_penalty = 0.1  # Base penalty\n",
                "penalty_factor = 1.1  # Exponential factor\n",
                "\n",
                "def exponential_penalty(individual):\n",
                "    distance = distance_to_feasibility(individual)\n",
                "    penalty_value = base_penalty * (penalty_factor ** distance)\n",
                "    weights = creator.FitnessMulti.weights\n",
                "    # print(f\"Distance: {distance}, Penalty Value: {penalty_value}\")\n",
                "    penalties = []\n",
                "    for weight in weights:\n",
                "        if weight > 0:  # Penalise maximisation objectives\n",
                "            penalties.append(-penalty_value)\n",
                "        else:           # Penalise minimisation objectives\n",
                "            penalties.append(penalty_value)\n",
                "    # print(f\"Penalties: {penalties}\")\n",
                "    return tuple(penalties)\n",
                "\n",
                "# Normalization function in order that no parameter dominations the evolutionary process simply due to its scale\n",
                "def normalize(raw_value, min_value, max_value):\n",
                "    return (raw_value - min_value) / (max_value - min_value)\n",
                "\n",
                "\n",
                "def eval_func(individual, activity_focus=None):\n",
                "    global inner_log_df, logs_df \n",
                "    # Initialize accumulators and counters\n",
                "    total_time = 0\n",
                "    total_population = 0\n",
                "    within_30_mins = 0\n",
                "    # constraint_adherence = 0\n",
                "    total_time_activity_weighted = 0\n",
                "    total_activity_count = 0\n",
                "\n",
                "    # Calculate activity counts for each site\n",
                "    activity_counts = calculate_activity_counts(individual)\n",
                "\n",
                "    missing_combinations = [] \n",
                "    \n",
                "    # Loop over each home-site pair in the individual\n",
                "    for home_idx, site_idx in enumerate(individual):\n",
                "        home = home_lsoas[home_idx]\n",
                "        site = current_site_list[site_idx]\n",
                "        key = (home, site)\n",
                "        if key not in travel_times_dict:\n",
                "            missing_combinations.append(key)\n",
                "\n",
                "        if (home, site) in travel_times_dict:\n",
                "            travel_time = travel_times_dict[(home, site)]\n",
                "            total_time += travel_time * home_populations[home_idx]\n",
                "            total_population += home_populations[home_idx]\n",
                "\n",
                "            if travel_time <= 30:\n",
                "                within_30_mins += home_populations[home_idx]\n",
                "\n",
                "            activity_counts_per_home = home_activities[home_idx]\n",
                "            for activity_count in activity_counts_per_home:\n",
                "                total_time_activity_weighted += travel_time * home_populations[home_idx] * activity_count\n",
                "                total_activity_count += activity_count\n",
                "\n",
                "    avg_time = total_time / total_population if total_population else 0\n",
                "    # avg_time_activity_weighted = total_time_activity_weighted / total_activity_count if total_activity_count else 0\n",
                "    prop_within_30_mins = within_30_mins / total_population if total_population else 0\n",
                "    travel_times = [travel_times_dict[(home_lsoas[home_idx], current_site_list[site_idx])]\n",
                "                for home_idx, site_idx in enumerate(individual)\n",
                "                if (home_lsoas[home_idx], current_site_list[site_idx]) in travel_times_dict]\n",
                "\n",
                "    max_distance = max(travel_times, default=0)  # default=0 handles empty list\n",
                "\n",
                "\n",
                "    site_activities = {site: sum(counts) for site, counts in activity_counts.items()}\n",
                "    \n",
                "    #print(site_activities)\n",
                "    \n",
                "    smallest_site = min(site_activities.values())\n",
                "    largest_site = max(site_activities.values())\n",
                "    \n",
                "    min_max_values = [\n",
                "        (min_avg_time, max_avg_time), \n",
                "        (min_prop_within_30_mins, max_prop_within_30_mins),\n",
                "        (min_min_max_distance, max_min_max_distance),\n",
                "        (min_number_of_sites_over_nicu_threshold, max_number_of_sites_over_nicu_threshold ),\n",
                "        (min_smallest_site, max_smallest_site),\n",
                "        (min_largest_site, max_largest_site),\n",
                "        (min_prop_within_30_mins_and_large_NICU, max_prop_within_30_mins_and_large_NICU),\n",
                "        (min_max_large_nicu, max_max_large_nicu),\n",
                "        (min_consolidation_metric,max_consolidation_metric)\n",
                "    ]\n",
                "    if not site_activities:\n",
                "        return [0] * len(min_max_values)  # Return a list of zeroes for each objective, or handle as appropriate\n",
                "    \n",
                "    # Count the number of sites that meet or exceed the threshold for NICU activities\n",
                "    NICU_INDEX = 2\n",
                "    HDU_INDEX = 0\n",
                "    \n",
                "    # Find the sites that meet the NICU threshold\n",
                "    nicu_sites = [site for site, counts in activity_counts.items() if counts[NICU_INDEX] >= nicu_activities_threshold]\n",
                "    # number_of_sites_over_nicu_threshold = len(nicu_sites)\n",
                "    large_nicu = [counts[NICU_INDEX] for site, counts in activity_counts.items()]\n",
                "    large_nicu_count = max(large_nicu)\n",
                "    \n",
                "    # Calculate the total NICU activity count across all sites\n",
                "    total_nicu_activities = sum(counts[NICU_INDEX] for site, counts in activity_counts.items())\n",
                "    # Calculate the NICU activity count at sites that exceed the threshold\n",
                "    over_threshold_nicu_activities = sum(counts[NICU_INDEX] for site, counts in activity_counts.items() if counts[NICU_INDEX] >= nicu_activities_threshold)\n",
                "    # Calculate the proportion of NICU activities that are at sites over the threshold\n",
                "    proportion_over_threshold_nicu_activities = (over_threshold_nicu_activities / total_nicu_activities \n",
                "                                                if total_nicu_activities != 0 else 0)\n",
                "\n",
                "    # Calculate the population within 30 minutes and going to a large NICU site\n",
                "    within_30_mins_and_large_NICU = 0\n",
                "    for home_idx, site_idx in enumerate(individual):\n",
                "        home = home_lsoas[home_idx]\n",
                "        site = current_site_list[site_idx]\n",
                "        travel_time = travel_times_dict.get((home, site), float('inf'))\n",
                "        if travel_time <= 30 and site in nicu_sites:\n",
                "            within_30_mins_and_large_NICU += home_populations[home_idx]\n",
                "            \n",
                "    # Calculate the proportion (or 0 if total_population is 0)\n",
                "    prop_within_30_mins_and_large_NICU = within_30_mins_and_large_NICU / total_population if total_population != 0 else 0\n",
                "\n",
                "    # CONSOLITDATION METRIC TO AID LARGER SITE CREATION\n",
                "    # Extract NICU activities for each site\n",
                "    nicu_activities_per_site = {site: counts[NICU_INDEX] for site, counts in activity_counts.items()}\n",
                "    # Get the list of NICU activities per site\n",
                "    nicu_activities = list(nicu_activities_per_site.values())\n",
                "    # Calculate the standard deviation of NICU activities as the consolidation score\n",
                "    std_dev_nicu_activities = np.std(nicu_activities) if nicu_activities else 0\n",
                "    # This score represents the spread in NICU activities across sites; higher values indicate more consolidation\n",
                "    consolidation_score_nicu = std_dev_nicu_activities\n",
                "\n",
                "\n",
                "    # Create a new DataFrame from the dictionary\n",
                "    new_row = pd.DataFrame([{\n",
                "        'individual': individual.index,  \n",
                "        'avg_time': avg_time,\n",
                "        'prop_within_30_mins': prop_within_30_mins,\n",
                "        'max_distance': max_distance,\n",
                "        'units_over_x': proportion_over_threshold_nicu_activities,\n",
                "        'smallest_site': smallest_site,\n",
                "        'largest_site': largest_site,\n",
                "        'totals' : total_population,\n",
                "        'activity_counts': activity_counts,\n",
                "        'large_nicu': large_nicu_count,\n",
                "        'consolidation_stdev': consolidation_score_nicu\n",
                "    }])\n",
                "\n",
                "    # Concatenate the new DataFrame with the existing one\n",
                "    logs_df = pd.concat([logs_df, new_row], ignore_index=True)\n",
                "\n",
                "\n",
                "    # Raw objective values\n",
                "    raw_objectives = [\n",
                "        avg_time, \n",
                "        prop_within_30_mins, \n",
                "        max_distance, \n",
                "        proportion_over_threshold_nicu_activities,\n",
                "        smallest_site, \n",
                "        largest_site, \n",
                "        prop_within_30_mins_and_large_NICU,\n",
                "        large_nicu_count,\n",
                "        consolidation_score_nicu\n",
                "    ]\n",
                "    \n",
                "    # Normalize objectives\n",
                "    normalized_objectives = [\n",
                "        normalize(raw, min_val, max_val) \n",
                "        for raw, (min_val, max_val) in zip(raw_objectives, min_max_values)\n",
                "    ]\n",
                "\n",
                "    # return (avg_time,\n",
                "    #         prop_within_30_mins,\n",
                "    #         max_distance,\n",
                "    #         proportion_over_threshold_nicu_activities,\n",
                "    #         smallest_site,\n",
                "    #         largest_site,\n",
                "    #         prop_within_30_mins_and_large_NICU,\n",
                "    #         large_nicu_count,\n",
                "    #         consolidation_score_nicu)\n",
                "            \n",
                "    return normalized_objectives\n",
                "\n",
                "# Random mutation function\n",
                "def restricted_mutUniformInt(individual, low, up, indpb, objective_stagnation_threshold, activity_limits):\n",
                "    for i, site_index in enumerate(individual):\n",
                "        if random.random() < indpb:\n",
                "            individual[i] = restricted_random_site()\n",
                "    if generations_since_improvement >= objective_stagnation_threshold and activity_limits:\n",
                "        # print(f'         forceful mutation used {generations_since_improvement}')\n",
                "        # print(f\"Scenario structure before calling forceful_mutation: {type(activity_limits)}\")\n",
                "        individual = forceful_mutation(individual, activity_limits, nearby_sites, cumulative_probs, home_lsoas)\n",
                "    return individual,\n",
                "\n",
                "# Let us also create an alternative mutation function which limits the choice of site to one of the 3 nearest rather than any\n",
                "# This should reflect the more realistic real world scenario whereby travel is more limited to nearer sites\n",
                "\n",
                "def restricted_mutNearbyInt(individual, indpb, nearby_passed_sites):\n",
                "    for i, site_index in enumerate(individual):\n",
                "        if random.random() < indpb:\n",
                "            home = home_lsoas[i]\n",
                "            if home in nearby_passed_sites:\n",
                "                # Choose from nearby random site\n",
                "                individual[i] = random.choice(nearby_passed_sites[home])\n",
                "            else:\n",
                "                # Fallback to random if nearby info is not available\n",
                "                individual[i] = restricted_random_site()\n",
                "    return individual,\n",
                "\n",
                "# # Following, there is another alternative mutation assigning nearby sites \n",
                "# # based on the real data distribution of sites based on travel times\n",
                "\n",
                "nearby_sites = {}\n",
                "\n",
                "def forceful_mutation(individual, objective_sites, nearby_sites, cumulative_probs, home_lsoas, maximization_percentage=1, minimization_percentage=1):\n",
                "    for i, assigned_site in enumerate(individual):\n",
                "        home = home_lsoas[i]\n",
                "        # original_site = assigned_site  \n",
                "        for site_code, objectives in objective_sites.items():\n",
                "            # Extract NICU objectives\n",
                "            nicu_objective = objectives.get('NICU', {})\n",
                "\n",
                "            sc_id = current_site_list.index(site_code)\n",
                "\n",
                "            # Apply Lower Limiting Logic\n",
                "            if 'min' in nicu_objective:\n",
                "                # print (f\"Assigned Site {assigned_site} NICU Objective Site Code {sc_id}\")\n",
                "                if assigned_site != sc_id and random.random() < maximization_percentage:\n",
                "                    # Check if the target site (for Lower Limiting Logic) is among the top preferences for this home\n",
                "                    potential_sites = nearby_sites[home][:3]  # Top 3 potential sites for this home\n",
                "                    if sc_id in potential_sites:\n",
                "                        new_site = weighted_site(home, nearby_sites, cumulative_probs)\n",
                "                        individual[i] = new_site\n",
                "                        # print(f\"Lower Limiting Logic: Home {home} from Site {original_site} to {new_site}\")\n",
                "\n",
                "            # Apply Upper Limiting Logic\n",
                "            elif 'max' in nicu_objective and assigned_site == sc_id:\n",
                "                if random.random() < minimization_percentage:\n",
                "                    # Exclude the current minimization target site from options\n",
                "                    options = [s for s in nearby_sites[home] if s != site_code]\n",
                "                    new_site = weighted_site(home, nearby_sites, cumulative_probs)\n",
                "                    individual[i] = new_site\n",
                "                    # print(f\"Upper Limiting Logic: Home {home} from Site {original_site} to {new_site}\")\n",
                "    return individual\n",
                "\n",
                "def weighted_mutation_function(individual, indpb, cumulative_probs, home_lsoas, objective_stagnation_threshold, activity_limits):\n",
                "    global generations_since_improvement\n",
                "# def weighted_mutation_function(individual, indpb, cumulative_probs, home_lsoas, objective_stagnation_threshold, generations_since_improvement, activity_limits):\n",
                "\n",
                "    for i, site_index in enumerate(individual):\n",
                "        if random.random() < indpb:\n",
                "            home = home_lsoas[i]\n",
                "            # home_nearby_sites = nearby_sites[home]\n",
                "            # weighted_index = weighted_random_choice(cumulative_probs)\n",
                "            weighted_site_code = weighted_site(home, nearby_sites, cumulative_probs)\n",
                "            # original_site = individual[i]\n",
                "            individual[i] = weighted_site_code\n",
                "            # print(f\"Mutated Home: {home}, Original Site: {original_site}, New Site: {nearest_site_code}, Weighted Index: {weighted_index}\")\n",
                "    \n",
                "    # Apply forceful mutation if stagnation is detected\n",
                "    if generations_since_improvement >= objective_stagnation_threshold and activity_limits:\n",
                "        # print(f'         forceful mutation used {generations_since_improvement}')\n",
                "        # print(f\"Scenario structure before calling forceful_mutation: {type(activity_limits)}\")\n",
                "        individual = forceful_mutation(individual, activity_limits, nearby_sites, cumulative_probs, home_lsoas)\n",
                "    \n",
                "    return individual,\n",
                "\n",
                "# def weighted_mutation_function(individual, indpb, cumulative_probs, home_lsoas):\n",
                "#     for i, site_index in enumerate(individual):\n",
                "#         if random.random() < indpb:\n",
                "#             home = home_lsoas[i]\n",
                "#             home_nearby_sites = nearby_sites[home]\n",
                "#             weighted_index = weighted_random_choice(cumulative_probs)\n",
                "#             weighted_site_code = weighted_site(home, nearby_sites, cumulative_probs)\n",
                "#             original_site = individual[i]\n",
                "#             individual[i] = weighted_site_code\n",
                "#             # print(f\"Mutated Home: {home}, Original Site: {original_site}, New Site: {nearest_site_code}, Weighted Index: {weighted_index}\")\n",
                "#     return individual,\n",
                "\n",
                "def evaluate_with_penalty(individual):\n",
                "    fitness_values = eval_func(individual)\n",
                "    if not is_feasible(individual):\n",
                "        penalties = exponential_penalty(individual)\n",
                "        penalized_fitness = tuple(fv + pv for fv, pv in zip(fitness_values, penalties))\n",
                "        return penalized_fitness\n",
                "    return fitness_values\n",
                "\n",
                "# Create a partial function that has activity_focus pre-specified\n",
                "eval_func_focused = partial(eval_func, activity_focus=activity_focus)\n",
                "\n",
                "toolbox.register(\"evaluate\", evaluate_with_penalty)\n",
                "# toolbox.register(\"evaluate\", eval_func_focused)\n",
                "# toolbox.decorate(\"evaluate\", tools.DeltaPenalty(is_feasible, 7.0, distance_to_feasibility))\n",
                "\n",
                "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
                "\n",
                "# Generate reference points for NSGA3\n",
                "# Parameters\n",
                "NOBJ = 9\n",
                "P = [2, 1]\n",
                "SCALES = [1, 0.5]\n",
                "\n",
                "# Create, combine and removed duplicates\n",
                "ref_points = [tools.uniform_reference_points(NOBJ, p, s) for p, s in zip(P, SCALES)]\n",
                "ref_points = np.concatenate(ref_points, axis=0)\n",
                "_, uniques = np.unique(ref_points, axis=0, return_index=True)\n",
                "ref_points = ref_points[uniques] \n",
                "\n",
                "history = tools.History()\n",
                "\n",
                "# ADAPTIVE STRATEGY FOR MUTATION RATE\n",
                "\n",
                "def adapt_mutation_rate_based_on_stagnation(generations_since_improvement, threshold, initial_mutation_prob, max_mutation_prob):\n",
                "    if generations_since_improvement > threshold:\n",
                "        # Increase mutation probability up to a maximum\n",
                "        return min(initial_mutation_prob * (1 + generations_since_improvement / threshold), max_mutation_prob)\n",
                "    else:\n",
                "        return initial_mutation_prob\n",
                "\n",
                "individual_mutation_prob_amt = 0.3\n",
                "\n",
                "def adapt_individual_mutation_rate_based_on_stagnation(generations_since_improvement, threshold, individual_mutation_prob, individual_mutation_max_prob):\n",
                "    if generations_since_improvement > threshold:\n",
                "        # Increase mutation probability up to a maximum\n",
                "        return min(individual_mutation_prob * (1 + generations_since_improvement / threshold), individual_mutation_max_prob)\n",
                "    else:\n",
                "        return initial_mutation_prob\n",
                "    \n",
                "def calculate_diversity(front):\n",
                "    if len(front) < 2:\n",
                "        return 0\n",
                "\n",
                "    distances = []\n",
                "    for i in range(len(front) - 1):\n",
                "        dist = np.linalg.norm(np.array(front[i].fitness.values) - np.array(front[i+1].fitness.values))\n",
                "        distances.append(dist)\n",
                "\n",
                "    return np.mean(distances)\n",
                "\n",
                "def has_pareto_front_improved(current_front, previous_front, diversity_threshold):\n",
                "    if previous_front is None:\n",
                "        return True\n",
                "\n",
                "    current_size = len(current_front)\n",
                "    previous_size = len(previous_front)\n",
                "\n",
                "    if current_size > previous_size:\n",
                "        return True\n",
                "\n",
                "    if current_size > diversity_threshold:\n",
                "        current_diversity = calculate_diversity(current_front)\n",
                "        previous_diversity = calculate_diversity(previous_front)\n",
                "        # print(f\"Current diversity: {current_diversity} > Previous diversity: {previous_diversity}?\")\n",
                "        if current_diversity > previous_diversity:\n",
                "            return True\n",
                "    \n",
                "    return False\n",
                "\n",
                "initial_diversity_threshold = 0.10\n",
                "stagnation_limit = 20\n",
                "max_number_generations = 10000\n",
                "stagnation_threshold = 10\n",
                "scenario = []\n",
                "\n",
                "generations_since_improvement = 0\n",
                "\n",
                "def main():\n",
                "    global generations_since_improvement\n",
                "    \n",
                "    # restricted_site_indices = {site_codes.index(code) for code in restricted_sites}\n",
                "    \n",
                "    if nsga3:\n",
                "        toolbox.register(\"select\", tools.selNSGA3, ref_points=ref_points)\n",
                "    else:\n",
                "        toolbox.register(\"select\", tools.selNSGA2)\n",
                "\n",
                "    # Define statistics for each objective\n",
                "    stats_time = tools.Statistics(key=lambda ind: ind.fitness.values[0])\n",
                "    stats_time.register(\"avg_time\", np.mean)\n",
                "\n",
                "    stats_prop = tools.Statistics(key=lambda ind: ind.fitness.values[1])\n",
                "    stats_prop.register(\"prop_within_30_mins\", np.max)\n",
                "    \n",
                "    stats_max_distance = tools.Statistics(key=lambda ind: ind.fitness.values[2])\n",
                "    stats_max_distance.register(\"max_distance\", np.mean)\n",
                "    \n",
                "    stats_large_sites = tools.Statistics(key=lambda ind: ind.fitness.values[3])\n",
                "    stats_large_sites.register(\"large_sites\", np.max)\n",
                "    \n",
                "    smallest_site_stats = tools.Statistics(key=lambda ind: ind.fitness.values[4])\n",
                "    smallest_site_stats.register(\"smallest_site\", np.max)\n",
                "    \n",
                "    largest_site_stats = tools.Statistics(key=lambda ind: ind.fitness.values[5])\n",
                "    largest_site_stats.register(\"largest_site\", np.max)\n",
                "    \n",
                "    thirty_and_large_stats = tools.Statistics(key=lambda ind: ind.fitness.values[6])\n",
                "    thirty_and_large_stats.register(\"30_and_large\", np.max)\n",
                "    \n",
                "    large_nicu_stats = tools.Statistics(key=lambda ind: ind.fitness.values[7])\n",
                "    large_nicu_stats.register(\"large_nicu\", np.max)\n",
                "\n",
                "    \n",
                "    consolidation_stats = tools.Statistics(key=lambda ind: ind.fitness.values[7])\n",
                "    consolidation_stats.register(\"consolidation\", np.max)\n",
                "\n",
                "    \n",
                "    # Combine statistics into MultiStatistics\n",
                "    mstats = tools.MultiStatistics(time=stats_time\n",
                "                                   , prop=stats_prop\n",
                "                                   , max_dist=stats_max_distance\n",
                "                                    , large_sites=stats_large_sites\n",
                "                                    ,smallest_site=smallest_site_stats\n",
                "                                    , largest_site=largest_site_stats,\n",
                "                                   thirty_and_large = thirty_and_large_stats\n",
                "                                , large_nicu = large_nicu_stats\n",
                "                                , consolidation = consolidation_stats\n",
                "                                   )\n",
                "\n",
                "    # Initialize and evaluate the population\n",
                "    pop = toolbox.population(n=pop_num)\n",
                "    history.update(pop)\n",
                "    hof = tools.HallOfFame(1)\n",
                "    paretofront = tools.ParetoFront()\n",
                "    fitnesses = map(toolbox.evaluate, pop)\n",
                "    for ind, fit in zip(pop, fitnesses):\n",
                "        ind.fitness.values = fit\n",
                "    bestie = ()\n",
                "\n",
                "    # Create a logbook and record initial statistics\n",
                "    logbook = tools.Logbook()\n",
                "    logbook.header = ['gen', 'nevals'] + (mstats.fields if mstats else [])\n",
                "    record = mstats.compile(pop) if mstats else {}\n",
                "    logbook.record(gen=0, nevals=len(pop), **record)\n",
                "    \n",
                "    # Function to select elite individuals for crossover\n",
                "    # def ranked_selection(population, k):\n",
                "    #     # Rank the population by fitness\n",
                "    #     sorted_pop = sorted(population, key=lambda ind: ind.fitness, reverse=True)\n",
                "    #     # Select the top k individuals\n",
                "    #     return sorted_pop[:k]\n",
                "\n",
                "    # # Number of individuals to select for crossover\n",
                "    # k = len(pop) // 2\n",
                "\n",
                "    generations_since_improvement = 0\n",
                "    previous_pareto_front = None\n",
                "    gen = 0\n",
                "    objective_stagnation_threshold = 20\n",
                "\n",
                "    while generations_since_improvement < stagnation_limit and gen < max_number_generations:\n",
                "    \n",
                "        gen += 1\n",
                "        \n",
                "        # Update hall of fame and Pareto front (paretofront)\n",
                "        hof.update(pop)\n",
                "        paretofront.update(pop)\n",
                "        \n",
                "        current_pop = len(pop) * gen\n",
                "        diversity_threshold = initial_diversity_threshold * current_pop\n",
                "    \n",
                "        # Check if the Pareto front has improved\n",
                "        if has_pareto_front_improved(paretofront, previous_pareto_front, diversity_threshold):\n",
                "            generations_since_improvement = 0\n",
                "            # Store the current Pareto front as the previous front for the next generation\n",
                "            previous_pareto_front = list(paretofront)\n",
                "        else:\n",
                "            generations_since_improvement += 1\n",
                "            \n",
                "        mutation_prob = adapt_mutation_rate_based_on_stagnation(generations_since_improvement,stagnation_threshold,initial_mutation_prob,max_mutation_prob)\n",
                "        individual_mutation_prob_amt = adapt_individual_mutation_rate_based_on_stagnation(generations_since_improvement,stagnation_threshold,individual_mutation_prob,individual_mutation_max_prob)\n",
                "        \n",
                "        # def mutation_wrapper(individual):\n",
                "        #     return weighted_mutation_function(individual\n",
                "        #                                       , individual_mutation_prob_amt\n",
                "        #                                       , cumulative_probs\n",
                "        #                                       , home_lsoas\n",
                "        #                                       , objective_stagnation_threshold\n",
                "        #                                       , generations_since_improvement\n",
                "        #                                       , activity_limits)\n",
                "        \n",
                "        # if weighted_mutation:\n",
                "        #     toolbox.register(\"mutate\", mutation_wrapper)\n",
                "\n",
                "\n",
                "        # print(f\"Generation: {gen} / Pareto Front Size:{len(paretofront)} / Diversity threshold: {diversity_threshold}\")     \n",
                "        # print(f\"Mutation probability {mutation_prob}\")\n",
                "        # print(f\"Individual mutation probability {individual_mutation_prob_amt}, at {generations_since_improvement} generations since improvement\")\n",
                "\n",
                "        # Select the next generation individuals\n",
                "        offspring = toolbox.select(pop, len(pop) - num_elites)\n",
                "        # Clone the selected individuals\n",
                "        offspring = list(map(toolbox.clone, offspring))\n",
                "        \n",
                "        # # Select individuals for crossover\n",
                "        # selected_for_crossover = ranked_selection(pop, k)\n",
                "\n",
                "        # # Apply crossover to elite selected individuals\n",
                "        # for child1, child2 in zip(selected_for_crossover[::2], selected_for_crossover[1::2]):\n",
                "        #     if np.random.rand() < cross_chance:\n",
                "        #         toolbox.mate(child1, child2)\n",
                "        #         del child1.fitness.values\n",
                "        #         del child2.fitness.values\n",
                "\n",
                "        # Apply crossover and mutation on the offspring\n",
                "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
                "            if np.random.rand() < cross_chance:\n",
                "                toolbox.mate(child1, child2)\n",
                "                del child1.fitness.values\n",
                "                del child2.fitness.values\n",
                "\n",
                "        for mutant in offspring:\n",
                "            if np.random.rand() < mutation_prob:\n",
                "                toolbox.mutate(mutant)\n",
                "                del mutant.fitness.values\n",
                "\n",
                "        # Evaluate the individuals with an invalid fitness\n",
                "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
                "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
                "        for ind, fit in zip(invalid_ind, fitnesses):\n",
                "            ind.fitness.values = fit\n",
                "                \n",
                "        # Select the elite individuals\n",
                "        elites = tools.selBest(pop, num_elites)\n",
                "        offspring.extend(elites)\n",
                "        pop[:] = offspring\n",
                "        \n",
                "        # Record statistics for this generation\n",
                "        record = mstats.compile(pop) if mstats else {}\n",
                "        logbook.record(gen=gen+1, nevals=len(invalid_ind), **record)\n",
                "        \n",
                "        sys.stdout.write(\"\\r        Generation: {}, Generations Since Improvement: {}  \".format(gen, generations_since_improvement))\n",
                "        sys.stdout.flush()\n",
                "        # print (\"\\r        Generation: {}, Generations Since Improvement: {}  \".format(gen, generations_since_improvement))\n",
                "        \n",
                "            \n",
                "    bestie = tools.selBest(pop, 1)[0]\n",
                "    print(\" \")\n",
                "\n",
                "    gc.collect()\n",
                "    \n",
                "    return pop, logbook, hof, paretofront, bestie"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "7618e7b1-44a5-45ef-bc71-17b3dffea54d"
            },
            "source": [
                "We can use DEAPs built in selBest tool to select the best individual from the population "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {
                "azdata_cell_guid": "6d906ff1-aeef-4dbd-9fff-3c672f20cc1e",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "# Here we translate the best individual (which is a list of site indices) into a list of (home_code, site_code) pairs\n",
                "def create_solution_list(bestind, home_lsoas, current_site_list):\n",
                "    solution = []\n",
                "    # used_sites = site_codes + proposed_additions\n",
                "    for i, site_index in enumerate(bestind):\n",
                "        home_code = home_lsoas[i]\n",
                "        site_code = current_site_list[site_index]\n",
                "        solution.append((home_code, site_code))\n",
                "    return solution  # return the solution list\n",
                "\n",
                "# def add_solution(activities, solution, solution_number, activity_focus):\n",
                "    \n",
                "#     solution_column_name = f'solution_{solution_number}'\n",
                "#     solution_unit_name = f'solution_{solution_number}_unit'\n",
                "    \n",
                "#     # Ensure the solution column exists\n",
                "#     if solution_column_name not in activities.columns:\n",
                "#         activities[solution_column_name] = np.nan\n",
                "    \n",
                "#     # Convert the solution list to a dictionary for faster lookup\n",
                "#     solution_dict = dict(solution)\n",
                "    \n",
                "#     # Iterate over the activities DataFrame and update where conditions match\n",
                "#     for idx, row in activities.iterrows():\n",
                "#         if (not activity_focus or row['CC_Level'] in activity_focus) and row['Der_Postcode_LSOA_Code'] in solution_dict:\n",
                "#             activities.at[idx, solution_column_name] = solution_dict[row['Der_Postcode_LSOA_Code']]\n",
                "            \n",
                "#     # Drop the solution_unit_name column if it exists\n",
                "#     if solution_unit_name in activities.columns:\n",
                "#         activities = activities.drop(solution_unit_name, axis=1)\n",
                "    \n",
                "#     # Merge and then drop the LSOA column, ensuring the merged column name is correct\n",
                "#     merged_df = pd.merge(activities, sites[['LSOA', 'UnitCode']], left_on=solution_column_name, right_on='LSOA', how='left')\n",
                "#     merged_df = merged_df.drop('LSOA', axis=1)\n",
                "#     merged_df.rename(columns={'UnitCode': solution_unit_name}, inplace=True)\n",
                "    \n",
                "#     return merged_df\n",
                "\n",
                "def add_solution(activities, solution, solution_number, activity_focus, travel_times):\n",
                "    solution_column_name = f'solution_{solution_number}'\n",
                "    solution_unit_name = f'solution_{solution_number}_unit'\n",
                "    solution_travel_time = f'solution_{solution_number}_travel_time'\n",
                "    \n",
                "    # Ensure the solution and travel time columns exist\n",
                "    if solution_column_name not in activities.columns:\n",
                "        activities[solution_column_name] = np.nan\n",
                "    activities[solution_travel_time] = np.nan  # New column for travel times\n",
                "    \n",
                "    # Convert the solution list to a dictionary for faster lookup\n",
                "    solution_dict = dict(solution)\n",
                "    \n",
                "    # Iterate over the activities DataFrame and update where conditions match\n",
                "    for idx, row in activities.iterrows():\n",
                "        lsoa_code = row['Der_Postcode_LSOA_Code']\n",
                "        if (not activity_focus or row['CC_Level'] in activity_focus) and lsoa_code in solution_dict:\n",
                "            target_code = solution_dict[lsoa_code]\n",
                "            activities.at[idx, solution_column_name] = target_code\n",
                "            \n",
                "            # Lookup and set the travel time\n",
                "            travel_time_key = (lsoa_code, target_code)\n",
                "            if travel_time_key in travel_times:\n",
                "                activities.at[idx, solution_travel_time] = travel_times[travel_time_key]\n",
                "    \n",
                "    if solution_unit_name in activities.columns:\n",
                "        activities = activities.drop(solution_unit_name, axis=1)\n",
                "    \n",
                "    merged_df = pd.merge(activities, sites[['LSOA', 'UnitCode']], left_on=solution_column_name, right_on='LSOA', how='left')\n",
                "    merged_df = merged_df.drop('LSOA', axis=1)\n",
                "    merged_df.rename(columns={'UnitCode': solution_unit_name}, inplace=True)\n",
                "    \n",
                "    return merged_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {
                "azdata_cell_guid": "b6cb1194-c905-482b-8017-acfe38445990",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "def export_log(solution_id, timestamp):\n",
                "    # now = datetime.now()\n",
                "    # timestamp = now.strftime(\"%Y%m%d_%H%M%S\") \n",
                "    ## Specify the file name\n",
                "    if solution_id:\n",
                "        log_name = f\"./Logs/activities_output_{timestamp}_solution_{solution_id}.csv.gz\"\n",
                "    else:\n",
                "        log_name = f\"./Logs/activities_output_{timestamp}.csv.gz\"\n",
                "    # Save the DataFrame to CSV\n",
                "    logs_df.to_csv(log_name, index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {
                "azdata_cell_guid": "46613e4e-764e-4ab8-b860-086d48629c7a",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "def export_solutions(activities_with_solutions, financial_year ,timestamp, details):\n",
                "    \n",
                "    file_name = f\"./Data_Output/activities_output_{financial_year.replace('/', '')}_AT_{timestamp}_{details}.csv\"\n",
                "    activities_with_solutions.to_csv(file_name, index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {
                "azdata_cell_guid": "8145c9ad-0ad8-470c-b0a4-5379076195c1",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "financial_year = ''\n",
                "\n",
                "def aggregate_results(df):\n",
                "      \n",
                "      # solution_columns = [col for col in df.columns if 'solution_' in col and '_unit' not in col]\n",
                "      solution_columns = [col for col in df.columns if 'solution_' in col and '_unit' not in col and '_travel_time' not in col]\n",
                "\n",
                "\n",
                "      df_melted = df.melt(id_vars=[col for col in df.columns if col not in solution_columns],\n",
                "                        value_vars=solution_columns, \n",
                "                        var_name='SolutionColumn', \n",
                "                        value_name='Solution')\n",
                "\n",
                "      df_melted['SolutionNumber'] = df_melted['SolutionColumn'].apply(lambda x: x.split('_')[1])\n",
                "\n",
                "      df_melted['CC_Activity_Date'] = pd.to_datetime(df_melted['CC_Activity_Date'])\n",
                "      df_melted['Fin_Year'] = pd.cut(df_melted['CC_Activity_Date'], \n",
                "                                    bins=[pd.Timestamp('2018-04-01'), pd.Timestamp('2019-04-01'),\n",
                "                                          pd.Timestamp('2020-04-01'), pd.Timestamp('2021-04-01'),\n",
                "                                          pd.Timestamp('2022-04-01')],\n",
                "                                    labels=['18/19', '19/20', '20/21', '21/22'])\n",
                "\n",
                "      grouped = df_melted.groupby(['Solution', 'SolutionNumber', \n",
                "                                    'CC_Level', 'Fin_Year']).size().reset_index(name='Activity_Count')\n",
                "\n",
                "      sorted_df = grouped.sort_values(by=['SolutionNumber', 'Solution', 'CC_Level', 'Fin_Year'])\n",
                "\n",
                "      final_df = sorted_df.loc[sorted_df['Fin_Year'] == financial_year]\n",
                "\n",
                "      return final_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {
                "azdata_cell_guid": "428a94be-a748-4509-b0ff-fbf290c2b079",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "def generate_detail_string(nsga3 = nsga3, restricted_mutation = restricted_mutation\n",
                "                           , restricted_mutation_depth = restricted_mutation_depth,\n",
                "                           include_extreme_individual = include_extreme_individual,\n",
                "                           include_original_sites = include_original_sites):\n",
                "    detail_string_parts = []\n",
                "    if nsga3:\n",
                "        detail_string_parts.append(\"NSGA3\")\n",
                "    if not nsga3:\n",
                "        detail_string_parts.append(\"NSGA2\")\n",
                "    if restricted_mutation:\n",
                "        detail_string_parts.append(f\"Site_Limit_{restricted_mutation_depth}\")\n",
                "    if weighted_mutation:\n",
                "        detail_string_parts.append(f\"Weighted\")\n",
                "    if include_extreme_individual:\n",
                "        detail_string_parts.append(\"EI_Inc\")\n",
                "    if include_original_sites:\n",
                "        detail_string_parts.append(\"OI_Inc\")\n",
                "    detail_string_parts.append(f\"Num_Elites_{elite_pop}\")\n",
                "    detail_string = '_'.join(detail_string_parts)\n",
                "    return detail_string"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {
                "azdata_cell_guid": "c708b3c6-a55d-4573-acb2-739915072f57",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "def output_results(results,timestamp,run_detail_string):\n",
                "\n",
                "    file_parts = [\"./Data_Output/activities_output_grouped\", financial_year.replace('/', ''), f\"AT_{timestamp}\"]\n",
                "    file_parts.append(run_detail_string)\n",
                "    file_name = '_'.join(file_parts) + \".csv\"\n",
                "\n",
                "    results.to_csv(file_name, index=False)\n",
                "    \n",
                "    return print(f\"File output: {file_name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {
                "azdata_cell_guid": "9a1de160-608b-493f-990c-ac506a5717ba",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "def output_map(m,timestamp,solution_number,run_detail_string):\n",
                "\n",
                "    file_parts = [\"./Data_Output/map\", financial_year.replace('/', ''), f\"AT_{timestamp}\"]\n",
                "    file_parts.append(f\"Solution_{solution_number}\")\n",
                "    file_parts.append(run_detail_string)\n",
                "    file_name = '_'.join(file_parts) + \".html\"\n",
                "\n",
                "    m.save(file_name)\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "1af6dd8c-1bdd-4e15-b151-3bd0ceea44d9"
            },
            "source": [
                "Then lets run our algorithm and evolve our solutions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {
                "azdata_cell_guid": "909ff712-a246-4d73-9d55-8abe8e01667f",
                "language": "python"
            },
            "outputs": [],
            "source": [
                " # https://hubble-live-assets.s3.eu-west-1.amazonaws.com/bapm/file_asset/file/64/LNU_doc_Nov_2018.pdf\n",
                " # LNUs over 600 advised to have dedicated tier 3 resource so ideally under\n",
                " # LNUs over 400 IC days dedicated resident Tier 2 resource \n",
                " # so surmising that the idealised tier 2 should have between 600 and 400 IC days\n",
                "scenarios = [\n",
                "           ['1: Full run Alder Hey removed',\n",
                "                 # objectives\n",
                "                {}\n",
                "                ,# proposed additions\n",
                "                []\n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]\n",
                "            ,\n",
                "            ['2: Burnley tier 2 IC Days Limit',\n",
                "                 # objectives\n",
                "                {'E01024897': {'NICU': {'max': 600}} # Burnley\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                []\n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]\n",
                "            ,\n",
                "            ['3: Preston tier 2 IC Days Limit',\n",
                "                 # objectives\n",
                "                {'E01025300': {'NICU': {'max': 600}} # Preston\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                []\n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]\n",
                "            ,\n",
                "            ['4: Burnley tier 1 ICU Limit',\n",
                "                 # objectives\n",
                "                {'E01024897': {'NICU': {'max': 400}} # Burnley\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                []\n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]\n",
                "            ,\n",
                "            ['5: Preston tier 1 ICU Limit',\n",
                "                 # objectives\n",
                "                {'E01025300': {'NICU': {'max': 400}} # Preston\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                []\n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]\n",
                "            ,\n",
                "            ['6: Blackburn added as tier 3',\n",
                "                 # objectives\n",
                "                {'E01012632': {'NICU': {'min': 1000}} # Blackburn\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                ['E01012632'] # Blackburn\n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]            \n",
                "            ,\n",
                "            ['7: Blackburn added as tier 2',\n",
                "                 # objectives\n",
                "                {'E01012632': {'NICU': {'max': 600}} # Blackburn\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                ['E01012632'] # Blackburn\n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]\n",
                "            ,\n",
                "            ['8: Blackburn added as tier 3 Burnley removed',\n",
                "                 # objectives\n",
                "                {'E01012632': {'NICU': {'min': 1000}} # Blackburn\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                ['E01012632'] # Blackburn\n",
                "                ,# proposed restrictions\n",
                "                ['E01006570','E01024897'] # Remove Alder Hey and Burnley\n",
                "                ]\n",
                "            ,\n",
                "            ['9: Blackburn added as tier 3 Preston Removed',\n",
                "                 # objectives\n",
                "                {'E01012632': {'NICU': {'min': 1000}} # Blackburn\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                ['E01012632'] # Blackburn\n",
                "                ,# proposed restrictions\n",
                "                ['E01006570','E01025300'] # Remove Alder Hey and Preston\n",
                "                ]\n",
                "            ,\n",
                "            ['10: Blackburn added as tier 3, Preston and Burnley restricted to tier 1',\n",
                "                 # objectives\n",
                "                {'E01012632': {'NICU': {'min': 2000}} # Blackburn\n",
                "                 ,'E01024897': {'NICU': {'max': 400}} # Burnley\n",
                "                 ,'E01025300': {'NICU': {'max': 400}} # Preston\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                ['E01012632'] # Blackburn\n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]  \n",
                "            ,\n",
                "            ['11: Tameside restricted to tier 1',\n",
                "                 # objectives\n",
                "                {'E01005944': {'NICU': {'max': 400}} # Tameside\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                [] \n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]      \n",
                "            ,\n",
                "            ['12: Bolton restricted to tier 1',\n",
                "                 # objectives\n",
                "                {'E01004880': {'NICU': {'max': 400}} # Bolton\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                [] \n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]    \n",
                "            ,  \n",
                "            ['13: Wigan (Royal Albert) restricted to tier 1',\n",
                "                 # objectives\n",
                "                {'E01006370': {'NICU': {'max': 400}} # Wigan\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                [] \n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ] \n",
                "            ,     \n",
                "            ['14: St Marys (MFT) restricted to tier 1',\n",
                "                 # objectives\n",
                "                {'E01005062': {'NICU': {'max': 400}} # St Marys\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                [] \n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]      \n",
                "            ,     \n",
                "            ['15: Oldham restricted to tier 1',\n",
                "                 # objectives\n",
                "                {'E01005354': {'NICU': {'max': 400}} # Oldham\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                [] \n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]          \n",
                "            ,     \n",
                "            ['16: Leighton restricted to tier 1',\n",
                "                 # objectives\n",
                "                {'E01018480': {'NICU': {'max': 400}} # Leighton\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                [] \n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ] \n",
                "            , \n",
                "            ['17: Arrowe Park (Wirral) restricted to tier 1',\n",
                "                 # objectives\n",
                "                {'E01007251': {'NICU': {'max': 400}} # Arrowe Park\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                [] \n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]             \n",
                "            ,     \n",
                "            ['18: COC restricted to tier 1',\n",
                "                 # objectives\n",
                "                {'E01018377': {'NICU': {'max': 400}} # COC\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                [] \n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]    \n",
                "            ,     \n",
                "            ['19: Arrowe Park (Wirral) removed',\n",
                "                 # objectives\n",
                "                {}\n",
                "                ,# proposed additions\n",
                "                [] \n",
                "                ,# proposed restrictions\n",
                "                ['E01006570', 'E01007251'] # Remove Alder Hey and Arrowe Park\n",
                "                ]             \n",
                "            ,     \n",
                "            ['20: COC removed',\n",
                "                 # objectives\n",
                "                {}\n",
                "                ,# proposed additions\n",
                "                [] \n",
                "                ,# proposed restrictions\n",
                "                ['E01006570','E01018377'] # Remove Alder Hey and COC\n",
                "                ]    \n",
                "            ,    \n",
                "            ['21: Warrington restricted to tier 1',\n",
                "                 # objectives\n",
                "                {'E01012457': {'NICU': {'max': 400}} # Warrington\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                [] \n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ] \n",
                "            ,     \n",
                "            ['22: Whiston restricted to tier 1',\n",
                "                 # objectives\n",
                "                {'E01006499': {'NICU': {'max': 400}} # Whiston\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                [] \n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]    \n",
                "            ,    \n",
                "            ['23: Warrington removed',\n",
                "                 # objectives\n",
                "                {}\n",
                "                ,# proposed additions\n",
                "                [] \n",
                "                ,# proposed restrictions\n",
                "                ['E01006570','E01012457'] # Remove Alder Hey\n",
                "                ] \n",
                "            ,     \n",
                "            ['24: Whiston removed',\n",
                "                 # objectives\n",
                "                {}\n",
                "                ,# proposed additions\n",
                "                [] \n",
                "                ,# proposed restrictions\n",
                "                ['E01006570','E01006499'] # Remove Alder Hey\n",
                "                ]   \n",
                "]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {
                "azdata_cell_guid": "1c451954-8a4a-439c-a82f-9b379a31e69a",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Start time: 2024-03-10 22:46:48.387916\n",
                        "No data present for 2019-04-01 00:00:00 to 2020-03-31 00:00:00\n",
                        "No data present for 2020-04-01 00:00:00 to 2021-03-31 00:00:00\n",
                        "Run as 21/22 using NSGA2 and OI included and 10 Elites\n",
                        "    Solution number: 1\n",
                        "        Current site list: ['E01007251', 'E01018377', 'E01018480', 'E01006512', 'E01018616', 'E01025488', 'E01012457', 'E01006499', 'E01005062', 'E01005164', 'E01005070', 'E01006370', 'E01004880', 'E01005354', 'E01005801', 'E01005944', 'E01019155', 'E01033071', 'E01025300', 'E01024897', 'E01012722']\n",
                        "        Scenario: 1: Full run Alder Hey removed\n",
                        "        Proposed Additions: []\n",
                        "        Site Objectives: {}\n",
                        "        Activity Focus: None\n",
                        "        Added 20 weighted individuals and 79 random individuals\n",
                        "        Generation: 24, Generations Since Improvement: 0  "
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn [31], line 120\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        Site Objectives: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivity_limits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        Activity Focus: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivity_focus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m pop, log, hof, paretofront, best \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m best_index \u001b[38;5;241m=\u001b[39m pop\u001b[38;5;241m.\u001b[39mindex(best)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        Index of the best individual: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
                        "Cell \u001b[1;32mIn [20], line 707\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    705\u001b[0m invalid_ind \u001b[38;5;241m=\u001b[39m [ind \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m offspring \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ind\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m.\u001b[39mvalid]\n\u001b[0;32m    706\u001b[0m fitnesses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(toolbox\u001b[38;5;241m.\u001b[39mevaluate, invalid_ind)\n\u001b[1;32m--> 707\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind, fit \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(invalid_ind, fitnesses):\n\u001b[0;32m    708\u001b[0m     ind\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m fit\n\u001b[0;32m    710\u001b[0m \u001b[38;5;66;03m# Select the elite individuals\u001b[39;00m\n",
                        "Cell \u001b[1;32mIn [20], line 469\u001b[0m, in \u001b[0;36mevaluate_with_penalty\u001b[1;34m(individual)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_with_penalty\u001b[39m(individual):\n\u001b[1;32m--> 469\u001b[0m     fitness_values \u001b[38;5;241m=\u001b[39m \u001b[43meval_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindividual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_feasible(individual):\n\u001b[0;32m    471\u001b[0m         penalties \u001b[38;5;241m=\u001b[39m exponential_penalty(individual)\n",
                        "Cell \u001b[1;32mIn [20], line 338\u001b[0m, in \u001b[0;36meval_func\u001b[1;34m(individual, activity_focus)\u001b[0m\n\u001b[0;32m    323\u001b[0m new_row \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([{\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindividual\u001b[39m\u001b[38;5;124m'\u001b[39m: individual\u001b[38;5;241m.\u001b[39mindex,  \n\u001b[0;32m    325\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_time\u001b[39m\u001b[38;5;124m'\u001b[39m: avg_time,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconsolidation_stdev\u001b[39m\u001b[38;5;124m'\u001b[39m: consolidation_score_nicu\n\u001b[0;32m    335\u001b[0m }])\n\u001b[0;32m    337\u001b[0m \u001b[38;5;66;03m# Concatenate the new DataFrame with the existing one\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m logs_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlogs_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_row\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Raw objective values\u001b[39;00m\n\u001b[0;32m    342\u001b[0m raw_objectives \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    343\u001b[0m     avg_time, \n\u001b[0;32m    344\u001b[0m     prop_within_30_mins, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    351\u001b[0m     consolidation_score_nicu\n\u001b[0;32m    352\u001b[0m ]\n",
                        "File \u001b[1;32mc:\\Users\\Duncan.Passey\\azuredatastudio-python\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:385\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    370\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    372\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    373\u001b[0m     objs,\n\u001b[0;32m    374\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    383\u001b[0m )\n\u001b[1;32m--> 385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\Duncan.Passey\\azuredatastudio-python\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:616\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    612\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    614\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 616\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    620\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
                        "File \u001b[1;32mc:\\Users\\Duncan.Passey\\azuredatastudio-python\\lib\\site-packages\\pandas\\core\\internals\\concat.py:232\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    226\u001b[0m vals \u001b[38;5;241m=\u001b[39m [ju\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_extension:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;66;03m#  than concat_compat\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
                        "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "num_switches = 1\n",
                "periods = ['19/20','20/21','21/22','22/23','23/24']\n",
                "mutation_limits = [20]\n",
                "pop_num = 100\n",
                "restricted_sites = [] \n",
                "include_extreme_individual = False\n",
                "include_original_sites = True\n",
                "elite_pop = int(pop_num * 0.1)\n",
                "proportion_weighted= 0.2\n",
                "base_penalty = 0.1 # for objective feasibility\n",
                "activity_focus = []\n",
                "# activity_focus_list = ['NICU',['HDU', 'SCBU']]\n",
                "activity_focus_list = []\n",
                "objective_stagnation_threshold = 20\n",
                "weighted_mutation = False\n",
                "\n",
                "restricted_mutation_depth = mutation_limits[0]\n",
                "\n",
                "# the maximum number of generations to run the evolution for\n",
                "max_number_generations = 1000\n",
                "# number of generations that the pareto front is stagnant before stopping\n",
                "stagnation_limit = 120 # stagnation before stopping\n",
                "# the number of generations to wait with a stagnant pareto fron before increasing the mutation rate\n",
                "stagnation_threshold = 10 # to increase mutaion rate after x generations\n",
                "\n",
                "start = datetime.now()\n",
                "print(f'Start time: {start}')\n",
                "\n",
                "log_columns = ['Start Time', 'Financial Year', 'Mutation Depth', 'Include Original Sites', 'NSGA Version', \n",
                "               'Scenario', 'Proposed Additions', 'Current Site List', \n",
                "               'Index of Best Individual', 'Fitness of Best Individual']\n",
                "\n",
                "for year in periods:\n",
                "    financial_year = year\n",
                "    start_date, end_date = get_fin_year_dates(financial_year)\n",
                "    \n",
                "    # check the dataset covers the period\n",
                "    if start_date >= startrange and end_date <= endrange:\n",
                "        activities_with_solutions = activities.loc[(activities['CC_Activity_Date'] >= start_date) & (activities['CC_Activity_Date'] <= end_date)].copy().reset_index(drop=True)\n",
                "        \n",
                "        # data_prep\n",
                "        filtered_activities, num_homes, num_sites, most_frequent_sites, home_lsoas, home_activities, home_populations = data_prep(activities, start_date, end_date, site_codes, int_to_activity)\n",
                "        \n",
                "        toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.random_site, num_homes)\n",
                "        \n",
                "        toolbox.register(\"population\", init_population, prop_weighted = proportion_weighted)\n",
                "        \n",
                "        for switch_tuple in itertools.product([False, True], repeat=num_switches):\n",
                "            nsga3 = switch_tuple[0] \n",
                "            # include_original_sites = switch_tuple[1] \n",
                "            nearby_sites = {}\n",
                "            num_elites = elite_pop\n",
                "\n",
                "            run_details_df = pd.DataFrame(columns=log_columns)\n",
                "        \n",
                "            \n",
                "            print(\n",
                "                f\"Run as {financial_year} using \"\n",
                "                f\"{'NSGA3' if nsga3 else 'NSGA2'}\"\n",
                "                f\"{' and EI included' if include_extreme_individual else ''}\"\n",
                "                f\"{' and OI included' if include_original_sites else ''}\"\n",
                "                f\"{f' and mutation distance limit {restricted_mutation_depth}' if restricted_mutation else ''}\"\n",
                "                f\"{f' and {elite_pop} Elites'}\"\n",
                "                )\n",
                "            \n",
                "            now = datetime.now()\n",
                "            timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
                "            deets = generate_detail_string(nsga3=nsga3, restricted_mutation=restricted_mutation,\n",
                "                                           restricted_mutation_depth=restricted_mutation_depth,\n",
                "                                           include_extreme_individual=include_extreme_individual,\n",
                "                                           include_original_sites=include_original_sites)\n",
                "\n",
                "            for solution, objectives in enumerate(scenarios, 1):\n",
                "                # solution_number = solution\n",
                "                scenario = objectives [0]\n",
                "                solution_number_str, _ = scenario.split(':', 1) \n",
                "                solution_number = int(solution_number_str)\n",
                "                activity_limits = objectives [1]\n",
                "                proposed_additions = objectives [2]\n",
                "                restricted_sites = objectives [3]\n",
                "\n",
                "                \n",
                "                # Check and register the appropriate mutation function based on the conditions\n",
                "                if restricted_mutation:\n",
                "                    toolbox.register(\"mutate\", restricted_mutNearbyInt, indpb=individual_mutation_prob_amt, nearby_passed_sites=nearby_sites)\n",
                "                    print(\"        Registered restricted mutation function\")\n",
                "                elif weighted_mutation:\n",
                "                    toolbox.register(\"mutate\", weighted_mutation_function, indpb=individual_mutation_prob_amt\n",
                "                                    , cumulative_probs = cumulative_probs, home_lsoas=home_lsoas\n",
                "                                    ,objective_stagnation_threshold = objective_stagnation_threshold, activity_limits=activity_limits \n",
                "                                    )\n",
                "                    print(\"        Registered weighted mutation function\")\n",
                "                else:\n",
                "                    toolbox.register(\"mutate\", restricted_mutUniformInt, low=0, up=num_sites-1, indpb=individual_mutation_prob\n",
                "                                    ,objective_stagnation_threshold = objective_stagnation_threshold, activity_limits=activity_limits )\n",
                "                    \n",
                "                # toolbox.decorate(\"mate\",   history.decorator)\n",
                "                # toolbox.decorate(\"mutate\", history.decorator)  \n",
                "\n",
                "                for activity in activity_focus_list if activity_focus_list else [None]:\n",
                "\n",
                "                    activity_focus = activity\n",
                "                \n",
                "                    restricted_site_indices = {site_codes.index(code) for code in restricted_sites if code in current_site_list}\n",
                "                    current_site_list = prepare_site_list(site_codes, proposed_additions, restricted_sites)\n",
                "                    nearby_sites = {home: get_nearby_sites(home, len(current_site_list)) for home in home_lsoas}\n",
                "\n",
                "                    if weighted_mutation:\n",
                "                        restricted_mutation_depth = len(current_site_list)\n",
                "\n",
                "                    history = tools.History()\n",
                "                    logs_df = create_logs_df()\n",
                "                    print(f\"    Solution number: {solution_number}\")\n",
                "                    print(f\"        Current site list: {current_site_list}\")\n",
                "                    print(f\"        Scenario: {scenario}\")\n",
                "                    print(f\"        Proposed Additions: {proposed_additions}\")\n",
                "                    print(f\"        Site Objectives: {activity_limits}\")\n",
                "                    print(f\"        Activity Focus: {activity_focus}\")\n",
                "                    \n",
                "                    pop, log, hof, paretofront, best = main()\n",
                "                    best_index = pop.index(best)\n",
                "\n",
                "                    print(f\"        Index of the best individual: {best_index}\")\n",
                "                    print(f\"        Fitness: {pop[best_index].fitness.values}\")\n",
                "\n",
                "                    log_entry = {\n",
                "                                'Start Time': start.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
                "                                'Financial Year': financial_year,\n",
                "                                'Mutation Depth': restricted_mutation_depth,\n",
                "                                'Include Original Sites': include_original_sites,\n",
                "                                'NSGA Version': 'NSGA3' if nsga3 else 'NSGA2',\n",
                "                                'Scenario': scenario,\n",
                "                                'Proposed Additions': proposed_additions,\n",
                "                                'Current Site List': current_site_list,\n",
                "                                'Index of Best Individual': best_index,\n",
                "                                'Fitness of Best Individual': pop[best_index].fitness.values\n",
                "                                }\n",
                "                    \n",
                "                    log_entry_df = pd.DataFrame([log_entry]) \n",
                "                    run_details_df = pd.concat([run_details_df, log_entry_df], ignore_index=True)\n",
                "                    \n",
                "                    # m = plot_assignments_folium(pop[best_index])\n",
                "                    # output_map(m,timestamp,solution_number,deets)\n",
                "                    # display(m)\n",
                "                    \n",
                "                    home_to_site_mapping = create_solution_list(best, home_lsoas, current_site_list)\n",
                "                    activities_with_solutions = add_solution(activities_with_solutions, home_to_site_mapping, solution_number, activity_focus, travel_times_dict)\n",
                "                    export_log(solution_number, timestamp)\n",
                "                \n",
                "            export_solutions(activities_with_solutions, financial_year ,timestamp, deets)\n",
                "            # results = aggregate_results(activities_with_solutions)\n",
                "            # output_results(results, timestamp, deets)\n",
                "\n",
                "            run_details_df.to_csv(f\"./Data_Output/Run_Details_Log_{deets}_{timestamp}.csv\", index=False)\n",
                "\n",
                "            gc.collect()\n",
                "    else:\n",
                "        print(f\"No data present for {start_date} to {end_date}\")    \n",
                "    \n",
                "    gc.collect()  \n",
                "\n",
                "end = datetime.now()\n",
                "duration = end - start\n",
                "print(f'End time: {end}')\n",
                "print(f'Total duration: {duration}')\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
