{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deap import base, creator, tools, algorithms\n",
    "from deap.benchmarks.tools import igd\n",
    "from math import factorial\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import folium\n",
    "import random\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoSeries\n",
    "from functools import partial\n",
    "from statistics import mean\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import itertools\n",
    "from itertools import product\n",
    "import time\n",
    "import os\n",
    "import networkx as nx\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets set up some initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsga3 = True\n",
    "weighted_mutation = False\n",
    "restricted_mutation = True\n",
    "restricted_mutation_depth = 10 # nearest x number of sites by travel times\n",
    "elite_pop = 10\n",
    "include_extreme_individual = False\n",
    "include_original_sites = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a function to turn our financial year into useable dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fin_year_dates(financial_year):\n",
    "    start_year_part, end_year_part = financial_year.split('/')\n",
    "\n",
    "    start_year = int(\"20\" + start_year_part)  \n",
    "    end_year = int(\"20\" + end_year_part)\n",
    "\n",
    "    start_date = pd.Timestamp(f\"{start_year}-04-01\")\n",
    "    end_date = pd.Timestamp(f\"{end_year}-03-31\")\n",
    "\n",
    "    return start_date, end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load our data, firstly our travel times which we have in a pre-calculated table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding file Missing_travel_times_20231117_153542.csv\n",
      "Adding file Missing_travel_times_20231117_163729.csv\n",
      "Adding file Missing_travel_times_20231120_002542.csv\n",
      "Adding file Missing_travel_times_20231120_083042.csv\n",
      "Adding file Missing_travel_times_20231120_083043.csv\n",
      "Adding file Missing_travel_times_20231120_143912.csv\n",
      "Adding file Missing_travel_times_20231121_025324.csv\n",
      "Adding file Missing_travel_times_20231121_075202.csv\n",
      "Adding file Missing_travel_times_20231121_075206.csv\n",
      "Adding file Missing_travel_times_20231122_132507.csv\n",
      "Adding file Missing_travel_times_20231122_151104.csv\n"
     ]
    }
   ],
   "source": [
    "# Read in the travel times data\n",
    "travel_times = pd.read_csv('./LSOA_Travel_Times.csv')\n",
    "travel_times = travel_times.dropna()\n",
    "\n",
    "# Initialize an empty DataFrame to hold the new travel times from CSV files\n",
    "new_travel_times_df = pd.DataFrame()\n",
    "\n",
    "directory = \"./\"\n",
    "\n",
    "# Loop through the files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith(\"Missing_travel_times\") and filename.endswith(\".csv\"):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        # Read the CSV file into a DataFrame\n",
    "        current_df = pd.read_csv(file_path)\n",
    "        \n",
    "        print(f\"Adding file {filename}\")\n",
    "        # Append the current DataFrame to the new travel times DataFrame\n",
    "        new_travel_times_df = new_travel_times_df.append(current_df, ignore_index=True)\n",
    "\n",
    "# Drop any rows with NaN values that may have appeared in the new DataFrame\n",
    "new_travel_times_df = new_travel_times_df.dropna()\n",
    "\n",
    "new_travel_times_df = new_travel_times_df.rename(columns={'Travel_Time': 'TT', 'home_LSOA': 'Home_LSOA'})\n",
    "\n",
    "# Concatenate the existing and new travel times DataFrames\n",
    "combined_travel_times_df = pd.concat([travel_times, new_travel_times_df], ignore_index=True)\n",
    "\n",
    "# Drop duplicates in case some entries are in both DataFrames\n",
    "combined_travel_times_df = combined_travel_times_df.drop_duplicates(subset=['Home_LSOA', 'Site_LSOA'])\n",
    "\n",
    "# Convert the combined DataFrame into a dictionary\n",
    "travel_times_dict = {(row[\"Home_LSOA\"], row[\"Site_LSOA\"]): row[\"TT\"] for _, row in combined_travel_times_df.iterrows()}\n",
    "\n",
    "# Now combined_travel_times_dict contains all the travel times from both sources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load and process our data about our sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load to a data frame\n",
    "sites = pd.read_csv('./Sites.csv', encoding='ISO-8859-1')\n",
    "#remove unnecessary columns\n",
    "sites = sites.loc[:, ['UnitCode', 'LSOA','NICU','LCU','SCBU']]\n",
    "\n",
    "#Apply data cleansing\n",
    "sites = sites.replace('', np.nan)\n",
    "sites = sites.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our activities data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load to a data frame\n",
    "activities = pd.read_csv('./Badgernet_Activity.csv', encoding='ISO-8859-1')\n",
    "\n",
    "#Remove unecessary columns\n",
    "activities_orig = activities.loc[:, ['Der_Postcode_LSOA_Code','CC_Activity_Date','SiteLSOA', 'CC_Level']]\n",
    "activities = activities.loc[:, ['Der_Postcode_LSOA_Code','CC_Activity_Date','SiteLSOA', 'CC_Level']]\n",
    "\n",
    "#Apply data cleansing\n",
    "activities = activities.replace('', np.nan)\n",
    "activities = activities.dropna()\n",
    "\n",
    "# Ensure the date is a date\n",
    "activities['CC_Activity_Date'] = pd.to_datetime(activities['CC_Activity_Date'], format='%d/%m/%Y')\n",
    "activities_indexed = activities.set_index('Der_Postcode_LSOA_Code')\n",
    "\n",
    "# time_periods = pd.date_range(start_date, end_date, freq='D')\n",
    "\n",
    "int_to_activity = {i: activity for i, activity in enumerate(activities['CC_Level'].unique())}\n",
    "\n",
    "def data_prep(activities, start_date, end_date):\n",
    "    filtered_activities = activities.loc[(activities['CC_Activity_Date'] >= start_date) & (activities['CC_Activity_Date'] <= end_date)]\n",
    "    filtered_activities = filtered_activities.set_index('Der_Postcode_LSOA_Code')\n",
    "    home_lsoas = sorted(filtered_activities.index.unique().tolist())\n",
    "    num_homes = len(home_lsoas)\n",
    "    num_sites = len(site_codes)# Group by DER_Postcode_LSOA_Code and count the occurrences\n",
    "    home_populations_dict = filtered_activities.groupby('Der_Postcode_LSOA_Code').size().to_dict()\n",
    "    home_activities_dict = filtered_activities.groupby('Der_Postcode_LSOA_Code')['CC_Level'].value_counts().unstack(fill_value=0).to_dict(orient='index')\n",
    "    home_activities = [[home_activities_dict[home][int_to_activity[i]] for i in range(3)] for home in home_lsoas]\n",
    "    # Convert it to list matching the order of home_lsoas\n",
    "    home_populations = [home_populations_dict.get(home, 0) for home in home_lsoas]\n",
    "    site_frequencies = filtered_activities.groupby(['Der_Postcode_LSOA_Code', 'SiteLSOA']).size().reset_index(name='counts')\n",
    "    most_frequent_sites = site_frequencies.loc[site_frequencies.groupby('Der_Postcode_LSOA_Code')['counts'].idxmax()]\n",
    "    return filtered_activities, num_homes, num_sites, most_frequent_sites, home_lsoas, home_activities, home_populations\n",
    "\n",
    "#Add site code to our df\n",
    "activities = pd.merge(activities, sites[['LSOA','UnitCode']], left_on='SiteLSOA', right_on='LSOA', how='left')\n",
    "activities = activities.drop('LSOA', axis=1)\n",
    "activities.rename(columns={'UnitCode': 'SiteCode'}, inplace=True)\n",
    "\n",
    "\n",
    "# Make a list of all our homes and sites\n",
    "site_codes = sites['LSOA'].unique().tolist()\n",
    "home_codes =  activities_indexed.index.unique().tolist()\n",
    "\n",
    "# print (f\"filtered_activities row count: {len(filtered_activities)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to look up any travel times that might be missing from our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated travel time: 4.70 minutes\n"
     ]
    }
   ],
   "source": [
    "class OutOfAPICallsException(Exception):\n",
    "    \"\"\"Exception raised when the API returns a 403 status code indicating the quota has been exceeded.\"\"\"\n",
    "    pass\n",
    "\n",
    "class NoSuchLocationException(Exception):\n",
    "    \"\"\"Exception raised when the API returns a 404 status code indicating the location hasnt been found.\"\"\"\n",
    "    pass \n",
    "\n",
    "class RateLimitException(Exception):\n",
    "    \"\"\"Exception raised when the API returns a 429 status code indicating too many requests.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_travel_time_openrouteservice(api_key, start_coords, end_coords, api_request_no, transport_mode='driving-car'):\n",
    "    \"\"\" Calculate travel time using the Openrouteservice API. \"\"\"\n",
    "    \n",
    "    url = \"https://api.openrouteservice.org/v2/directions/{}/geojson\".format(transport_mode)\n",
    "    \n",
    "    # Set up the headers with the API key\n",
    "    headers = {\n",
    "        'Authorization': api_key,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    # Set up the parameters with the start and end coordinates\n",
    "    body = {\n",
    "        'coordinates': [start_coords, end_coords]\n",
    "    }\n",
    "    \n",
    "    # Make the request \n",
    "    response = requests.post(url, headers=headers, json=body)\n",
    "    \n",
    "    # Check response\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response\n",
    "        directions = response.json()\n",
    "        try:\n",
    "            # Travel time in seconds is nested in the 'features' list, under the 'properties' dictionary\n",
    "            duration_seconds = directions['features'][0]['properties']['segments'][0]['duration']\n",
    "            return duration_seconds\n",
    "        except (IndexError, KeyError):\n",
    "            print(\"Error parsing the response.\")\n",
    "            return None\n",
    "    elif response.status_code == 403:  # Out of API calls\n",
    "        print(f\"API request {api_request_no} failed with status code {response.status_code}\")\n",
    "        raise OutOfAPICallsException(\"API quota exceeded\")\n",
    "    elif response.status_code == 404:  # Out of API calls\n",
    "        print(f\"API request {api_request_no} failed with location not found {response.status_code}\")\n",
    "        raise NoSuchLocationException(\"No location found\")\n",
    "    elif response.status_code == 429:  # Rate limited by the API\n",
    "        print(f\"API request {api_request_no} has been rate-limited with status code {response.status_code}\")\n",
    "        raise RateLimitException(\"Rate limit exceeded\")\n",
    "    else:\n",
    "        print(f\"API request {api_request_no} failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "api_key = '5b3ce3597851110001cf62486f4bed53db4c47b7a841e3da98655493'\n",
    "start_coordinates = (8.681495, 49.41461)  # Example coordinates (longitude, latitude)\n",
    "end_coordinates = (8.687872, 49.420318)  # Example coordinates (longitude, latitude)\n",
    "transport_mode = 'driving-car'  # Mode of transportation\n",
    "\n",
    "# Calculate travel time\n",
    "travel_time_seconds = calculate_travel_time_openrouteservice(api_key, start_coordinates, end_coordinates, transport_mode)\n",
    "print(f\"Estimated travel time: {travel_time_seconds / 60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSOA_LL_df = pd.read_csv('./LSOA_to_LL.csv')\n",
    "\n",
    "# Create the Cartesian product of home_codes and site_codes\n",
    "combination_product = list(product(home_codes, site_codes))\n",
    "\n",
    "# List to store lat/lng details\n",
    "lat_lng_details = list()\n",
    "\n",
    "LSOA_LL_df\n",
    "\n",
    "# Loop through each combination\n",
    "for home, site in combination_product:\n",
    "    # Check if we have the travel time for this home and site\n",
    "    if (home, site) not in travel_times_dict and home != 'M99999999':\n",
    "        # Filter the DataFrame for the home and check if it's empty\n",
    "        home_rows = LSOA_LL_df[LSOA_LL_df['LSOA'] == home][['Latitude_1m', 'Longitude_1m']]\n",
    "        if not home_rows.empty:\n",
    "            home_lat_lng = home_rows.iloc[0]\n",
    "        else:\n",
    "            # Handle the case where no match is found, for example by continuing to the next iteration\n",
    "            continue\n",
    "\n",
    "        # Filter the DataFrame for the site and check if it's empty\n",
    "        site_rows = LSOA_LL_df[LSOA_LL_df['LSOA'] == site][['Latitude_1m', 'Longitude_1m']]\n",
    "        if not site_rows.empty:\n",
    "            site_lat_lng = site_rows.iloc[0]\n",
    "        else:\n",
    "            # Handle the case where no match is found, for example by continuing to the next iteration\n",
    "            continue\n",
    "        \n",
    "        # Store the details in a dictionary\n",
    "        lat_lng_detail = {\n",
    "            'home_code': home,\n",
    "            'home_latitude': home_lat_lng['Latitude_1m'],\n",
    "            'home_longitude': home_lat_lng['Longitude_1m'],\n",
    "            'site_code': site,\n",
    "            'site_latitude': site_lat_lng['Latitude_1m'],\n",
    "            'site_longitude': site_lat_lng['Longitude_1m']\n",
    "        }\n",
    "        \n",
    "        # Add the dictionary to our list\n",
    "        lat_lng_details.append(lat_lng_detail)\n",
    "        \n",
    "len(lat_lng_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01007251\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01018377\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01018480\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01006512\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01018616\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01025488\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01012457\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01006499\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01006570\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01005062\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01005164\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01005070\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01006370\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01004880\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01005354\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01005801\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01005944\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01019155\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01025300\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01024897\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01012722\n",
      "Empty DataFrame\n",
      "Columns: [home_LSOA, Site_LSOA, Travel_Time]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty DataFrame to store home_LSOA, Site_LSOA, and Travel Time\n",
    "missing_travel_times_df = pd.DataFrame(columns=['home_LSOA', 'Site_LSOA', 'Travel_Time'])\n",
    "\n",
    "# API rate limiting parameters\n",
    "api_request_count = 0\n",
    "api_limit = 2000\n",
    "api_per_minute_limit = 40  # Adjust to per-minute limit\n",
    "delay_between_requests = 60 / api_per_minute_limit  # Delay to adhere to per-minute limit\n",
    "\n",
    "not_found_details = []\n",
    "\n",
    "max_retries = 5\n",
    "\n",
    "# Loop through each home-site pair in the lat_lng_details list\n",
    "for detail in lat_lng_details:\n",
    "    if api_request_count >= api_limit:\n",
    "        # If the API limit is reached, exit the loop\n",
    "        print(\"Stopped due to API quota being exceeded.\")\n",
    "        break\n",
    "\n",
    "    home_LSOA = detail['home_code']\n",
    "    Site_LSOA = detail['site_code']\n",
    "    \n",
    "    # Extract start and end coordinates\n",
    "    start_coords = (detail['home_longitude'], detail['home_latitude'])\n",
    "    end_coords = (detail['site_longitude'], detail['site_latitude'])\n",
    "\n",
    "    success = False  # Flag to check if request was successful\n",
    "    try:\n",
    "        # Calculate travel time with the provided function\n",
    "        travel_time_seconds = calculate_travel_time_openrouteservice(api_key, start_coords, end_coords, api_request_count, transport_mode)\n",
    "\n",
    "        if travel_time_seconds is not None:\n",
    "            travel_time_minutes = round(travel_time_seconds / 60, 1)\n",
    "            # Add the results to the DataFrame\n",
    "            missing_travel_times_df = missing_travel_times_df.append({\n",
    "                'home_LSOA': home_LSOA,\n",
    "                'Site_LSOA': Site_LSOA,\n",
    "                'Travel_Time': travel_time_minutes\n",
    "            }, ignore_index=True)\n",
    "            success = True\n",
    "\n",
    "    except RateLimitException:\n",
    "        print(\"Rate limit hit. Skipping to next pairing.\")\n",
    "    except NoSuchLocationException:\n",
    "        not_found_details.append((home_LSOA, Site_LSOA))\n",
    "        print(f\"Location not found for {home_LSOA} - {Site_LSOA}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected exception occurred: {e}\")\n",
    "\n",
    "    # Increment the request count only if the request was successful\n",
    "    if success:\n",
    "        api_request_count += 1\n",
    "\n",
    "    # Wait the appropriate time before making the next request\n",
    "    if not success:\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "if api_request_count >= api_limit:\n",
    "    print(\"Stopped after reaching API quota.\")\n",
    "\n",
    "print(missing_travel_times_df)\n",
    "\n",
    "def export_travel_times(df):\n",
    "    if len(df) > 0:\n",
    "        now = datetime.now()\n",
    "        timestamp = now.strftime(\"%Y%m%d_%H%M%S\") \n",
    "        # Specify the file name\n",
    "        log_name = f\"./Missing_travel_times_{timestamp}.csv\"\n",
    "        # Save the DataFrame to CSV\n",
    "        df.to_csv(log_name, index=False)\n",
    "        print(f\"Exported file {log_name}\")\n",
    "    \n",
    "# Call the export function\n",
    "export_travel_times(missing_travel_times_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll split these up into daily groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_activities = []\n",
    "# for _, daily_df in filtered_activities.groupby('CC_Activity_Date'):\n",
    "#     daily_activities.append(daily_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and organise our geographic information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the LSOA shape file\n",
    "# lsoas = gpd.read_file('./LSOA_Dec_2011_PWC_in_England_and_Wales/LSOA_Dec_2011_PWC_in_England_and_Wales.shp')\n",
    "\n",
    "# # Make a sites GeoDF\n",
    "# sites_geo_df = lsoas[lsoas['lsoa11cd'].isin(site_codes)]\n",
    "# sites_geo_df = sites_geo_df.set_index('lsoa11cd')\n",
    "# sites_geo_df['centroid'] = sites_geo_df.geometry.centroid\n",
    "\n",
    "# # Make a homes GeoDF\n",
    "# homes_geo_df = lsoas[lsoas['lsoa11cd'].isin(home_codes)]\n",
    "# homes_geo_df = homes_geo_df.set_index('lsoa11cd')\n",
    "# homes_geo_df['centroid'] = homes_geo_df.geometry.centroid\n",
    "\n",
    "# # Extract the centroids as a GeoSeries\n",
    "# homes_centroids = GeoSeries(homes_geo_df['centroid'])\n",
    "# sites_centroids = GeoSeries(sites_geo_df['centroid'])\n",
    "\n",
    "# # Set the CRS of the centroids to EPSG:27700\n",
    "# homes_centroids.crs = \"EPSG:27700\"\n",
    "# sites_centroids.crs = \"EPSG:27700\"\n",
    "\n",
    "# # Convert the centroids to EPSG:4326 (latitude and longitude)\n",
    "# homes_centroids_ll = homes_centroids.to_crs(epsg=4326)\n",
    "# sites_centroids_ll = sites_centroids.to_crs(epsg=4326)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can then plot the assignments on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a map centered at the mean coordinates\n",
    "# center_latitude = homes_centroids_ll.apply(lambda p: p.y).mean()\n",
    "# center_longitude = homes_centroids_ll.apply(lambda p: p.x).mean()\n",
    "# m = folium.Map(location=[center_latitude, center_longitude], zoom_start=8)\n",
    "\n",
    "\n",
    "# # Add the home locations as small red circle markers (adjust radius as needed)\n",
    "# for point in homes_centroids_ll:\n",
    "#     folium.CircleMarker([point.y, point.x], radius=2, color=\"red\", fill=True, fill_colour=\"red\").add_to(m)\n",
    "\n",
    "# # Add the site locations as blue markers\n",
    "# for point in sites_centroids_ll:\n",
    "#     folium.Marker([point.y, point.x], icon=folium.Icon(color=\"blue\")).add_to(m)\n",
    "\n",
    "\n",
    "# # Create a dictionary mapping home and site codes to centroids\n",
    "# home_centroids_mapping = {code: point for code, point in zip(homes_geo_df.index, homes_centroids_ll)}\n",
    "# site_centroids_mapping = {code: point for code, point in zip(sites_geo_df.index, sites_centroids_ll)}\n",
    "\n",
    "# # Plot lines from home to site using the mappings (skip if home or site code is not found)\n",
    "# for home_code, site_code in unique_assignments_list:\n",
    "#     if home_code in home_centroids_mapping and site_code in site_centroids_mapping:\n",
    "#         home_point = home_centroids_mapping[home_code]\n",
    "#         site_point = site_centroids_mapping[site_code]\n",
    "#         coords = [[home_point.y, home_point.x], [site_point.y, site_point.x]]\n",
    "#         folium.PolyLine(coords, color=\"grey\", weight=1, opacity=0.8).add_to(m)\n",
    "\n",
    "# # Display the map\n",
    "# m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_percs = False \n",
    "\n",
    "if calc_percs:\n",
    "    activities_to_rank = filtered_activities.copy()\n",
    "    home_lsoa_codes = sorted(activities_to_rank.index.unique().tolist())\n",
    "\n",
    "    def get_sites(home, num_sites=None):\n",
    "        # If num_sites is None, return all sites\n",
    "        num_sites = num_sites if num_sites is not None else len(site_codes)\n",
    "        # Returns a list of Site_LSOA codes sorted by distance (nearest first)\n",
    "        sorted_sites = sorted(site_codes, key=lambda site: travel_times_dict.get((home, site), float('inf')))\n",
    "        return sorted_sites[:num_sites]\n",
    "\n",
    "    nearby_sites = {home: get_sites(home) for home in home_lsoa_codes}\n",
    "\n",
    "    def determine_site_ranking(data, nearby_sites):\n",
    "        # Add a new column to the data for ranking\n",
    "        data['Ranking'] = None\n",
    "        for index, row in data.iterrows():\n",
    "            home_lsoa = index  # Accessing the index\n",
    "            site_lsoa = row['SiteLSOA']\n",
    "            if home_lsoa in nearby_sites:\n",
    "                try:\n",
    "                    rank = nearby_sites[home_lsoa].index(site_lsoa) + 1  # Adding 1 to start ranking from 1\n",
    "                    data.at[index, 'Ranking'] = rank\n",
    "                except ValueError:\n",
    "                    # Site LSOA not in the list for this home LSOA\n",
    "                    data.at[index, 'Ranking'] = None\n",
    "            else:\n",
    "                data.at[index, 'Ranking'] = None\n",
    "\n",
    "    determine_site_ranking(activities_to_rank, nearby_sites)\n",
    "\n",
    "    def calculate_percentages(data):\n",
    "        ranking_counts = data['Ranking'].value_counts()\n",
    "        total_counts = data['Ranking'].count()  # Count only non-null rankings\n",
    "        percentages = (ranking_counts / total_counts) * 100\n",
    "        return percentages.sort_index()\n",
    "\n",
    "    percentages = calculate_percentages(activities_to_rank)\n",
    "\n",
    "    percentages \n",
    "\n",
    "# 1     65.824697\n",
    "# 2     14.283368\n",
    "# 3      6.887122\n",
    "# 4      3.835668\n",
    "# 5      2.270525\n",
    "# 6      2.157204\n",
    "# 7      0.976367\n",
    "# 8      0.979651\n",
    "# 9      0.283302\n",
    "# 10     0.194616\n",
    "# 11     0.463138\n",
    "# 12     0.156843\n",
    "# 13     0.068978\n",
    "# 14     0.394981\n",
    "# 15     0.918885\n",
    "# 16     0.128923\n",
    "# 17     0.135492\n",
    "# 18     0.000821\n",
    "# 19     0.003285\n",
    "# 20     0.026277\n",
    "# 22     0.009854"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our simple nearest assignment has distinct limitations and does not allow us to balance any other competing priorities. \n",
    "\n",
    "This kind of problem is a variant of the travelling salesman problem (https://en.wikipedia.org/wiki/Travelling_salesman_problem) which is NP-Hard meaning that it is too computationally expensive to compute all possible solutions and find the best one. Therefore needs to be approached and solved using a heuristic approach.\n",
    "\n",
    "For this purpose we are going to use a genetic algorithm to enable us to balance competing priorities and come up with a balanced good solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to define the parameters for our genetic algorithm\n",
    "\n",
    "    * Population Size\n",
    "    * Chance to cross breed\n",
    "    * Mutation Probabilities\n",
    "    * Max number of generations to breed\n",
    "\n",
    "The base mutation rate probability is adaptive and increased based on the stagnation of both the size and diversity of the pareto front, but we can also set the probability that elements of an individual will be mutated once the individual has been selected for mutation.\n",
    "\n",
    "Cessation of the process is also controlled by stagnation in the pareto front, once the mutation rate has been increased and yet still no improvements have been realised in both size and diversity of the pareto front for a number of generations then the evolution is stopped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us set up sone of the parameters for the evolution of our solution\n",
    "# number of solutions in a population\n",
    "pop_num = 200\n",
    "# percentage chance to cross breed one solution with another\n",
    "cross_chance = 0.3\n",
    "# percentage chance to introduce random mutations into the solutions, % of selected individuals\n",
    "initial_mutation_prob = 0.05\n",
    "# maximum percentage chance to introduce random mutations into the solutions, % of selected individuals\n",
    "max_mutation_prob = 0.3\n",
    "# percentage chance to introduce random mutations into the individuals selected for mutation\n",
    "individual_mutation_prob = 0.2\n",
    "# the maximum number of generations to run the evolution for\n",
    "max_number_generations = 1000000\n",
    "# number of generations that the pareto front is stagnant before stopping\n",
    "stagnation_limit = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of homes and sites\n",
    "\n",
    "\n",
    "# activity_to_int = {activity: i for i, activity in enumerate(activities['CC_Level'].unique())} # Not being used\n",
    "\n",
    "\n",
    "\n",
    "#home_populations_dict\n",
    "\n",
    "#home_lsoas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set up and run our evolutionary algorithm. The most important part is the custom evaluation function. Most of the population, generations, breeding and mutating is handled by the DEAP library, but we need to define our own custom function to assess the fitness of each solution. These scores are then used to find the best individual solutions in each generation to breed off and mutate in later generations to evelove the population towards a 'good' solution to our problem with competing priorities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add all our competing priorities in to our evaluation function as per the source paper: https://www.journalslibrary.nihr.ac.uk/hsdr/hsdr06350/#/abstract\n",
    "\n",
    "We want to:\n",
    "\n",
    "    * Minimise the average travel time\n",
    "    * Maximise the proportion within 30 minutes\n",
    "    * Minimise the maximum distance for any assignment\n",
    "    * Maximise the number taking place in units with more than x admissions per year\n",
    "    * Maximise the smallest number of admissions per year  \n",
    "    * Minimise the largest number of admissions per year \n",
    "    * Maximise the proportion within 30 minutes and in units with more than x admissions per year\n",
    "\n",
    "The fourth and final of these are different in this approach as we are not working with admissions data but with critical care information, what we will model instead here is whether a NICU, LNU, and SCBU site meets the minimum required number of days as set out in the BAPM standards https://hubble-live-assets.s3.amazonaws.com/bapm/file_asset/file/1494/BAPM_Service_Quality_Standards_FINAL.pdf and we will look at the proiportion of activities taking place in the nicu sites as a general positive given these sites are the most specialised.\n",
    "\n",
    "So we have:\n",
    "\n",
    "    * Minimise the average travel time\n",
    "    * Maximise the proportion within 30 minutes\n",
    "    * Minimise the maximum distance for any assignment\n",
    "    * Maximise the number taking place in level 3 nicu units\n",
    "    * Maximise the smallest number of admissions per year  \n",
    "    * Minimise the largest number of admissions per year \n",
    "    * Maximise the proportion within 30 minutes and in in level 3 nicu units\n",
    "\n",
    "We can also adjust the weightings that we give to each of these should we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us set up variables for the weightings\n",
    "min_travel_time         = -1.0\n",
    "max_in_30               = 1.0\n",
    "min_max_distance        = -1.0\n",
    "max_large_unit          = 1.0\n",
    "max_min_no              = 1.0\n",
    "min_max_no              = -1.0\n",
    "# constraint_adherence    = -2.0\n",
    "max_in_30_and_large     = 1.0\n",
    "max_large_nicu          = 1.0\n",
    "\n",
    "# Define the threshold for minimum admissions\n",
    "nicu_activities_threshold = 2000  # set to 1000 to make the algorithm reach the threshold of over lnu range and insentivise those solutions\n",
    "lnu_activities_threshold = 1000  \n",
    "scbu_activities_threshold = 500\n",
    "\n",
    "# Using this we can provide particular objectives to our evolutionary process\n",
    "# must be structured like this {\n",
    "#     'E01024897': {'NICU': {'min': 0, 'max': 500}}\n",
    "#     ,'E01005062': {'NICU': {'min': 4000}}\n",
    "#     }\n",
    "# can provide both minimums, maximums to any existing site and any activity level\n",
    "activity_limits = set()\n",
    "\n",
    "# Sites that should not be assigned to any home, for modelling full site closures\n",
    "restricted_sites = set()\n",
    "\n",
    "# Do we want to propose a new site, we can add the LSOA of the proposed site and run our process against it\n",
    "# E01012632 would be blackburn hospital\n",
    "proposed_additions = list()\n",
    "\n",
    "# Activity to focus on in the evolutionary assignment\n",
    "activity_focus = list()\n",
    "\n",
    "# We can also add an extreme individual to the population this is to ensure that the population space contains \n",
    "# the most optimal fitness for one of our evaluation metrics.. in this case the minimisation of travel time\n",
    "include_original_sites = False\n",
    "\n",
    "# Number of elite individuals to carry to the next generation\n",
    "num_elites = elite_pop\n",
    "\n",
    "# normalisation boundaries, these are bnased an pragmatic known results, these could need further evaluation\n",
    "min_avg_time = 10\n",
    "max_avg_time = 200\n",
    "min_prop_within_30_mins = 0.0\n",
    "max_prop_within_30_mins = 1\n",
    "min_min_max_distance = 280\n",
    "max_min_max_distance = 440\n",
    "min_number_of_sites_over_nicu_threshold = 0.2\n",
    "max_number_of_sites_over_nicu_threshold = 0.8\n",
    "min_smallest_site = 4000 \n",
    "max_smallest_site = 14000\n",
    "min_largest_site = 18000 \n",
    "max_largest_site = 36000\n",
    "min_constraint_adherence = 0 \n",
    "max_constraint_adherence = 3000\n",
    "min_prop_within_30_mins_and_large_NICU = 0.05 \n",
    "max_prop_within_30_mins_and_large_NICU = 0.20\n",
    "min_max_large_nicu = 0\n",
    "max_max_large_nicu = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add these priorities in to our evaluation function algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(min_travel_time\n",
    "                                                      , max_in_30\n",
    "                                                      , min_max_distance\n",
    "                                                      , max_large_unit\n",
    "                                                      , max_min_no\n",
    "                                                      , min_max_no\n",
    "                                                    #   , constraint_adherence\n",
    "                                                      , max_in_30_and_large\n",
    "                                                      , max_large_nicu\n",
    "                                                      ))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Function to asign a random site to each individual in the population but allow us to add or remove sites\n",
    "def restricted_random_site():\n",
    "    global proposed_additions\n",
    "    working_site_list = num_sites + len(proposed_additions)\n",
    "    valid_sites = set(range(working_site_list)) - restricted_site_indeces\n",
    "    return random.choice(list(valid_sites))\n",
    "\n",
    "# function to re-index the sites in the individual based on the adjusted list\n",
    "def index_of_site_code(individual):\n",
    "    used_sites = site_codes + proposed_additions\n",
    "    return [used_sites.index(site) for site in individual]\n",
    "\n",
    "restricted_site_indeces = {site_codes.index(code) for code in restricted_sites}\n",
    "\n",
    "toolbox.register(\"random_site\", restricted_random_site)\n",
    "\n",
    "# Create an extreme individual based on the sites in the data using most frequent where more than one site\n",
    "\n",
    "def nearest_allowed_site(home, restricted_site_indices, site_codes, travel_times_dict):\n",
    "    # Find the nearest non-restricted site\n",
    "    valid_sites_indices = [i for i in range(len(site_codes)) if i not in restricted_site_indices]\n",
    "    nearest_site_idx = min(valid_sites_indices, key=lambda site_idx: travel_times_dict.get((home, site_codes[site_idx]), float('inf')))\n",
    "    return nearest_site_idx\n",
    "\n",
    "def create_individual_based_on_data(most_frequent_sites, home_lsoas, site_codes, restricted_site_indices):\n",
    "    site_code_indices = {code: idx for idx, code in enumerate(site_codes)}\n",
    "    \n",
    "    site_index_map = {}\n",
    "    for _, row in most_frequent_sites.iterrows():\n",
    "        home_code = row['Der_Postcode_LSOA_Code']\n",
    "        site_code = row['SiteLSOA']\n",
    "        site_idx = site_code_indices.get(site_code)\n",
    "        \n",
    "        # Check if site is not restricted. Find nearest non restricted site if it is\n",
    "        if site_idx is not None and site_idx not in restricted_site_indices:\n",
    "            site_index_map[home_code] = site_idx\n",
    "        else:\n",
    "            # Assign nearest non-restricted site index\n",
    "            site_index_map[home_code] = nearest_allowed_site(home_code, restricted_site_indices, site_codes, travel_times_dict)\n",
    "\n",
    "    # Build the individual based on the most frequented site index or nearest allowed site\n",
    "    individual = [site_index_map.get(home, nearest_allowed_site(home, restricted_site_indices, site_codes, travel_times_dict)) for home in home_lsoas]\n",
    "\n",
    "    return creator.Individual(individual)\n",
    "\n",
    "def create_extreme_individual():\n",
    "    individual = []\n",
    "    for home_idx, home in enumerate(home_lsoas):\n",
    "        nearest_site_idx = min(range(num_sites), key=lambda site_idx: travel_times_dict.get((home, site_codes[site_idx]), float('inf')))\n",
    "        individual.append(nearest_site_idx)\n",
    "    return creator.Individual(individual)\n",
    "\n",
    "def init_population(n):\n",
    "    population = []\n",
    "    z = 0\n",
    "    # Add the extreme individual if flagged to\n",
    "    if include_extreme_individual:\n",
    "        population.append(create_extreme_individual())\n",
    "        z += 1\n",
    "    # Add the individual based on the actual data if flagged to\n",
    "    if include_original_sites:\n",
    "        population.append(create_individual_based_on_data(most_frequent_sites, home_lsoas, site_codes, restricted_site_indeces))\n",
    "        z += 1\n",
    "    # Fill the rest of the population with individuals with randomly assigned sites\n",
    "    for _ in range(n - z):\n",
    "        population.append(toolbox.individual())\n",
    "    return population\n",
    "\n",
    "toolbox.register(\"population\", init_population)\n",
    "\n",
    "def create_logs_df():\n",
    "    column_types = {'individual': 'str',\n",
    "                    'avg_time': 'float64'\n",
    "                    ,'prop_within_30_mins': 'float64'\n",
    "                    ,'max_distance': 'float64'\n",
    "                    ,'units_over_x': 'float64'\n",
    "                    ,'smallest_site': 'float64'\n",
    "                    ,'largest_site': 'float64'\n",
    "                    ,'max_in_30_and_large': 'float64'\n",
    "                    ,'totals': 'float64'\n",
    "                    ,'large_nicu': 'float64'\n",
    "                    }\n",
    "\n",
    "    # Create a DataFrame with the specified columns and data types\n",
    "    logs_df = pd.DataFrame(columns=column_types.keys()).astype(column_types)\n",
    "    return logs_df\n",
    "\n",
    "inner_log_df = pd.DataFrame(columns=['site',\n",
    "                                    'home',\n",
    "                                    'activity_type',\n",
    "                                    'activity_counts'])\n",
    "\n",
    "activity_log_df = pd.DataFrame(columns=['Generation', 'Site', 'HDU', 'SCBU', 'NICU'])\n",
    "\n",
    "def calculate_activity_counts(individual):\n",
    "    activity_counts = defaultdict(lambda: [0, 0, 0])  # Initialize counts for each activity at each site\n",
    "    used_sites = site_codes + proposed_additions  # Combine existing and proposed sites\n",
    "    # Iterate over each home-site pair\n",
    "    for home_idx, site_idx in enumerate(individual):\n",
    "        site = used_sites[site_idx]  # Get the site assigned to this home\n",
    "        home_activity_counts = home_activities[home_idx]  # Get the activity counts for this home\n",
    "        # Aggregate activities at the assigned site\n",
    "        for i in range(len(home_activity_counts)):\n",
    "            activity_counts[site][i] += home_activity_counts[i]\n",
    "\n",
    "    return activity_counts\n",
    "\n",
    "def is_feasible(individual):\n",
    "    activity_counts = calculate_activity_counts(individual)\n",
    "    for site, counts in activity_counts.items():\n",
    "        if site in activity_limits:\n",
    "            for i, activity in enumerate(['HDU', 'SCBU', 'NICU']):\n",
    "                limits = activity_limits[site].get(activity)\n",
    "                if limits:\n",
    "                    if counts[i] < limits.get('min', 0) or counts[i] > limits.get('max', float('inf')):\n",
    "                        return False\n",
    "    return True\n",
    "\n",
    "def distance_to_feasibility(individual):\n",
    "    distance = 0\n",
    "    activity_counts = calculate_activity_counts(individual)\n",
    "    for site, counts in activity_counts.items():\n",
    "        if site in activity_limits:\n",
    "            for i, activity in enumerate(['HDU', 'SCBU', 'NICU']):\n",
    "                limits = activity_limits[site].get(activity)\n",
    "                if limits:\n",
    "                    excess = max(0, counts[i] - limits.get('max', float('inf')))\n",
    "                    shortfall = max(0, limits.get('min', 0) - counts[i])\n",
    "                    distance += excess + shortfall\n",
    "    return distance\n",
    "\n",
    "base_penalty = 1.0  # Base penalty\n",
    "penalty_factor = 1.1  # Exponential factor\n",
    "\n",
    "# def exponential_penalty(individual):\n",
    "#     distance = distance_to_feasibility(individual)\n",
    "#     penalty_value = base_penalty * (penalty_factor ** distance)\n",
    "\n",
    "#     weights = creator.FitnessMulti.weights\n",
    "\n",
    "#     print(f\"Distance: {distance}, Penalty Value: {penalty_value}\")\n",
    "\n",
    "#     penalties = []\n",
    "#     for weight in weights:\n",
    "#         if weight > 0:  # Penalise maximisation\n",
    "#             penalties.append(-penalty_value)\n",
    "#         else:           # Penalise minimisation\n",
    "#             penalties.append(penalty_value)\n",
    "\n",
    "#     print(f\"Penalties: {penalties}\")\n",
    "#     return tuple(penalties)\n",
    "\n",
    "# def simple_penalty(individual):\n",
    "#     # Just return a fixed penalty for testing\n",
    "#     return (-10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0)\n",
    "\n",
    "# Normalization function in order that no parameter dominations the evolutionary process simply due to its scale\n",
    "def normalize(raw_value, min_value, max_value):\n",
    "    return (raw_value - min_value) / (max_value - min_value)\n",
    "\n",
    "def eval_func(individual, activity_focus=None):\n",
    "    global inner_log_df, logs_df \n",
    "    \n",
    "    # Initialize accumulators and counters\n",
    "    total_time = 0\n",
    "    total_population = 0\n",
    "    within_30_mins = 0\n",
    "    # constraint_adherence = 0\n",
    "    total_time_activity_weighted = 0\n",
    "    total_activity_count = 0\n",
    "\n",
    "    # Combine existing and proposed sites\n",
    "    used_sites = site_codes + proposed_additions\n",
    "\n",
    "    # Calculate activity counts for each site\n",
    "    activity_counts = calculate_activity_counts(individual)\n",
    "\n",
    "    # Loop over each home-site pair in the individual\n",
    "    for home_idx, site_idx in enumerate(individual):\n",
    "        home = home_lsoas[home_idx]\n",
    "        site = used_sites[site_idx]\n",
    "\n",
    "        if (home, site) in travel_times_dict:\n",
    "            travel_time = travel_times_dict[(home, site)]\n",
    "            total_time += travel_time * home_populations[home_idx]\n",
    "            total_population += home_populations[home_idx]\n",
    "\n",
    "            if travel_time <= 30:\n",
    "                within_30_mins += home_populations[home_idx]\n",
    "\n",
    "            activity_counts_per_home = home_activities[home_idx]\n",
    "            for activity_count in activity_counts_per_home:\n",
    "                total_time_activity_weighted += travel_time * home_populations[home_idx] * activity_count\n",
    "                total_activity_count += activity_count\n",
    "\n",
    "    # # Check for activity count violations (constraint adherence)\n",
    "    # for site, counts in activity_counts.items():\n",
    "    #     if site in activity_limits:\n",
    "    #         for i, activity in enumerate(['HDU', 'SCBU', 'NICU']):\n",
    "    #             limit = activity_limits[site].get(activity, float('inf'))\n",
    "    #             constraint_adherence += max(0, counts[i] - limit)\n",
    "\n",
    "    # Calculations for average time, max distance, etc.\n",
    "    avg_time = total_time / total_population if total_population else 0\n",
    "    avg_time_activity_weighted = total_time_activity_weighted / total_activity_count if total_activity_count else 0\n",
    "    prop_within_30_mins = within_30_mins / total_population if total_population else 0\n",
    "    max_distance = max(travel_times_dict.get((home_lsoas[home_idx], used_sites[site_idx]), 0) for home_idx, site_idx in enumerate(individual))\n",
    "\n",
    "    site_activities = {site: sum(counts) for site, counts in activity_counts.items()}\n",
    "    \n",
    "    #print(site_activities)\n",
    "    \n",
    "    smallest_site = min(site_activities.values())\n",
    "    largest_site = max(site_activities.values())\n",
    "    \n",
    "    min_max_values = [\n",
    "        (min_avg_time, max_avg_time), \n",
    "        (min_prop_within_30_mins, max_prop_within_30_mins),\n",
    "        (min_min_max_distance, max_min_max_distance),\n",
    "        (min_number_of_sites_over_nicu_threshold, max_number_of_sites_over_nicu_threshold ),\n",
    "        (min_smallest_site, max_smallest_site),\n",
    "        (min_largest_site, max_largest_site),\n",
    "        (min_prop_within_30_mins_and_large_NICU, max_prop_within_30_mins_and_large_NICU),\n",
    "        (min_max_large_nicu, max_max_large_nicu)\n",
    "    ]\n",
    "    if not site_activities:\n",
    "        return [0] * len(min_max_values)  # Return a list of zeroes for each objective, or handle as appropriate\n",
    "    \n",
    "    # Count the number of sites that meet or exceed the threshold for NICU activities\n",
    "    NICU_INDEX = 2\n",
    "    HDU_INDEX = 0\n",
    "    \n",
    "    # Find the sites that meet the NICU threshold\n",
    "    nicu_sites = [site for site, counts in activity_counts.items() if counts[NICU_INDEX] >= nicu_activities_threshold]\n",
    "    # number_of_sites_over_nicu_threshold = len(nicu_sites)\n",
    "    large_nicu = [counts[NICU_INDEX] for site, counts in activity_counts.items()]\n",
    "    large_nicu_count = max(large_nicu)\n",
    "    \n",
    "    # Calculate the total NICU activity count across all sites\n",
    "    total_nicu_activities = sum(counts[NICU_INDEX] for site, counts in activity_counts.items())\n",
    "    # Calculate the NICU activity count at sites that exceed the threshold\n",
    "    over_threshold_nicu_activities = sum(counts[NICU_INDEX] for site, counts in activity_counts.items() if counts[NICU_INDEX] >= nicu_activities_threshold)\n",
    "    # Calculate the proportion of NICU activities that are at sites over the threshold\n",
    "    proportion_over_threshold_nicu_activities = (over_threshold_nicu_activities / total_nicu_activities \n",
    "                                                if total_nicu_activities != 0 else 0)\n",
    "\n",
    "    # grand_total = sum(sum(values) for key, values in activity_counts.items())\n",
    "    # print(f\"Total Activities Assigned: {grand_total}\")\n",
    "    \n",
    "    # print(individual.id)\n",
    "    # print(activity_counts)\n",
    "    # print(f\"number_of_sites_over_nicu_threshold: {number_of_sites_over_nicu_threshold}\")\n",
    "    # print(f\"nicu_sites: {nicu_sites}\")\n",
    "    \n",
    "    # lnu_sites = [site for site, counts in activity_counts.items() if counts[NICU_INDEX]+counts[HDU_INDEX] >= 1000]\n",
    "\n",
    "    # Calculate the population within 30 minutes and going to a large NICU site\n",
    "    within_30_mins_and_large_NICU = 0\n",
    "    for home_idx, site_idx in enumerate(individual):\n",
    "        home = home_lsoas[home_idx]\n",
    "        site = used_sites[site_idx]\n",
    "        travel_time = travel_times_dict.get((home, site), float('inf'))\n",
    "        if travel_time <= 30 and site in nicu_sites:\n",
    "            within_30_mins_and_large_NICU += home_populations[home_idx]\n",
    "            \n",
    "    # Calculate the proportion (or 0 if total_population is 0)\n",
    "    prop_within_30_mins_and_large_NICU = within_30_mins_and_large_NICU / total_population if total_population != 0 else 0\n",
    "\n",
    "    # Add a new row to the DataFrame\n",
    "    logs_df = logs_df.append({\n",
    "        'individual': individual.index,  \n",
    "        'avg_time': avg_time,\n",
    "        'prop_within_30_mins': prop_within_30_mins,\n",
    "        'max_distance': max_distance,\n",
    "        'units_over_x': proportion_over_threshold_nicu_activities,\n",
    "        'smallest_site': smallest_site,\n",
    "        'largest_site': largest_site,\n",
    "        'totals' : total_population,\n",
    "        'activity_counts': activity_counts,\n",
    "        'large_nicu': large_nicu_count\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    # Raw objective values\n",
    "    raw_objectives = [\n",
    "        avg_time, \n",
    "        prop_within_30_mins, \n",
    "        max_distance, \n",
    "        proportion_over_threshold_nicu_activities,\n",
    "        smallest_site, \n",
    "        largest_site, \n",
    "        prop_within_30_mins_and_large_NICU,\n",
    "        large_nicu_count\n",
    "    ]\n",
    "    \n",
    "    # Normalize objectives\n",
    "    normalized_objectives = [\n",
    "        normalize(raw, min_val, max_val) \n",
    "        for raw, (min_val, max_val) in zip(raw_objectives, min_max_values)\n",
    "    ]\n",
    "\n",
    "    # return (avg_time,\n",
    "    #         prop_within_30_mins,\n",
    "    #         max_distance,\n",
    "    #         proportion_over_threshold_nicu_activities,\n",
    "    #         smallest_site,\n",
    "    #         largest_site,\n",
    "    #         prop_within_30_mins_and_large_NICU,\n",
    "    #         large_nicu_count)\n",
    "            \n",
    "    return normalized_objectives\n",
    "\n",
    "# Random mutation function\n",
    "def restricted_mutUniformInt(individual, low, up, indpb):\n",
    "    for i, site_index in enumerate(individual):\n",
    "        if random.random() < indpb:\n",
    "            individual[i] = restricted_random_site()\n",
    "    return individual,\n",
    "\n",
    "# Let us also create an alternative mutation function which limits the choice of site to one of the 3 nearest rather than any\n",
    "# This should reflect the more realistic real world scenario whereby travel is more limited to nearer sites\n",
    "def get_nearby_sites(home, num_sites=restricted_mutation_depth):\n",
    "    # Returns a list of site indices sorted by distance (nearest first)\n",
    "    sorted_sites = sorted(range(len(site_codes)), key=lambda site_idx: travel_times_dict.get((home, site_codes[site_idx]), float('inf')))\n",
    "    return sorted_sites[:num_sites]\n",
    "\n",
    "\n",
    "def restricted_mutNearbyInt(individual, indpb, nearby_sites):\n",
    "    for i, site_index in enumerate(individual):\n",
    "        if random.random() < indpb:\n",
    "            home = home_lsoas[i]\n",
    "            if home in nearby_sites:\n",
    "                # Choose from the first, second, or third nearest sites\n",
    "                individual[i] = random.choice(nearby_sites[home])\n",
    "            else:\n",
    "                # Fallback to random if nearby info is not available\n",
    "                individual[i] = restricted_random_site()\n",
    "    return individual,\n",
    "\n",
    "# Following there is another alternative mutation assigning nearby sites \n",
    "# based on the real data distribution of sites based on travel times\n",
    "\n",
    "def weighted_random_choice(cumulative_probs):\n",
    "    rnd = random.random()\n",
    "    for i, prob in enumerate(cumulative_probs):\n",
    "        if rnd <= prob:\n",
    "            return i\n",
    "    return len(cumulative_probs) - 1  # Fallback in case of rounding errors\n",
    "\n",
    "def weighted_mutation_function(individual, indpb, cumulative_probs):\n",
    "    for i, _ in enumerate(individual):\n",
    "        if random.random() < indpb:\n",
    "            new_site_index = weighted_random_choice(cumulative_probs)\n",
    "            individual[i] = new_site_index\n",
    "    return individual,\n",
    "\n",
    "# Create a partial function that has activity_focus pre-specified\n",
    "eval_func_focused = partial(eval_func, activity_focus=activity_focus)\n",
    "\n",
    "toolbox.register(\"evaluate\", eval_func_focused)\n",
    "toolbox.decorate(\"evaluate\", tools.DeltaPenalty(is_feasible, 7.0, distance_to_feasibility))\n",
    "\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "\n",
    "# Generate reference points for NSGA3\n",
    "# Parameters\n",
    "NOBJ = 8\n",
    "P = [2, 1]\n",
    "SCALES = [1, 0.5]\n",
    "\n",
    "# Create, combine and removed duplicates\n",
    "ref_points = [tools.uniform_reference_points(NOBJ, p, s) for p, s in zip(P, SCALES)]\n",
    "ref_points = np.concatenate(ref_points, axis=0)\n",
    "_, uniques = np.unique(ref_points, axis=0, return_index=True)\n",
    "ref_points = ref_points[uniques] \n",
    "\n",
    "if nsga3:\n",
    "    toolbox.register(\"select\", tools.selNSGA3, ref_points=ref_points)\n",
    "else:\n",
    "    toolbox.register(\"select\", tools.selNSGA2)\n",
    "\n",
    "history = tools.History()\n",
    "\n",
    "# ADAPTIVE STRATEGY FOR MUTATION RATE\n",
    "stagnation_threshold = 10 # generations\n",
    "\n",
    "def adapt_mutation_rate_based_on_stagnation(generations_since_improvement, threshold, initial_mutation_prob, max_mutation_prob):\n",
    "    if generations_since_improvement > threshold:\n",
    "        # Increase mutation probability up to a maximum\n",
    "        return min(initial_mutation_prob * (1 + generations_since_improvement / threshold), max_mutation_prob)\n",
    "    else:\n",
    "        return initial_mutation_prob\n",
    "    \n",
    "def calculate_diversity(front):\n",
    "    if len(front) < 2:\n",
    "        return 0\n",
    "\n",
    "    distances = []\n",
    "    for i in range(len(front) - 1):\n",
    "        dist = np.linalg.norm(np.array(front[i].fitness.values) - np.array(front[i+1].fitness.values))\n",
    "        distances.append(dist)\n",
    "\n",
    "    return np.mean(distances)\n",
    "\n",
    "def has_pareto_front_improved(current_front, previous_front, diversity_threshold):\n",
    "    if previous_front is None:\n",
    "        return True\n",
    "\n",
    "    current_size = len(current_front)\n",
    "    previous_size = len(previous_front)\n",
    "\n",
    "    if current_size > previous_size:\n",
    "        return True\n",
    "\n",
    "    if current_size > diversity_threshold:\n",
    "        current_diversity = calculate_diversity(current_front)\n",
    "        previous_diversity = calculate_diversity(previous_front)\n",
    "        # print(f\"Current diversity: {current_diversity} > Previous diversity: {previous_diversity}?\")\n",
    "        if current_diversity > previous_diversity:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    global best_fitness\n",
    "\n",
    "    # Define statistics for each objective\n",
    "    stats_time = tools.Statistics(key=lambda ind: ind.fitness.values[0])\n",
    "    stats_time.register(\"avg_time\", np.mean)\n",
    "\n",
    "    stats_prop = tools.Statistics(key=lambda ind: ind.fitness.values[1])\n",
    "    stats_prop.register(\"prop_within_30_mins\", np.max)\n",
    "    \n",
    "    stats_max_distance = tools.Statistics(key=lambda ind: ind.fitness.values[2])\n",
    "    stats_max_distance.register(\"max_distance\", np.mean)\n",
    "    \n",
    "    stats_large_sites = tools.Statistics(key=lambda ind: ind.fitness.values[3])\n",
    "    stats_large_sites.register(\"large_sites\", np.max)\n",
    "    \n",
    "    smallest_site_stats = tools.Statistics(key=lambda ind: ind.fitness.values[4])\n",
    "    smallest_site_stats.register(\"smallest_site\", np.max)\n",
    "    \n",
    "    largest_site_stats = tools.Statistics(key=lambda ind: ind.fitness.values[5])\n",
    "    largest_site_stats.register(\"largest_site\", np.max)\n",
    "    \n",
    "    thirty_and_large_stats = tools.Statistics(key=lambda ind: ind.fitness.values[6])\n",
    "    thirty_and_large_stats.register(\"30_and_large\", np.max)\n",
    "    \n",
    "    large_nicu_stats = tools.Statistics(key=lambda ind: ind.fitness.values[7])\n",
    "    large_nicu_stats.register(\"large_nicu\", np.max)\n",
    "    \n",
    "    # Combine statistics into MultiStatistics\n",
    "    mstats = tools.MultiStatistics(time=stats_time\n",
    "                                   , prop=stats_prop\n",
    "                                   , max_dist=stats_max_distance\n",
    "                                    , large_sites=stats_large_sites\n",
    "                                    ,smallest_site=smallest_site_stats\n",
    "                                    , largest_site=largest_site_stats,\n",
    "                                #    constraint_adherence = constraint_adherence_stats,\n",
    "                                   thirty_and_large = thirty_and_large_stats\n",
    "                                , large_nicu = large_nicu_stats\n",
    "                                   )\n",
    "\n",
    "    # Initialize and evaluate the population\n",
    "    pop = toolbox.population(n=pop_num)\n",
    "    history.update(pop)\n",
    "    hof = tools.HallOfFame(1)\n",
    "    hof2 = tools.ParetoFront()\n",
    "    fitnesses = map(toolbox.evaluate, pop)\n",
    "    for ind, fit in zip(pop, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    # Create a logbook and record initial statistics\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (mstats.fields if mstats else [])\n",
    "    record = mstats.compile(pop) if mstats else {}\n",
    "    logbook.record(gen=0, nevals=len(pop), **record)\n",
    "    \n",
    "    # Function to select elite individuals for crossover\n",
    "    # def ranked_selection(population, k):\n",
    "    #     # Rank the population by fitness\n",
    "    #     sorted_pop = sorted(population, key=lambda ind: ind.fitness, reverse=True)\n",
    "    #     # Select the top k individuals\n",
    "    #     return sorted_pop[:k]\n",
    "\n",
    "    # # Number of individuals to select for crossover\n",
    "    # k = len(pop) // 2\n",
    "    \n",
    "    initial_mutation_prob = 0.05\n",
    "    previous_pareto_front = None\n",
    "    initial_diversity_threshold = 0.10\n",
    "    generations_since_improvement = 0\n",
    "\n",
    "    gen = 0\n",
    "\n",
    "    while generations_since_improvement < stagnation_limit and gen < max_number_generations:\n",
    "    \n",
    "        gen += 1\n",
    "        \n",
    "        # Update hall of fame and Pareto front (hof2)\n",
    "        hof.update(pop)\n",
    "        hof2.update(pop)\n",
    "        \n",
    "        current_pop = len(pop) * gen\n",
    "        diversity_threshold = initial_diversity_threshold * current_pop\n",
    "    \n",
    "        # Check if the Pareto front has improved\n",
    "        if has_pareto_front_improved(hof2, previous_pareto_front, diversity_threshold):\n",
    "            generations_since_improvement = 0\n",
    "            # Store the current Pareto front as the previous front for the next generation\n",
    "            previous_pareto_front = list(hof2)\n",
    "        else:\n",
    "            generations_since_improvement += 1\n",
    "            \n",
    "        mutation_prob = adapt_mutation_rate_based_on_stagnation(generations_since_improvement,stagnation_threshold,initial_mutation_prob,max_mutation_prob)\n",
    "        \n",
    "        # print(f\"Generation: {gen} / Pareto Front Size:{len(hof2)} / Diversity threshold: {diversity_threshold}\")     \n",
    "        # print(f\"Mutation probability {mutation_prob}, at {generations_since_improvement} generations since improvement\")\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(pop, len(pop) - num_elites)\n",
    "        # Clone the selected individuals\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "        \n",
    "        # # Select individuals for crossover\n",
    "        # selected_for_crossover = ranked_selection(pop, k)\n",
    "\n",
    "        # # Apply crossover to elite selected individuals\n",
    "        # for child1, child2 in zip(selected_for_crossover[::2], selected_for_crossover[1::2]):\n",
    "        #     if np.random.rand() < cross_chance:\n",
    "        #         toolbox.mate(child1, child2)\n",
    "        #         del child1.fitness.values\n",
    "        #         del child2.fitness.values\n",
    "\n",
    "        # Apply crossover and mutation on the offspring\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if np.random.rand() < cross_chance:\n",
    "                toolbox.mate(child1, child2)\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "\n",
    "        for mutant in offspring:\n",
    "            if np.random.rand() < mutation_prob:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "                \n",
    "        # Select the elite individuals\n",
    "        elites = tools.selBest(pop, num_elites)\n",
    "        offspring.extend(elites)\n",
    "        pop[:] = offspring\n",
    "        \n",
    "        # Record statistics for this generation\n",
    "        record = mstats.compile(pop) if mstats else {}\n",
    "        logbook.record(gen=gen+1, nevals=len(invalid_ind), **record)\n",
    "        \n",
    "        sys.stdout.write(\"\\rGeneration: {}, Generations Since Improvement: {}  \".format(gen, generations_since_improvement))\n",
    "        sys.stdout.flush()\n",
    "            \n",
    "    bestie = tools.selBest(pop, 1)[0]\n",
    "    print(\" \")\n",
    "    \n",
    "    return pop, logbook, hof, hof2, bestie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use DEAPs built in selBest tool to select the best individual from the population "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we translate the best individual (which is a list of site indices) into a list of (home_code, site_code) pairs\n",
    "def create_solution_list(bestind, home_lsoas, site_codes):\n",
    "    solution = []\n",
    "    used_sites = site_codes + proposed_additions\n",
    "    for i, site_index in enumerate(bestind):\n",
    "        home_code = home_lsoas[i]\n",
    "        site_code = used_sites[site_index]\n",
    "        solution.append((home_code, site_code))\n",
    "    return solution  # return the solution list\n",
    "\n",
    "def add_solution(activities, solution, solution_number, activity_focus):\n",
    "    \n",
    "    solution_column_name = f'solution_{solution_number}'\n",
    "    solution_unit_name = f'solution_{solution_number}_unit'\n",
    "    \n",
    "    # Ensure the solution column exists\n",
    "    if solution_column_name not in activities.columns:\n",
    "        activities[solution_column_name] = np.nan\n",
    "    \n",
    "    # Convert the solution list to a dictionary for faster lookup\n",
    "    solution_dict = dict(solution)\n",
    "    \n",
    "    # Iterate over the activities DataFrame and update where conditions match\n",
    "    for idx, row in activities.iterrows():\n",
    "        if (not activity_focus or row['CC_Level'] in activity_focus) and row['Der_Postcode_LSOA_Code'] in solution_dict:\n",
    "            activities.at[idx, solution_column_name] = solution_dict[row['Der_Postcode_LSOA_Code']]\n",
    "            \n",
    "    # Drop the solution_unit_name column if it exists\n",
    "    if solution_unit_name in activities.columns:\n",
    "        activities = activities.drop(solution_unit_name, axis=1)\n",
    "    \n",
    "    # Merge and then drop the LSOA column, ensuring the merged column name is correct\n",
    "    merged_df = pd.merge(activities, sites[['LSOA', 'UnitCode']], left_on=solution_column_name, right_on='LSOA', how='left')\n",
    "    merged_df = merged_df.drop('LSOA', axis=1)\n",
    "    merged_df.rename(columns={'UnitCode': solution_unit_name}, inplace=True)\n",
    "    \n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_log(solution_id, timestamp):\n",
    "    # now = datetime.now()\n",
    "    # timestamp = now.strftime(\"%Y%m%d_%H%M%S\") \n",
    "    ## Specify the file name\n",
    "    if solution_id:\n",
    "        log_name = f\"./Logs/activities_output_{timestamp}_solution_{solution_id}.csv.gz\"\n",
    "    else:\n",
    "        log_name = f\"./Logs/activities_output_{timestamp}.csv.gz\"\n",
    "    # Save the DataFrame to CSV\n",
    "    logs_df.to_csv(log_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_results(df):\n",
    "      \n",
    "      solution_columns = [col for col in df.columns if 'solution_' in col and '_unit' not in col]\n",
    "\n",
    "      df_melted = df.melt(id_vars=[col for col in df.columns if col not in solution_columns],\n",
    "                        value_vars=solution_columns, \n",
    "                        var_name='SolutionColumn', \n",
    "                        value_name='Solution')\n",
    "\n",
    "      df_melted['SolutionNumber'] = df_melted['SolutionColumn'].apply(lambda x: x.split('_')[1])\n",
    "\n",
    "      df_melted['CC_Activity_Date'] = pd.to_datetime(df_melted['CC_Activity_Date'])\n",
    "      df_melted['Fin_Year'] = pd.cut(df_melted['CC_Activity_Date'], \n",
    "                                    bins=[pd.Timestamp('2018-04-01'), pd.Timestamp('2019-04-01'),\n",
    "                                          pd.Timestamp('2020-04-01'), pd.Timestamp('2021-04-01'),\n",
    "                                          pd.Timestamp('2022-04-01')],\n",
    "                                    labels=['18/19', '19/20', '20/21', '21/22'])\n",
    "\n",
    "      grouped = df_melted.groupby(['Solution', 'SolutionNumber', \n",
    "                                    'CC_Level', 'Fin_Year']).size().reset_index(name='Activity_Count')\n",
    "\n",
    "      sorted_df = grouped.sort_values(by=['SolutionNumber', 'Solution', 'CC_Level', 'Fin_Year'])\n",
    "\n",
    "      final_df = sorted_df.loc[sorted_df['Fin_Year'] == financial_year]\n",
    "\n",
    "      return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_results(results,timestamp):\n",
    "\n",
    "    file_parts = [\"./Data_Output/activities_output_grouped\", financial_year.replace('/', ''), f\"AT_{timestamp}\"]\n",
    "\n",
    "    if nsga3:\n",
    "        file_parts.append(\"NSGA3\")\n",
    "    if not nsga3:\n",
    "        file_parts.append(\"NSGA2\")\n",
    "    if restricted_mutation:\n",
    "        file_parts.append(f\"Site_Limit_{restricted_mutation_depth}\")\n",
    "    if include_extreme_individual:\n",
    "        file_parts.append(\"EI_Inc\")\n",
    "    if include_original_sites:\n",
    "        file_parts.append(\"OI_Inc\")\n",
    "    \n",
    "    file_parts.append(f\"Num_Elites_{num_elites}\")\n",
    "        \n",
    "    file_name = '_'.join(file_parts) + \".csv\"\n",
    "\n",
    "    results.to_csv(file_name, index=False)\n",
    "    \n",
    "    return print(f\"File output: {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then lets run our algorithm and evolve our solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = ['19/20','20/21']\n",
    "num_switches = 3\n",
    "mutation_limits = [3,5,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run as 19/20 using NSGA3 and EI included and OI included and mutation distance limit 3\n",
      "Generation: 34, Generations Since Improvement: 0  "
     ]
    }
   ],
   "source": [
    "for year in periods:\n",
    "    financial_year = year\n",
    "    \n",
    "    start_date, end_date = get_fin_year_dates(financial_year)\n",
    "    activities_with_solutions = activities.loc[(activities['CC_Activity_Date'] >= start_date) & (activities['CC_Activity_Date'] <= end_date)].copy().reset_index(drop=True)\n",
    "    # data_prep\n",
    "    filtered_activities, num_homes, num_sites, most_frequent_sites, home_lsoas, home_activities, home_populations = data_prep(activities, start_date, end_date)\n",
    "    nearby_sites = {home: get_nearby_sites(home) for home in home_lsoas}\n",
    "    \n",
    "    # Check and register the appropriate mutation function based on the conditions\n",
    "    if restricted_mutation:\n",
    "        toolbox.register(\"mutate\", restricted_mutNearbyInt, indpb=individual_mutation_prob, nearby_sites=nearby_sites)\n",
    "    elif weighted_mutation:\n",
    "        toolbox.register(\"mutate\", weighted_mutation_function, indpb=individual_mutation_prob, cumulative_probs=cumulative_probs)\n",
    "    else:\n",
    "        toolbox.register(\"mutate\", restricted_mutUniformInt, low=0, up=num_sites-1, indpb=individual_mutation_prob)\n",
    "        \n",
    "    toolbox.decorate(\"mate\",   history.decorator)\n",
    "    toolbox.decorate(\"mutate\", history.decorator)  \n",
    "    \n",
    "    for combination in itertools.product(mutation_limits,itertools.product([True, False], repeat=num_switches)):\n",
    "        restricted_mutation_depth = combination[0]\n",
    "        switches = combination[1]\n",
    "        nsga3, include_extreme_individual, include_original_sites = switches\n",
    "        \n",
    "        print(\n",
    "            f\"Run as {financial_year} using \"\n",
    "            f\"{'NSGA3' if nsga3 else 'NSGA2'}\"\n",
    "            f\"{' and EI included' if include_extreme_individual else ''}\"\n",
    "            f\"{' and OI included' if include_original_sites else ''}\"\n",
    "            f\"{f' and mutation distance limit {restricted_mutation_depth}' if restricted_mutation else ''}\"\n",
    "            )\n",
    "        \n",
    "        now = datetime.now()\n",
    "        timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        for solution in range(1):\n",
    "            solution_number = solution\n",
    "            toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.random_site, num_homes)\n",
    "            num_elites = elite_pop\n",
    "            activity_focus = list()\n",
    "            activity_limits = set()\n",
    "            restricted_sites = {'E01006570'}\n",
    "            RESTRICTED_SITE_INDICES = {site_codes.index(code) for code in restricted_sites}\n",
    "            proposed_additions = list()\n",
    "            all_sites = site_codes + proposed_additions\n",
    "\n",
    "            history = tools.History()\n",
    "            logs_df = create_logs_df()\n",
    "            pop, log, hof, hof2, best = main()\n",
    "            best_index = pop.index(best)\n",
    "            print(f\"Fitness: {pop[best_index].fitness.values}\")\n",
    "            \n",
    "            home_to_site_mapping = create_solution_list(best, home_lsoas, site_codes)\n",
    "            activities_with_solutions = add_solution(activities_with_solutions, home_to_site_mapping, solution_number, activity_focus)\n",
    "            export_log(solution_number, timestamp)\n",
    "            results = aggregate_results(activities_with_solutions)\n",
    "            output_results(results, timestamp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
