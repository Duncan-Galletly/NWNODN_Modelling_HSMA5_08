{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deap import base, creator, tools, algorithms\n",
    "from deap.benchmarks.tools import igd\n",
    "from math import factorial\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import folium\n",
    "import random\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoSeries\n",
    "from functools import partial\n",
    "from statistics import mean\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from itertools import product\n",
    "import time\n",
    "import os\n",
    "import networkx as nx\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets set up some initial parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsga3 = True\n",
    "weighted_mutation = False\n",
    "restricted_mutation = True\n",
    "restricted_mutation_depth = 10 # nearest x number of sites by travel times\n",
    "elite_pop = 10\n",
    "include_extreme_individual = False\n",
    "include_original_sites = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a function to turn our financial year into useable dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fin_year_dates(financial_year):\n",
    "    start_year_part, end_year_part = financial_year.split('/')\n",
    "\n",
    "    start_year = int(\"20\" + start_year_part)  \n",
    "    end_year = int(\"20\" + end_year_part)\n",
    "\n",
    "    start_date = pd.Timestamp(f\"{start_year}-04-01\")\n",
    "    end_date = pd.Timestamp(f\"{end_year}-03-31\")\n",
    "\n",
    "    return start_date, end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to load our data, firstly our travel times which we have in a pre-calculated table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding file Missing_travel_times_20231117_153542.csv\n",
      "Adding file Missing_travel_times_20231117_163729.csv\n",
      "Adding file Missing_travel_times_20231120_002542.csv\n",
      "Adding file Missing_travel_times_20231120_083042.csv\n",
      "Adding file Missing_travel_times_20231120_083043.csv\n",
      "Adding file Missing_travel_times_20231120_143912.csv\n",
      "Adding file Missing_travel_times_20231121_025324.csv\n",
      "Adding file Missing_travel_times_20231121_075202.csv\n",
      "Adding file Missing_travel_times_20231121_075206.csv\n",
      "Adding file Missing_travel_times_20231122_132507.csv\n",
      "Adding file Missing_travel_times_20231122_151104.csv\n"
     ]
    }
   ],
   "source": [
    "# Read in the travel times data\n",
    "travel_times = pd.read_csv('./LSOA_Travel_Times.csv')\n",
    "travel_times = travel_times.dropna()\n",
    "\n",
    "# Initialize an empty DataFrame to hold the new travel times from CSV files\n",
    "new_travel_times_df = pd.DataFrame()\n",
    "\n",
    "directory = \"./\"\n",
    "\n",
    "# Loop through the files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith(\"Missing_travel_times\") and filename.endswith(\".csv\"):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        # Read the CSV file into a DataFrame\n",
    "        current_df = pd.read_csv(file_path)\n",
    "        \n",
    "        print(f\"Adding file {filename}\")\n",
    "        # Append the current DataFrame to the new travel times DataFrame\n",
    "        new_travel_times_df = new_travel_times_df.append(current_df, ignore_index=True)\n",
    "\n",
    "# Drop any rows with NaN values that may have appeared in the new DataFrame\n",
    "new_travel_times_df = new_travel_times_df.dropna()\n",
    "\n",
    "new_travel_times_df = new_travel_times_df.rename(columns={'Travel_Time': 'TT', 'home_LSOA': 'Home_LSOA'})\n",
    "\n",
    "# Concatenate the existing and new travel times DataFrames\n",
    "combined_travel_times_df = pd.concat([travel_times, new_travel_times_df], ignore_index=True)\n",
    "\n",
    "# Drop duplicates in case some entries are in both DataFrames\n",
    "combined_travel_times_df = combined_travel_times_df.drop_duplicates(subset=['Home_LSOA', 'Site_LSOA'])\n",
    "\n",
    "# Convert the combined DataFrame into a dictionary\n",
    "travel_times_dict = {(row[\"Home_LSOA\"], row[\"Site_LSOA\"]): row[\"TT\"] for _, row in combined_travel_times_df.iterrows()}\n",
    "\n",
    "# Now combined_travel_times_dict contains all the travel times from both sources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load and process our data about our sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load to a data frame\n",
    "sites = pd.read_csv('./Sites.csv', encoding='ISO-8859-1')\n",
    "#remove unnecessary columns\n",
    "sites = sites.loc[:, ['UnitCode', 'LSOA','NICU','LCU','SCBU']]\n",
    "\n",
    "#Apply data cleansing\n",
    "sites = sites.replace('', np.nan)\n",
    "sites = sites.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our activities data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load to a data frame\n",
    "activities = pd.read_csv('./Badgernet_Activity.csv', encoding='ISO-8859-1')\n",
    "\n",
    "#Remove unecessary columns\n",
    "activities_orig = activities.loc[:, ['Der_Postcode_LSOA_Code','CC_Activity_Date','SiteLSOA', 'CC_Level']]\n",
    "activities = activities.loc[:, ['Der_Postcode_LSOA_Code','CC_Activity_Date','SiteLSOA', 'CC_Level']]\n",
    "\n",
    "#Apply data cleansing\n",
    "activities = activities.replace('', np.nan)\n",
    "activities = activities.dropna()\n",
    "\n",
    "# Ensure the date is a date\n",
    "activities['CC_Activity_Date'] = pd.to_datetime(activities['CC_Activity_Date'], format='%d/%m/%Y')\n",
    "activities_indexed = activities.set_index('Der_Postcode_LSOA_Code')\n",
    "\n",
    "# time_periods = pd.date_range(start_date, end_date, freq='D')\n",
    "\n",
    "int_to_activity = {i: activity for i, activity in enumerate(activities['CC_Level'].unique())}\n",
    "\n",
    "def data_prep(activities, start_date, end_date):\n",
    "    filtered_activities = activities.loc[(activities['CC_Activity_Date'] >= start_date) & (activities['CC_Activity_Date'] <= end_date)]\n",
    "    filtered_activities = filtered_activities.set_index('Der_Postcode_LSOA_Code')\n",
    "    home_lsoas = sorted(filtered_activities.index.unique().tolist())\n",
    "    num_homes = len(home_lsoas)\n",
    "    num_sites = len(site_codes)# Group by DER_Postcode_LSOA_Code and count the occurrences\n",
    "    home_populations_dict = filtered_activities.groupby('Der_Postcode_LSOA_Code').size().to_dict()\n",
    "    home_activities_dict = filtered_activities.groupby('Der_Postcode_LSOA_Code')['CC_Level'].value_counts().unstack(fill_value=0).to_dict(orient='index')\n",
    "    home_activities = [[home_activities_dict[home][int_to_activity[i]] for i in range(3)] for home in home_lsoas]\n",
    "    # Convert it to list matching the order of home_lsoas\n",
    "    home_populations = [home_populations_dict.get(home, 0) for home in home_lsoas]\n",
    "    site_frequencies = filtered_activities.groupby(['Der_Postcode_LSOA_Code', 'SiteLSOA']).size().reset_index(name='counts')\n",
    "    most_frequent_sites = site_frequencies.loc[site_frequencies.groupby('Der_Postcode_LSOA_Code')['counts'].idxmax()]\n",
    "    return filtered_activities, num_homes, num_sites, most_frequent_sites, home_lsoas, home_activities, home_populations\n",
    "\n",
    "#Add site code to our df\n",
    "activities = pd.merge(activities, sites[['LSOA','UnitCode']], left_on='SiteLSOA', right_on='LSOA', how='left')\n",
    "activities = activities.drop('LSOA', axis=1)\n",
    "activities.rename(columns={'UnitCode': 'SiteCode'}, inplace=True)\n",
    "\n",
    "\n",
    "# Make a list of all our homes and sites\n",
    "site_codes = sites['LSOA'].unique().tolist()\n",
    "home_codes =  activities_indexed.index.unique().tolist()\n",
    "\n",
    "# print (f\"filtered_activities row count: {len(filtered_activities)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to look up any travel times that might be missing from our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated travel time: 4.70 minutes\n"
     ]
    }
   ],
   "source": [
    "class OutOfAPICallsException(Exception):\n",
    "    \"\"\"Exception raised when the API returns a 403 status code indicating the quota has been exceeded.\"\"\"\n",
    "    pass\n",
    "\n",
    "class NoSuchLocationException(Exception):\n",
    "    \"\"\"Exception raised when the API returns a 404 status code indicating the location hasnt been found.\"\"\"\n",
    "    pass \n",
    "\n",
    "class RateLimitException(Exception):\n",
    "    \"\"\"Exception raised when the API returns a 429 status code indicating too many requests.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_travel_time_openrouteservice(api_key, start_coords, end_coords, api_request_no, transport_mode='driving-car'):\n",
    "    \"\"\" Calculate travel time using the Openrouteservice API. \"\"\"\n",
    "    \n",
    "    url = \"https://api.openrouteservice.org/v2/directions/{}/geojson\".format(transport_mode)\n",
    "    \n",
    "    # Set up the headers with the API key\n",
    "    headers = {\n",
    "        'Authorization': api_key,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    # Set up the parameters with the start and end coordinates\n",
    "    body = {\n",
    "        'coordinates': [start_coords, end_coords]\n",
    "    }\n",
    "    \n",
    "    # Make the request \n",
    "    response = requests.post(url, headers=headers, json=body)\n",
    "    \n",
    "    # Check response\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response\n",
    "        directions = response.json()\n",
    "        try:\n",
    "            # Travel time in seconds is nested in the 'features' list, under the 'properties' dictionary\n",
    "            duration_seconds = directions['features'][0]['properties']['segments'][0]['duration']\n",
    "            return duration_seconds\n",
    "        except (IndexError, KeyError):\n",
    "            print(\"Error parsing the response.\")\n",
    "            return None\n",
    "    elif response.status_code == 403:  # Out of API calls\n",
    "        print(f\"API request {api_request_no} failed with status code {response.status_code}\")\n",
    "        raise OutOfAPICallsException(\"API quota exceeded\")\n",
    "    elif response.status_code == 404:  # Out of API calls\n",
    "        print(f\"API request {api_request_no} failed with location not found {response.status_code}\")\n",
    "        raise NoSuchLocationException(\"No location found\")\n",
    "    elif response.status_code == 429:  # Rate limited by the API\n",
    "        print(f\"API request {api_request_no} has been rate-limited with status code {response.status_code}\")\n",
    "        raise RateLimitException(\"Rate limit exceeded\")\n",
    "    else:\n",
    "        print(f\"API request {api_request_no} failed with status code {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "api_key = '5b3ce3597851110001cf62486f4bed53db4c47b7a841e3da98655493'\n",
    "start_coordinates = (8.681495, 49.41461)  # Example coordinates (longitude, latitude)\n",
    "end_coordinates = (8.687872, 49.420318)  # Example coordinates (longitude, latitude)\n",
    "transport_mode = 'driving-car'  # Mode of transportation\n",
    "\n",
    "# Calculate travel time\n",
    "travel_time_seconds = calculate_travel_time_openrouteservice(api_key, start_coordinates, end_coordinates, transport_mode)\n",
    "print(f\"Estimated travel time: {travel_time_seconds / 60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSOA_LL_df = pd.read_csv('./LSOA_to_LL.csv')\n",
    "\n",
    "# Create the Cartesian product of home_codes and site_codes\n",
    "combination_product = list(product(home_codes, site_codes))\n",
    "\n",
    "# List to store lat/lng details\n",
    "lat_lng_details = list()\n",
    "\n",
    "LSOA_LL_df\n",
    "\n",
    "# Loop through each combination\n",
    "for home, site in combination_product:\n",
    "    # Check if we have the travel time for this home and site\n",
    "    if (home, site) not in travel_times_dict and home != 'M99999999':\n",
    "        # Filter the DataFrame for the home and check if it's empty\n",
    "        home_rows = LSOA_LL_df[LSOA_LL_df['LSOA'] == home][['Latitude_1m', 'Longitude_1m']]\n",
    "        if not home_rows.empty:\n",
    "            home_lat_lng = home_rows.iloc[0]\n",
    "        else:\n",
    "            # Handle the case where no match is found, for example by continuing to the next iteration\n",
    "            continue\n",
    "\n",
    "        # Filter the DataFrame for the site and check if it's empty\n",
    "        site_rows = LSOA_LL_df[LSOA_LL_df['LSOA'] == site][['Latitude_1m', 'Longitude_1m']]\n",
    "        if not site_rows.empty:\n",
    "            site_lat_lng = site_rows.iloc[0]\n",
    "        else:\n",
    "            # Handle the case where no match is found, for example by continuing to the next iteration\n",
    "            continue\n",
    "        \n",
    "        # Store the details in a dictionary\n",
    "        lat_lng_detail = {\n",
    "            'home_code': home,\n",
    "            'home_latitude': home_lat_lng['Latitude_1m'],\n",
    "            'home_longitude': home_lat_lng['Longitude_1m'],\n",
    "            'site_code': site,\n",
    "            'site_latitude': site_lat_lng['Latitude_1m'],\n",
    "            'site_longitude': site_lat_lng['Longitude_1m']\n",
    "        }\n",
    "        \n",
    "        # Add the dictionary to our list\n",
    "        lat_lng_details.append(lat_lng_detail)\n",
    "        \n",
    "len(lat_lng_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01007251\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01018377\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01018480\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01006512\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01018616\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01025488\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01012457\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01006499\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01006570\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01005062\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01005164\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01005070\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01006370\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01004880\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01005354\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01005801\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01005944\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01019155\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01025300\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01024897\n",
      "API request 0 failed with location not found 404\n",
      "Location not found for E01016196 - E01012722\n",
      "Empty DataFrame\n",
      "Columns: [home_LSOA, Site_LSOA, Travel_Time]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty DataFrame to store home_LSOA, Site_LSOA, and Travel Time\n",
    "missing_travel_times_df = pd.DataFrame(columns=['home_LSOA', 'Site_LSOA', 'Travel_Time'])\n",
    "\n",
    "# API rate limiting parameters\n",
    "api_request_count = 0\n",
    "api_limit = 2000\n",
    "api_per_minute_limit = 40  # Adjust to per-minute limit\n",
    "delay_between_requests = 60 / api_per_minute_limit  # Delay to adhere to per-minute limit\n",
    "\n",
    "not_found_details = []\n",
    "\n",
    "max_retries = 5\n",
    "\n",
    "# Loop through each home-site pair in the lat_lng_details list\n",
    "for detail in lat_lng_details:\n",
    "    if api_request_count >= api_limit:\n",
    "        # If the API limit is reached, exit the loop\n",
    "        print(\"Stopped due to API quota being exceeded.\")\n",
    "        break\n",
    "\n",
    "    home_LSOA = detail['home_code']\n",
    "    Site_LSOA = detail['site_code']\n",
    "    \n",
    "    # Extract start and end coordinates\n",
    "    start_coords = (detail['home_longitude'], detail['home_latitude'])\n",
    "    end_coords = (detail['site_longitude'], detail['site_latitude'])\n",
    "\n",
    "    success = False  # Flag to check if request was successful\n",
    "    try:\n",
    "        # Calculate travel time with the provided function\n",
    "        travel_time_seconds = calculate_travel_time_openrouteservice(api_key, start_coords, end_coords, api_request_count, transport_mode)\n",
    "\n",
    "        if travel_time_seconds is not None:\n",
    "            travel_time_minutes = round(travel_time_seconds / 60, 1)\n",
    "            # Add the results to the DataFrame\n",
    "            missing_travel_times_df = missing_travel_times_df.append({\n",
    "                'home_LSOA': home_LSOA,\n",
    "                'Site_LSOA': Site_LSOA,\n",
    "                'Travel_Time': travel_time_minutes\n",
    "            }, ignore_index=True)\n",
    "            success = True\n",
    "\n",
    "    except RateLimitException:\n",
    "        print(\"Rate limit hit. Skipping to next pairing.\")\n",
    "    except NoSuchLocationException:\n",
    "        not_found_details.append((home_LSOA, Site_LSOA))\n",
    "        print(f\"Location not found for {home_LSOA} - {Site_LSOA}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected exception occurred: {e}\")\n",
    "\n",
    "    # Increment the request count only if the request was successful\n",
    "    if success:\n",
    "        api_request_count += 1\n",
    "\n",
    "    # Wait the appropriate time before making the next request\n",
    "    if not success:\n",
    "        time.sleep(delay_between_requests)\n",
    "\n",
    "if api_request_count >= api_limit:\n",
    "    print(\"Stopped after reaching API quota.\")\n",
    "\n",
    "print(missing_travel_times_df)\n",
    "\n",
    "def export_travel_times(df):\n",
    "    if len(df) > 0:\n",
    "        now = datetime.now()\n",
    "        timestamp = now.strftime(\"%Y%m%d_%H%M%S\") \n",
    "        # Specify the file name\n",
    "        log_name = f\"./Missing_travel_times_{timestamp}.csv\"\n",
    "        # Save the DataFrame to CSV\n",
    "        df.to_csv(log_name, index=False)\n",
    "        print(f\"Exported file {log_name}\")\n",
    "    \n",
    "# Call the export function\n",
    "export_travel_times(missing_travel_times_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll split these up into daily groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_activities = []\n",
    "# for _, daily_df in filtered_activities.groupby('CC_Activity_Date'):\n",
    "#     daily_activities.append(daily_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and organise our geographic information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the LSOA shape file\n",
    "# lsoas = gpd.read_file('./LSOA_Dec_2011_PWC_in_England_and_Wales/LSOA_Dec_2011_PWC_in_England_and_Wales.shp')\n",
    "\n",
    "# # Make a sites GeoDF\n",
    "# sites_geo_df = lsoas[lsoas['lsoa11cd'].isin(site_codes)]\n",
    "# sites_geo_df = sites_geo_df.set_index('lsoa11cd')\n",
    "# sites_geo_df['centroid'] = sites_geo_df.geometry.centroid\n",
    "\n",
    "# # Make a homes GeoDF\n",
    "# homes_geo_df = lsoas[lsoas['lsoa11cd'].isin(home_codes)]\n",
    "# homes_geo_df = homes_geo_df.set_index('lsoa11cd')\n",
    "# homes_geo_df['centroid'] = homes_geo_df.geometry.centroid\n",
    "\n",
    "# # Extract the centroids as a GeoSeries\n",
    "# homes_centroids = GeoSeries(homes_geo_df['centroid'])\n",
    "# sites_centroids = GeoSeries(sites_geo_df['centroid'])\n",
    "\n",
    "# # Set the CRS of the centroids to EPSG:27700\n",
    "# homes_centroids.crs = \"EPSG:27700\"\n",
    "# sites_centroids.crs = \"EPSG:27700\"\n",
    "\n",
    "# # Convert the centroids to EPSG:4326 (latitude and longitude)\n",
    "# homes_centroids_ll = homes_centroids.to_crs(epsg=4326)\n",
    "# sites_centroids_ll = sites_centroids.to_crs(epsg=4326)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can then plot the assignments on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a map centered at the mean coordinates\n",
    "# center_latitude = homes_centroids_ll.apply(lambda p: p.y).mean()\n",
    "# center_longitude = homes_centroids_ll.apply(lambda p: p.x).mean()\n",
    "# m = folium.Map(location=[center_latitude, center_longitude], zoom_start=8)\n",
    "\n",
    "\n",
    "# # Add the home locations as small red circle markers (adjust radius as needed)\n",
    "# for point in homes_centroids_ll:\n",
    "#     folium.CircleMarker([point.y, point.x], radius=2, color=\"red\", fill=True, fill_colour=\"red\").add_to(m)\n",
    "\n",
    "# # Add the site locations as blue markers\n",
    "# for point in sites_centroids_ll:\n",
    "#     folium.Marker([point.y, point.x], icon=folium.Icon(color=\"blue\")).add_to(m)\n",
    "\n",
    "\n",
    "# # Create a dictionary mapping home and site codes to centroids\n",
    "# home_centroids_mapping = {code: point for code, point in zip(homes_geo_df.index, homes_centroids_ll)}\n",
    "# site_centroids_mapping = {code: point for code, point in zip(sites_geo_df.index, sites_centroids_ll)}\n",
    "\n",
    "# # Plot lines from home to site using the mappings (skip if home or site code is not found)\n",
    "# for home_code, site_code in unique_assignments_list:\n",
    "#     if home_code in home_centroids_mapping and site_code in site_centroids_mapping:\n",
    "#         home_point = home_centroids_mapping[home_code]\n",
    "#         site_point = site_centroids_mapping[site_code]\n",
    "#         coords = [[home_point.y, home_point.x], [site_point.y, site_point.x]]\n",
    "#         folium.PolyLine(coords, color=\"grey\", weight=1, opacity=0.8).add_to(m)\n",
    "\n",
    "# # Display the map\n",
    "# m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_percs = False \n",
    "\n",
    "if calc_percs:\n",
    "    activities_to_rank = filtered_activities.copy()\n",
    "    home_lsoa_codes = sorted(activities_to_rank.index.unique().tolist())\n",
    "\n",
    "    def get_sites(home, num_sites=None):\n",
    "        # If num_sites is None, return all sites\n",
    "        num_sites = num_sites if num_sites is not None else len(site_codes)\n",
    "        # Returns a list of Site_LSOA codes sorted by distance (nearest first)\n",
    "        sorted_sites = sorted(site_codes, key=lambda site: travel_times_dict.get((home, site), float('inf')))\n",
    "        return sorted_sites[:num_sites]\n",
    "\n",
    "    nearby_sites = {home: get_sites(home) for home in home_lsoa_codes}\n",
    "\n",
    "    def determine_site_ranking(data, nearby_sites):\n",
    "        # Add a new column to the data for ranking\n",
    "        data['Ranking'] = None\n",
    "        for index, row in data.iterrows():\n",
    "            home_lsoa = index  # Accessing the index\n",
    "            site_lsoa = row['SiteLSOA']\n",
    "            if home_lsoa in nearby_sites:\n",
    "                try:\n",
    "                    rank = nearby_sites[home_lsoa].index(site_lsoa) + 1  # Adding 1 to start ranking from 1\n",
    "                    data.at[index, 'Ranking'] = rank\n",
    "                except ValueError:\n",
    "                    # Site LSOA not in the list for this home LSOA\n",
    "                    data.at[index, 'Ranking'] = None\n",
    "            else:\n",
    "                data.at[index, 'Ranking'] = None\n",
    "\n",
    "    determine_site_ranking(activities_to_rank, nearby_sites)\n",
    "\n",
    "    def calculate_percentages(data):\n",
    "        ranking_counts = data['Ranking'].value_counts()\n",
    "        total_counts = data['Ranking'].count()  # Count only non-null rankings\n",
    "        percentages = (ranking_counts / total_counts) * 100\n",
    "        return percentages.sort_index()\n",
    "\n",
    "    percentages = calculate_percentages(activities_to_rank)\n",
    "\n",
    "    percentages \n",
    "\n",
    "# 1     65.824697\n",
    "# 2     14.283368\n",
    "# 3      6.887122\n",
    "# 4      3.835668\n",
    "# 5      2.270525\n",
    "# 6      2.157204\n",
    "# 7      0.976367\n",
    "# 8      0.979651\n",
    "# 9      0.283302\n",
    "# 10     0.194616\n",
    "# 11     0.463138\n",
    "# 12     0.156843\n",
    "# 13     0.068978\n",
    "# 14     0.394981\n",
    "# 15     0.918885\n",
    "# 16     0.128923\n",
    "# 17     0.135492\n",
    "# 18     0.000821\n",
    "# 19     0.003285\n",
    "# 20     0.026277\n",
    "# 22     0.009854"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our simple nearest assignment has distinct limitations and does not allow us to balance any other competing priorities. \n",
    "\n",
    "This kind of problem is a variant of the travelling salesman problem (https://en.wikipedia.org/wiki/Travelling_salesman_problem) which is NP-Hard meaning that it is too computationally expensive to compute all possible solutions and find the best one. Therefore needs to be approached and solved using a heuristic approach.\n",
    "\n",
    "For this purpose we are going to use a genetic algorithm to enable us to balance competing priorities and come up with a balanced good solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to define the parameters for our genetic algorithm\n",
    "\n",
    "    * Population Size\n",
    "    * Chance to cross breed\n",
    "    * Mutation Probabilities\n",
    "    * Max number of generations to breed\n",
    "\n",
    "The base mutation rate probability is adaptive and increased based on the stagnation of both the size and diversity of the pareto front, but we can also set the probability that elements of an individual will be mutated once the individual has been selected for mutation.\n",
    "\n",
    "Cessation of the process is also controlled by stagnation in the pareto front, once the mutation rate has been increased and yet still no improvements have been realised in both size and diversity of the pareto front for a number of generations then the evolution is stopped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us set up sone of the parameters for the evolution of our solution\n",
    "# number of solutions in a population\n",
    "pop_num = 200\n",
    "# percentage chance to cross breed one solution with another\n",
    "cross_chance = 0.3\n",
    "# percentage chance to introduce random mutations into the solutions, % of selected individuals\n",
    "initial_mutation_prob = 0.05\n",
    "# maximum percentage chance to introduce random mutations into the solutions, % of selected individuals\n",
    "max_mutation_prob = 0.3\n",
    "# percentage chance to introduce random mutations into the individuals selected for mutation\n",
    "individual_mutation_prob = 0.2\n",
    "# the maximum number of generations to run the evolution for\n",
    "max_number_generations = 1000000\n",
    "# number of generations that the pareto front is stagnant before stopping\n",
    "stagnation_limit = 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of homes and sites\n",
    "\n",
    "\n",
    "# activity_to_int = {activity: i for i, activity in enumerate(activities['CC_Level'].unique())} # Not being used\n",
    "\n",
    "\n",
    "\n",
    "#home_populations_dict\n",
    "\n",
    "#home_lsoas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set up and run our evolutionary algorithm. The most important part is the custom evaluation function. Most of the population, generations, breeding and mutating is handled by the DEAP library, but we need to define our own custom function to assess the fitness of each solution. These scores are then used to find the best individual solutions in each generation to breed off and mutate in later generations to evelove the population towards a 'good' solution to our problem with competing priorities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add all our competing priorities in to our evaluation function as per the source paper: https://www.journalslibrary.nihr.ac.uk/hsdr/hsdr06350/#/abstract\n",
    "\n",
    "We want to:\n",
    "\n",
    "    * Minimise the average travel time\n",
    "    * Maximise the proportion within 30 minutes\n",
    "    * Minimise the maximum distance for any assignment\n",
    "    * Maximise the number taking place in units with more than x admissions per year\n",
    "    * Maximise the smallest number of admissions per year  \n",
    "    * Minimise the largest number of admissions per year \n",
    "    * Maximise the proportion within 30 minutes and in units with more than x admissions per year\n",
    "\n",
    "The fourth and final of these are different in this approach as we are not working with admissions data but with critical care information, what we will model instead here is whether a NICU, LNU, and SCBU site meets the minimum required number of days as set out in the BAPM standards https://hubble-live-assets.s3.amazonaws.com/bapm/file_asset/file/1494/BAPM_Service_Quality_Standards_FINAL.pdf and we will look at the proiportion of activities taking place in the nicu sites as a general positive given these sites are the most specialised.\n",
    "\n",
    "So we have:\n",
    "\n",
    "    * Minimise the average travel time\n",
    "    * Maximise the proportion within 30 minutes\n",
    "    * Minimise the maximum distance for any assignment\n",
    "    * Maximise the number taking place in level 3 nicu units\n",
    "    * Maximise the smallest number of admissions per year  \n",
    "    * Minimise the largest number of admissions per year \n",
    "    * Maximise the proportion within 30 minutes and in in level 3 nicu units\n",
    "\n",
    "We can also adjust the weightings that we give to each of these should we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us set up variables for the weightings\n",
    "min_travel_time         = -1.0\n",
    "max_in_30               = 1.0\n",
    "min_max_distance        = -1.0\n",
    "max_large_unit          = 1.0\n",
    "max_min_no              = 1.0\n",
    "min_max_no              = -1.0\n",
    "# constraint_adherence    = -2.0\n",
    "max_in_30_and_large     = 1.0\n",
    "max_large_nicu          = 1.0\n",
    "\n",
    "# Define the threshold for minimum admissions\n",
    "nicu_activities_threshold = 2000  # set to 1000 to make the algorithm reach the threshold of over lnu range and insentivise those solutions\n",
    "lnu_activities_threshold = 1000  \n",
    "scbu_activities_threshold = 500\n",
    "\n",
    "# Using this we can provide particular objectives to our evolutionary process\n",
    "# must be structured like this {\n",
    "#     'E01024897': {'NICU': {'min': 0, 'max': 500}}\n",
    "#     ,'E01005062': {'NICU': {'min': 4000}}\n",
    "#     }\n",
    "# can provide both minimums, maximums to any existing site and any activity level\n",
    "activity_limits = set()\n",
    "\n",
    "# Sites that should not be assigned to any home, for modelling full site closures\n",
    "restricted_sites = set()\n",
    "\n",
    "# Do we want to propose a new site, we can add the LSOA of the proposed site and run our process against it\n",
    "# E01012632 would be blackburn hospital\n",
    "proposed_additions = list()\n",
    "\n",
    "# Activity to focus on in the evolutionary assignment\n",
    "activity_focus = list()\n",
    "\n",
    "# We can also add an extreme individual to the population this is to ensure that the population space contains \n",
    "# the most optimal fitness for one of our evaluation metrics.. in this case the minimisation of travel time\n",
    "include_original_sites = False\n",
    "\n",
    "# Number of elite individuals to carry to the next generation\n",
    "num_elites = elite_pop\n",
    "\n",
    "# normalisation boundaries, these are bnased an pragmatic known results, these could need further evaluation\n",
    "min_avg_time = 10\n",
    "max_avg_time = 200\n",
    "min_prop_within_30_mins = 0.0\n",
    "max_prop_within_30_mins = 1\n",
    "min_min_max_distance = 280\n",
    "max_min_max_distance = 440\n",
    "min_number_of_sites_over_nicu_threshold = 0.2\n",
    "max_number_of_sites_over_nicu_threshold = 0.8\n",
    "min_smallest_site = 4000 \n",
    "max_smallest_site = 14000\n",
    "min_largest_site = 18000 \n",
    "max_largest_site = 36000\n",
    "min_constraint_adherence = 0 \n",
    "max_constraint_adherence = 3000\n",
    "min_prop_within_30_mins_and_large_NICU = 0.05 \n",
    "max_prop_within_30_mins_and_large_NICU = 0.20\n",
    "min_max_large_nicu = 0\n",
    "max_max_large_nicu = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add these priorities in to our evaluation function algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(min_travel_time\n",
    "                                                      , max_in_30\n",
    "                                                      , min_max_distance\n",
    "                                                      , max_large_unit\n",
    "                                                      , max_min_no\n",
    "                                                      , min_max_no\n",
    "                                                    #   , constraint_adherence\n",
    "                                                      , max_in_30_and_large\n",
    "                                                      , max_large_nicu\n",
    "                                                      ))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Function to asign a random site to each individual in the population but allow us to add or remove sites\n",
    "def restricted_random_site():\n",
    "    global proposed_additions\n",
    "    working_site_list = num_sites + len(proposed_additions)\n",
    "    valid_sites = set(range(working_site_list)) - restricted_site_indeces\n",
    "    return random.choice(list(valid_sites))\n",
    "\n",
    "# function to re-index the sites in the individual based on the adjusted list\n",
    "def index_of_site_code(individual):\n",
    "    used_sites = site_codes + proposed_additions\n",
    "    return [used_sites.index(site) for site in individual]\n",
    "\n",
    "restricted_site_indeces = {site_codes.index(code) for code in restricted_sites}\n",
    "\n",
    "toolbox.register(\"random_site\", restricted_random_site)\n",
    "\n",
    "# Create an extreme individual based on the sites in the data using most frequent where more than one site\n",
    "\n",
    "def nearest_allowed_site(home, restricted_site_indices, site_codes, travel_times_dict):\n",
    "    # Find the nearest non-restricted site\n",
    "    valid_sites_indices = [i for i in range(len(site_codes)) if i not in restricted_site_indices]\n",
    "    nearest_site_idx = min(valid_sites_indices, key=lambda site_idx: travel_times_dict.get((home, site_codes[site_idx]), float('inf')))\n",
    "    return nearest_site_idx\n",
    "\n",
    "def create_individual_based_on_data(most_frequent_sites, home_lsoas, site_codes, restricted_site_indices):\n",
    "    site_code_indices = {code: idx for idx, code in enumerate(site_codes)}\n",
    "    \n",
    "    site_index_map = {}\n",
    "    for _, row in most_frequent_sites.iterrows():\n",
    "        home_code = row['Der_Postcode_LSOA_Code']\n",
    "        site_code = row['SiteLSOA']\n",
    "        site_idx = site_code_indices.get(site_code)\n",
    "        \n",
    "        # Check if site is not restricted. Find nearest non restricted site if it is\n",
    "        if site_idx is not None and site_idx not in restricted_site_indices:\n",
    "            site_index_map[home_code] = site_idx\n",
    "        else:\n",
    "            # Assign nearest non-restricted site index\n",
    "            site_index_map[home_code] = nearest_allowed_site(home_code, restricted_site_indices, site_codes, travel_times_dict)\n",
    "\n",
    "    # Build the individual based on the most frequented site index or nearest allowed site\n",
    "    individual = [site_index_map.get(home, nearest_allowed_site(home, restricted_site_indices, site_codes, travel_times_dict)) for home in home_lsoas]\n",
    "\n",
    "    return creator.Individual(individual)\n",
    "\n",
    "def create_extreme_individual():\n",
    "    individual = []\n",
    "    for home_idx, home in enumerate(home_lsoas):\n",
    "        nearest_site_idx = min(range(num_sites), key=lambda site_idx: travel_times_dict.get((home, site_codes[site_idx]), float('inf')))\n",
    "        individual.append(nearest_site_idx)\n",
    "    return creator.Individual(individual)\n",
    "\n",
    "def init_population(n):\n",
    "    population = []\n",
    "    z = 0\n",
    "    # Add the extreme individual if flagged to\n",
    "    if include_extreme_individual:\n",
    "        population.append(create_extreme_individual())\n",
    "        z += 1\n",
    "    # Add the individual based on the actual data if flagged to\n",
    "    if include_original_sites:\n",
    "        population.append(create_individual_based_on_data(most_frequent_sites, home_lsoas, site_codes, restricted_site_indeces))\n",
    "        z += 1\n",
    "    # Fill the rest of the population with individuals with randomly assigned sites\n",
    "    for _ in range(n - z):\n",
    "        population.append(toolbox.individual())\n",
    "    return population\n",
    "\n",
    "toolbox.register(\"population\", init_population)\n",
    "\n",
    "def create_logs_df():\n",
    "    column_types = {'individual': 'str',\n",
    "                    'avg_time': 'float64'\n",
    "                    ,'prop_within_30_mins': 'float64'\n",
    "                    ,'max_distance': 'float64'\n",
    "                    ,'units_over_x': 'float64'\n",
    "                    ,'smallest_site': 'float64'\n",
    "                    ,'largest_site': 'float64'\n",
    "                    ,'max_in_30_and_large': 'float64'\n",
    "                    ,'totals': 'float64'\n",
    "                    ,'large_nicu': 'float64'\n",
    "                    }\n",
    "\n",
    "    # Create a DataFrame with the specified columns and data types\n",
    "    logs_df = pd.DataFrame(columns=column_types.keys()).astype(column_types)\n",
    "    return logs_df\n",
    "\n",
    "inner_log_df = pd.DataFrame(columns=['site',\n",
    "                                    'home',\n",
    "                                    'activity_type',\n",
    "                                    'activity_counts'])\n",
    "\n",
    "activity_log_df = pd.DataFrame(columns=['Generation', 'Site', 'HDU', 'SCBU', 'NICU'])\n",
    "\n",
    "def calculate_activity_counts(individual):\n",
    "    activity_counts = defaultdict(lambda: [0, 0, 0])  # Initialize counts for each activity at each site\n",
    "    used_sites = site_codes + proposed_additions  # Combine existing and proposed sites\n",
    "    # Iterate over each home-site pair\n",
    "    for home_idx, site_idx in enumerate(individual):\n",
    "        site = used_sites[site_idx]  # Get the site assigned to this home\n",
    "        home_activity_counts = home_activities[home_idx]  # Get the activity counts for this home\n",
    "        # Aggregate activities at the assigned site\n",
    "        for i in range(len(home_activity_counts)):\n",
    "            activity_counts[site][i] += home_activity_counts[i]\n",
    "\n",
    "    return activity_counts\n",
    "\n",
    "def is_feasible(individual):\n",
    "    activity_counts = calculate_activity_counts(individual)\n",
    "    for site, counts in activity_counts.items():\n",
    "        if site in activity_limits:\n",
    "            for i, activity in enumerate(['HDU', 'SCBU', 'NICU']):\n",
    "                limits = activity_limits[site].get(activity)\n",
    "                if limits:\n",
    "                    if counts[i] < limits.get('min', 0) or counts[i] > limits.get('max', float('inf')):\n",
    "                        return False\n",
    "    return True\n",
    "\n",
    "def distance_to_feasibility(individual):\n",
    "    distance = 0\n",
    "    activity_counts = calculate_activity_counts(individual)\n",
    "    for site, counts in activity_counts.items():\n",
    "        if site in activity_limits:\n",
    "            for i, activity in enumerate(['HDU', 'SCBU', 'NICU']):\n",
    "                limits = activity_limits[site].get(activity)\n",
    "                if limits:\n",
    "                    excess = max(0, counts[i] - limits.get('max', float('inf')))\n",
    "                    shortfall = max(0, limits.get('min', 0) - counts[i])\n",
    "                    distance += excess + shortfall\n",
    "    return distance\n",
    "\n",
    "base_penalty = 1.0  # Base penalty\n",
    "penalty_factor = 1.1  # Exponential factor\n",
    "\n",
    "# def exponential_penalty(individual):\n",
    "#     distance = distance_to_feasibility(individual)\n",
    "#     penalty_value = base_penalty * (penalty_factor ** distance)\n",
    "\n",
    "#     weights = creator.FitnessMulti.weights\n",
    "\n",
    "#     print(f\"Distance: {distance}, Penalty Value: {penalty_value}\")\n",
    "\n",
    "#     penalties = []\n",
    "#     for weight in weights:\n",
    "#         if weight > 0:  # Penalise maximisation\n",
    "#             penalties.append(-penalty_value)\n",
    "#         else:           # Penalise minimisation\n",
    "#             penalties.append(penalty_value)\n",
    "\n",
    "#     print(f\"Penalties: {penalties}\")\n",
    "#     return tuple(penalties)\n",
    "\n",
    "# def simple_penalty(individual):\n",
    "#     # Just return a fixed penalty for testing\n",
    "#     return (-10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0)\n",
    "\n",
    "# Normalization function in order that no parameter dominations the evolutionary process simply due to its scale\n",
    "def normalize(raw_value, min_value, max_value):\n",
    "    return (raw_value - min_value) / (max_value - min_value)\n",
    "\n",
    "def eval_func(individual, activity_focus=None):\n",
    "    global inner_log_df, logs_df \n",
    "    \n",
    "    # Initialize accumulators and counters\n",
    "    total_time = 0\n",
    "    total_population = 0\n",
    "    within_30_mins = 0\n",
    "    # constraint_adherence = 0\n",
    "    total_time_activity_weighted = 0\n",
    "    total_activity_count = 0\n",
    "\n",
    "    # Combine existing and proposed sites\n",
    "    used_sites = site_codes + proposed_additions\n",
    "\n",
    "    # Calculate activity counts for each site\n",
    "    activity_counts = calculate_activity_counts(individual)\n",
    "\n",
    "    # Loop over each home-site pair in the individual\n",
    "    for home_idx, site_idx in enumerate(individual):\n",
    "        home = home_lsoas[home_idx]\n",
    "        site = used_sites[site_idx]\n",
    "\n",
    "        if (home, site) in travel_times_dict:\n",
    "            travel_time = travel_times_dict[(home, site)]\n",
    "            total_time += travel_time * home_populations[home_idx]\n",
    "            total_population += home_populations[home_idx]\n",
    "\n",
    "            if travel_time <= 30:\n",
    "                within_30_mins += home_populations[home_idx]\n",
    "\n",
    "            activity_counts_per_home = home_activities[home_idx]\n",
    "            for activity_count in activity_counts_per_home:\n",
    "                total_time_activity_weighted += travel_time * home_populations[home_idx] * activity_count\n",
    "                total_activity_count += activity_count\n",
    "\n",
    "    # # Check for activity count violations (constraint adherence)\n",
    "    # for site, counts in activity_counts.items():\n",
    "    #     if site in activity_limits:\n",
    "    #         for i, activity in enumerate(['HDU', 'SCBU', 'NICU']):\n",
    "    #             limit = activity_limits[site].get(activity, float('inf'))\n",
    "    #             constraint_adherence += max(0, counts[i] - limit)\n",
    "\n",
    "    # Calculations for average time, max distance, etc.\n",
    "    avg_time = total_time / total_population if total_population else 0\n",
    "    avg_time_activity_weighted = total_time_activity_weighted / total_activity_count if total_activity_count else 0\n",
    "    prop_within_30_mins = within_30_mins / total_population if total_population else 0\n",
    "    max_distance = max(travel_times_dict.get((home_lsoas[home_idx], used_sites[site_idx]), 0) for home_idx, site_idx in enumerate(individual))\n",
    "\n",
    "    site_activities = {site: sum(counts) for site, counts in activity_counts.items()}\n",
    "    \n",
    "    #print(site_activities)\n",
    "    \n",
    "    smallest_site = min(site_activities.values())\n",
    "    largest_site = max(site_activities.values())\n",
    "    \n",
    "    min_max_values = [\n",
    "        (min_avg_time, max_avg_time), \n",
    "        (min_prop_within_30_mins, max_prop_within_30_mins),\n",
    "        (min_min_max_distance, max_min_max_distance),\n",
    "        (min_number_of_sites_over_nicu_threshold, max_number_of_sites_over_nicu_threshold ),\n",
    "        (min_smallest_site, max_smallest_site),\n",
    "        (min_largest_site, max_largest_site),\n",
    "        (min_prop_within_30_mins_and_large_NICU, max_prop_within_30_mins_and_large_NICU),\n",
    "        (min_max_large_nicu, max_max_large_nicu)\n",
    "    ]\n",
    "    if not site_activities:\n",
    "        return [0] * len(min_max_values)  # Return a list of zeroes for each objective, or handle as appropriate\n",
    "    \n",
    "    # Count the number of sites that meet or exceed the threshold for NICU activities\n",
    "    NICU_INDEX = 2\n",
    "    HDU_INDEX = 0\n",
    "    \n",
    "    # Find the sites that meet the NICU threshold\n",
    "    nicu_sites = [site for site, counts in activity_counts.items() if counts[NICU_INDEX] >= nicu_activities_threshold]\n",
    "    # number_of_sites_over_nicu_threshold = len(nicu_sites)\n",
    "    large_nicu = [counts[NICU_INDEX] for site, counts in activity_counts.items()]\n",
    "    large_nicu_count = max(large_nicu)\n",
    "    \n",
    "    # Calculate the total NICU activity count across all sites\n",
    "    total_nicu_activities = sum(counts[NICU_INDEX] for site, counts in activity_counts.items())\n",
    "    # Calculate the NICU activity count at sites that exceed the threshold\n",
    "    over_threshold_nicu_activities = sum(counts[NICU_INDEX] for site, counts in activity_counts.items() if counts[NICU_INDEX] >= nicu_activities_threshold)\n",
    "    # Calculate the proportion of NICU activities that are at sites over the threshold\n",
    "    proportion_over_threshold_nicu_activities = (over_threshold_nicu_activities / total_nicu_activities \n",
    "                                                if total_nicu_activities != 0 else 0)\n",
    "\n",
    "    # grand_total = sum(sum(values) for key, values in activity_counts.items())\n",
    "    # print(f\"Total Activities Assigned: {grand_total}\")\n",
    "    \n",
    "    # print(individual.id)\n",
    "    # print(activity_counts)\n",
    "    # print(f\"number_of_sites_over_nicu_threshold: {number_of_sites_over_nicu_threshold}\")\n",
    "    # print(f\"nicu_sites: {nicu_sites}\")\n",
    "    \n",
    "    # lnu_sites = [site for site, counts in activity_counts.items() if counts[NICU_INDEX]+counts[HDU_INDEX] >= 1000]\n",
    "\n",
    "    # Calculate the population within 30 minutes and going to a large NICU site\n",
    "    within_30_mins_and_large_NICU = 0\n",
    "    for home_idx, site_idx in enumerate(individual):\n",
    "        home = home_lsoas[home_idx]\n",
    "        site = used_sites[site_idx]\n",
    "        travel_time = travel_times_dict.get((home, site), float('inf'))\n",
    "        if travel_time <= 30 and site in nicu_sites:\n",
    "            within_30_mins_and_large_NICU += home_populations[home_idx]\n",
    "            \n",
    "    # Calculate the proportion (or 0 if total_population is 0)\n",
    "    prop_within_30_mins_and_large_NICU = within_30_mins_and_large_NICU / total_population if total_population != 0 else 0\n",
    "\n",
    "    # Add a new row to the DataFrame\n",
    "    logs_df = logs_df.append({\n",
    "        'individual': individual.index,  \n",
    "        'avg_time': avg_time,\n",
    "        'prop_within_30_mins': prop_within_30_mins,\n",
    "        'max_distance': max_distance,\n",
    "        'units_over_x': proportion_over_threshold_nicu_activities,\n",
    "        'smallest_site': smallest_site,\n",
    "        'largest_site': largest_site,\n",
    "        'totals' : total_population,\n",
    "        'activity_counts': activity_counts,\n",
    "        'large_nicu': large_nicu_count\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    # Raw objective values\n",
    "    raw_objectives = [\n",
    "        avg_time, \n",
    "        prop_within_30_mins, \n",
    "        max_distance, \n",
    "        proportion_over_threshold_nicu_activities,\n",
    "        smallest_site, \n",
    "        largest_site, \n",
    "        prop_within_30_mins_and_large_NICU,\n",
    "        large_nicu_count\n",
    "    ]\n",
    "    \n",
    "    # Normalize objectives\n",
    "    normalized_objectives = [\n",
    "        normalize(raw, min_val, max_val) \n",
    "        for raw, (min_val, max_val) in zip(raw_objectives, min_max_values)\n",
    "    ]\n",
    "\n",
    "    # return (avg_time,\n",
    "    #         prop_within_30_mins,\n",
    "    #         max_distance,\n",
    "    #         proportion_over_threshold_nicu_activities,\n",
    "    #         smallest_site,\n",
    "    #         largest_site,\n",
    "    #         prop_within_30_mins_and_large_NICU,\n",
    "    #         large_nicu_count)\n",
    "            \n",
    "    return normalized_objectives\n",
    "\n",
    "# Random mutation function\n",
    "def restricted_mutUniformInt(individual, low, up, indpb):\n",
    "    for i, site_index in enumerate(individual):\n",
    "        if random.random() < indpb:\n",
    "            individual[i] = restricted_random_site()\n",
    "    return individual,\n",
    "\n",
    "# Let us also create an alternative mutation function which limits the choice of site to one of the 3 nearest rather than any\n",
    "# This should reflect the more realistic real world scenario whereby travel is more limited to nearer sites\n",
    "def get_nearby_sites(home, num_sites=restricted_mutation_depth):\n",
    "    # Returns a list of site indices sorted by distance (nearest first)\n",
    "    sorted_sites = sorted(range(len(site_codes)), key=lambda site_idx: travel_times_dict.get((home, site_codes[site_idx]), float('inf')))\n",
    "    return sorted_sites[:num_sites]\n",
    "\n",
    "\n",
    "def restricted_mutNearbyInt(individual, indpb, nearby_sites):\n",
    "    for i, site_index in enumerate(individual):\n",
    "        if random.random() < indpb:\n",
    "            home = home_lsoas[i]\n",
    "            if home in nearby_sites:\n",
    "                # Choose from the first, second, or third nearest sites\n",
    "                individual[i] = random.choice(nearby_sites[home])\n",
    "            else:\n",
    "                # Fallback to random if nearby info is not available\n",
    "                individual[i] = restricted_random_site()\n",
    "    return individual,\n",
    "\n",
    "# Following there is another alternative mutation assigning nearby sites \n",
    "# based on the real data distribution of sites based on travel times\n",
    "\n",
    "def weighted_random_choice(cumulative_probs):\n",
    "    rnd = random.random()\n",
    "    for i, prob in enumerate(cumulative_probs):\n",
    "        if rnd <= prob:\n",
    "            return i\n",
    "    return len(cumulative_probs) - 1  # Fallback in case of rounding errors\n",
    "\n",
    "def weighted_mutation_function(individual, indpb, cumulative_probs):\n",
    "    for i, _ in enumerate(individual):\n",
    "        if random.random() < indpb:\n",
    "            new_site_index = weighted_random_choice(cumulative_probs)\n",
    "            individual[i] = new_site_index\n",
    "    return individual,\n",
    "\n",
    "# Create a partial function that has activity_focus pre-specified\n",
    "eval_func_focused = partial(eval_func, activity_focus=activity_focus)\n",
    "\n",
    "toolbox.register(\"evaluate\", eval_func_focused)\n",
    "toolbox.decorate(\"evaluate\", tools.DeltaPenalty(is_feasible, 7.0, distance_to_feasibility))\n",
    "\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "\n",
    "# Generate reference points for NSGA3\n",
    "# Parameters\n",
    "NOBJ = 8\n",
    "P = [2, 1]\n",
    "SCALES = [1, 0.5]\n",
    "\n",
    "# Create, combine and removed duplicates\n",
    "ref_points = [tools.uniform_reference_points(NOBJ, p, s) for p, s in zip(P, SCALES)]\n",
    "ref_points = np.concatenate(ref_points, axis=0)\n",
    "_, uniques = np.unique(ref_points, axis=0, return_index=True)\n",
    "ref_points = ref_points[uniques] \n",
    "\n",
    "if nsga3:\n",
    "    toolbox.register(\"select\", tools.selNSGA3, ref_points=ref_points)\n",
    "else:\n",
    "    toolbox.register(\"select\", tools.selNSGA2)\n",
    "\n",
    "history = tools.History()\n",
    "\n",
    "# ADAPTIVE STRATEGY FOR MUTATION RATE\n",
    "stagnation_threshold = 10 # generations\n",
    "\n",
    "def adapt_mutation_rate_based_on_stagnation(generations_since_improvement, threshold, initial_mutation_prob, max_mutation_prob):\n",
    "    if generations_since_improvement > threshold:\n",
    "        # Increase mutation probability up to a maximum\n",
    "        return min(initial_mutation_prob * (1 + generations_since_improvement / threshold), max_mutation_prob)\n",
    "    else:\n",
    "        return initial_mutation_prob\n",
    "    \n",
    "def calculate_diversity(front):\n",
    "    if len(front) < 2:\n",
    "        return 0\n",
    "\n",
    "    distances = []\n",
    "    for i in range(len(front) - 1):\n",
    "        dist = np.linalg.norm(np.array(front[i].fitness.values) - np.array(front[i+1].fitness.values))\n",
    "        distances.append(dist)\n",
    "\n",
    "    return np.mean(distances)\n",
    "\n",
    "def has_pareto_front_improved(current_front, previous_front, diversity_threshold):\n",
    "    if previous_front is None:\n",
    "        return True\n",
    "\n",
    "    current_size = len(current_front)\n",
    "    previous_size = len(previous_front)\n",
    "\n",
    "    if current_size > previous_size:\n",
    "        return True\n",
    "\n",
    "    if current_size > diversity_threshold:\n",
    "        current_diversity = calculate_diversity(current_front)\n",
    "        previous_diversity = calculate_diversity(previous_front)\n",
    "        # print(f\"Current diversity: {current_diversity} > Previous diversity: {previous_diversity}?\")\n",
    "        if current_diversity > previous_diversity:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    global best_fitness\n",
    "\n",
    "    # Define statistics for each objective\n",
    "    stats_time = tools.Statistics(key=lambda ind: ind.fitness.values[0])\n",
    "    stats_time.register(\"avg_time\", np.mean)\n",
    "\n",
    "    stats_prop = tools.Statistics(key=lambda ind: ind.fitness.values[1])\n",
    "    stats_prop.register(\"prop_within_30_mins\", np.max)\n",
    "    \n",
    "    stats_max_distance = tools.Statistics(key=lambda ind: ind.fitness.values[2])\n",
    "    stats_max_distance.register(\"max_distance\", np.mean)\n",
    "    \n",
    "    stats_large_sites = tools.Statistics(key=lambda ind: ind.fitness.values[3])\n",
    "    stats_large_sites.register(\"large_sites\", np.max)\n",
    "    \n",
    "    smallest_site_stats = tools.Statistics(key=lambda ind: ind.fitness.values[4])\n",
    "    smallest_site_stats.register(\"smallest_site\", np.max)\n",
    "    \n",
    "    largest_site_stats = tools.Statistics(key=lambda ind: ind.fitness.values[5])\n",
    "    largest_site_stats.register(\"largest_site\", np.max)\n",
    "    \n",
    "    thirty_and_large_stats = tools.Statistics(key=lambda ind: ind.fitness.values[6])\n",
    "    thirty_and_large_stats.register(\"30_and_large\", np.max)\n",
    "    \n",
    "    large_nicu_stats = tools.Statistics(key=lambda ind: ind.fitness.values[7])\n",
    "    large_nicu_stats.register(\"large_nicu\", np.max)\n",
    "    \n",
    "    # Combine statistics into MultiStatistics\n",
    "    mstats = tools.MultiStatistics(time=stats_time\n",
    "                                   , prop=stats_prop\n",
    "                                   , max_dist=stats_max_distance\n",
    "                                    , large_sites=stats_large_sites\n",
    "                                    ,smallest_site=smallest_site_stats\n",
    "                                    , largest_site=largest_site_stats,\n",
    "                                #    constraint_adherence = constraint_adherence_stats,\n",
    "                                   thirty_and_large = thirty_and_large_stats\n",
    "                                , large_nicu = large_nicu_stats\n",
    "                                   )\n",
    "\n",
    "    # Initialize and evaluate the population\n",
    "    pop = toolbox.population(n=pop_num)\n",
    "    history.update(pop)\n",
    "    hof = tools.HallOfFame(1)\n",
    "    hof2 = tools.ParetoFront()\n",
    "    fitnesses = map(toolbox.evaluate, pop)\n",
    "    for ind, fit in zip(pop, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    # Create a logbook and record initial statistics\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = ['gen', 'nevals'] + (mstats.fields if mstats else [])\n",
    "    record = mstats.compile(pop) if mstats else {}\n",
    "    logbook.record(gen=0, nevals=len(pop), **record)\n",
    "    \n",
    "    # Function to select elite individuals for crossover\n",
    "    # def ranked_selection(population, k):\n",
    "    #     # Rank the population by fitness\n",
    "    #     sorted_pop = sorted(population, key=lambda ind: ind.fitness, reverse=True)\n",
    "    #     # Select the top k individuals\n",
    "    #     return sorted_pop[:k]\n",
    "\n",
    "    # # Number of individuals to select for crossover\n",
    "    # k = len(pop) // 2\n",
    "    \n",
    "    initial_mutation_prob = 0.05\n",
    "    previous_pareto_front = None\n",
    "    initial_diversity_threshold = 0.10\n",
    "    generations_since_improvement = 0\n",
    "\n",
    "    gen = 0\n",
    "\n",
    "    while generations_since_improvement < stagnation_limit and gen < max_number_generations:\n",
    "    \n",
    "        gen += 1\n",
    "        \n",
    "        # Update hall of fame and Pareto front (hof2)\n",
    "        hof.update(pop)\n",
    "        hof2.update(pop)\n",
    "        \n",
    "        current_pop = len(pop) * gen\n",
    "        diversity_threshold = initial_diversity_threshold * current_pop\n",
    "    \n",
    "        # Check if the Pareto front has improved\n",
    "        if has_pareto_front_improved(hof2, previous_pareto_front, diversity_threshold):\n",
    "            generations_since_improvement = 0\n",
    "            # Store the current Pareto front as the previous front for the next generation\n",
    "            previous_pareto_front = list(hof2)\n",
    "        else:\n",
    "            generations_since_improvement += 1\n",
    "            \n",
    "        mutation_prob = adapt_mutation_rate_based_on_stagnation(generations_since_improvement,stagnation_threshold,initial_mutation_prob,max_mutation_prob)\n",
    "        \n",
    "        # print(f\"Generation: {gen} / Pareto Front Size:{len(hof2)} / Diversity threshold: {diversity_threshold}\")     \n",
    "        # print(f\"Mutation probability {mutation_prob}, at {generations_since_improvement} generations since improvement\")\n",
    "\n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(pop, len(pop) - num_elites)\n",
    "        # Clone the selected individuals\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "        \n",
    "        # # Select individuals for crossover\n",
    "        # selected_for_crossover = ranked_selection(pop, k)\n",
    "\n",
    "        # # Apply crossover to elite selected individuals\n",
    "        # for child1, child2 in zip(selected_for_crossover[::2], selected_for_crossover[1::2]):\n",
    "        #     if np.random.rand() < cross_chance:\n",
    "        #         toolbox.mate(child1, child2)\n",
    "        #         del child1.fitness.values\n",
    "        #         del child2.fitness.values\n",
    "\n",
    "        # Apply crossover and mutation on the offspring\n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if np.random.rand() < cross_chance:\n",
    "                toolbox.mate(child1, child2)\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "\n",
    "        for mutant in offspring:\n",
    "            if np.random.rand() < mutation_prob:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "\n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "                \n",
    "        # Select the elite individuals\n",
    "        elites = tools.selBest(pop, num_elites)\n",
    "        offspring.extend(elites)\n",
    "        pop[:] = offspring\n",
    "        \n",
    "        # Record statistics for this generation\n",
    "        record = mstats.compile(pop) if mstats else {}\n",
    "        logbook.record(gen=gen+1, nevals=len(invalid_ind), **record)\n",
    "        \n",
    "        sys.stdout.write(\"\\rGeneration: {}, Generations Since Improvement: {}\".format(gen, generations_since_improvement))\n",
    "        sys.stdout.flush()\n",
    "            \n",
    "    bestie = tools.selBest(pop, 1)[0]\n",
    "    # print(gen)\n",
    "    \n",
    "    return pop, logbook, hof, hof2, bestie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use DEAPs built in selBest tool to select the best individual from the population "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we translate the best individual (which is a list of site indices) into a list of (home_code, site_code) pairs\n",
    "def create_solution_list(bestind, home_lsoas, site_codes):\n",
    "    solution = []\n",
    "    used_sites = site_codes + proposed_additions\n",
    "    for i, site_index in enumerate(bestind):\n",
    "        home_code = home_lsoas[i]\n",
    "        site_code = used_sites[site_index]\n",
    "        solution.append((home_code, site_code))\n",
    "    return solution  # return the solution list\n",
    "\n",
    "def add_solution(activities, solution, solution_number, activity_focus):\n",
    "    \n",
    "    solution_column_name = f'solution_{solution_number}'\n",
    "    solution_unit_name = f'solution_{solution_number}_unit'\n",
    "    \n",
    "    # Ensure the solution column exists\n",
    "    if solution_column_name not in activities.columns:\n",
    "        activities[solution_column_name] = np.nan\n",
    "    \n",
    "    # Convert the solution list to a dictionary for faster lookup\n",
    "    solution_dict = dict(solution)\n",
    "    \n",
    "    # Iterate over the activities DataFrame and update where conditions match\n",
    "    for idx, row in activities.iterrows():\n",
    "        if (not activity_focus or row['CC_Level'] in activity_focus) and row['Der_Postcode_LSOA_Code'] in solution_dict:\n",
    "            activities.at[idx, solution_column_name] = solution_dict[row['Der_Postcode_LSOA_Code']]\n",
    "            \n",
    "    # Drop the solution_unit_name column if it exists\n",
    "    if solution_unit_name in activities.columns:\n",
    "        activities = activities.drop(solution_unit_name, axis=1)\n",
    "    \n",
    "    # Merge and then drop the LSOA column, ensuring the merged column name is correct\n",
    "    merged_df = pd.merge(activities, sites[['LSOA', 'UnitCode']], left_on=solution_column_name, right_on='LSOA', how='left')\n",
    "    merged_df = merged_df.drop('LSOA', axis=1)\n",
    "    merged_df.rename(columns={'UnitCode': solution_unit_name}, inplace=True)\n",
    "    \n",
    "    return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_log(solution_id):\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%Y%m%d_%H%M%S\") \n",
    "    ## Specify the file name\n",
    "    if solution_id:\n",
    "        log_name = f\"./Logs/activities_output_{timestamp}_solution_{solution_id}.csv.gz\"\n",
    "    else:\n",
    "        log_name = f\"./Logs/activities_output_{timestamp}.csv.gz\"\n",
    "    # Save the DataFrame to CSV\n",
    "    logs_df.to_csv(log_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_results(df):\n",
    "      \n",
    "      solution_columns = [col for col in df.columns if 'solution_' in col and '_unit' not in col]\n",
    "\n",
    "      df_melted = df.melt(id_vars=[col for col in df.columns if col not in solution_columns],\n",
    "                        value_vars=solution_columns, \n",
    "                        var_name='SolutionColumn', \n",
    "                        value_name='Solution')\n",
    "\n",
    "      df_melted['SolutionNumber'] = df_melted['SolutionColumn'].apply(lambda x: x.split('_')[1])\n",
    "\n",
    "      df_melted['CC_Activity_Date'] = pd.to_datetime(df_melted['CC_Activity_Date'])\n",
    "      df_melted['Fin_Year'] = pd.cut(df_melted['CC_Activity_Date'], \n",
    "                                    bins=[pd.Timestamp('2018-04-01'), pd.Timestamp('2019-04-01'),\n",
    "                                          pd.Timestamp('2020-04-01'), pd.Timestamp('2021-04-01'),\n",
    "                                          pd.Timestamp('2022-04-01')],\n",
    "                                    labels=['18/19', '19/20', '20/21', '21/22'])\n",
    "\n",
    "      grouped = df_melted.groupby(['Solution', 'SolutionNumber', \n",
    "                                    'CC_Level', 'Fin_Year']).size().reset_index(name='Activity_Count')\n",
    "\n",
    "      sorted_df = grouped.sort_values(by=['SolutionNumber', 'Solution', 'CC_Level', 'Fin_Year'])\n",
    "\n",
    "      final_df = sorted_df.loc[sorted_df['Fin_Year'] == financial_year]\n",
    "\n",
    "      return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_results(results):\n",
    "\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_parts = [\"./Data_Output/activities_output_grouped\", financial_year.replace('/', ''), f\"AT_{timestamp}\"]\n",
    "\n",
    "    if nsga3:\n",
    "        file_parts.append(\"NSGA3\")\n",
    "    if not nsga3:\n",
    "        file_parts.append(\"NSGA2\")\n",
    "    if restricted_mutation:\n",
    "        file_parts.append(f\"Site_Limit_{restricted_mutation_depth}\")\n",
    "    if include_extreme_individual:\n",
    "        file_parts.append(\"EI_Inc\")\n",
    "    if include_original_sites:\n",
    "        file_parts.append(\"OI_Inc\")\n",
    "    \n",
    "    file_parts.append(f\"Num_Elites_{num_elites}\")\n",
    "        \n",
    "    file_name = '_'.join(file_parts) + \".csv\"\n",
    "\n",
    "    results.to_csv(file_name, index=False)\n",
    "    \n",
    "    return print(f\"File output: {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then lets run our algorithm and evolve our solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = [True,False]\n",
    "periods = ['19/20','20/21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run as 19/20 using NSGA3\n",
      "Generation: 379, Generations Since Improvement: 20379\n",
      "Fitness: (0.11018052477014308, 0.5506245548896126, 0.13249999999999992, -0.11790636400486422, -0.2779, -0.3357777777777778, 0.1599089907630425, 0.63775)\n",
      "File output: ./Data_Output/activities_output_grouped_1920_AT_20240110_165047_NSGA3_Site_Limit_10_Num_Elites_10.csv\n",
      "Run as 20/21 using NSGA3\n",
      "Generation: 49, Generations Since Improvement: 0"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_105676\\3202974287.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHistory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mlogs_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_logs_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mpop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhof\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhof2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m             \u001b[1;31m# print(log.stream)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mbest_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_105676\\3134602295.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    563\u001b[0m         \u001b[0minvalid_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moffspring\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[0mfitnesses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoolbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minvalid_ind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minvalid_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfitnesses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    566\u001b[0m             \u001b[0mind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Duncan.passey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\deap\\tools\\constraint.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(individual, *args, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindividual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfbty_fct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindividual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindividual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindividual\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_105676\\3134602295.py\u001b[0m in \u001b[0;36meval_func\u001b[1;34m(individual, activity_focus)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[1;31m# Add a new row to the DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m     logs_df = logs_df.append({\n\u001b[0m\u001b[0;32m    285\u001b[0m         \u001b[1;34m'individual'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mindividual\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;34m'avg_time'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mavg_time\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Duncan.passey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[0;32m   9766\u001b[0m         )\n\u001b[0;32m   9767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9768\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_append\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   9769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9770\u001b[0m     def _append(\n",
      "\u001b[1;32mc:\\Users\\Duncan.passey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_append\u001b[1;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[0;32m   9790\u001b[0m             \u001b[1;31m# infer_objects is needed for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9791\u001b[0m             \u001b[1;31m#  test_append_empty_frame_to_series_with_dateutil_tz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9792\u001b[1;33m             \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_objects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   9793\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9794\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Duncan.passey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[0mkind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPOSITIONAL_OR_KEYWORD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Duncan.passey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 )\n\u001b[1;32m--> 331\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Duncan.passey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mrename_axis\u001b[1;34m(self, mapper, inplace, **kwargs)\u001b[0m\n\u001b[0;32m   1297\u001b[0m             )\n\u001b[0;32m   1298\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnon_mapper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1299\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1300\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1301\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Use `.rename` to alter labels with a mapper.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Duncan.passey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_set_axis_name\u001b[1;34m(self, name, axis, inplace)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m         \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"inplace\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1378\u001b[1;33m         \u001b[0mrenamed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1379\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m             \u001b[0mrenamed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Duncan.passey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6366\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6367\u001b[0m         \"\"\"\n\u001b[1;32m-> 6368\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6369\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6370\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"copy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Duncan.passey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Duncan.passey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1870\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1871\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1872\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1873\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_consolidate_with_refs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Duncan.passey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2327\u001b[0m     \u001b[0mnew_blocks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2328\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2329\u001b[1;33m         merged_blocks, _ = _merge_blocks(\n\u001b[0m\u001b[0;32m   2330\u001b[0m             \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcan_consolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2331\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\Duncan.passey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2370\u001b[0m         \u001b[1;31m# TODO: optimization potential in case all mgrs contain slices and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2371\u001b[0m         \u001b[1;31m# combination of those slices is a slice, too.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2372\u001b[1;33m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_array\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2374\u001b[0m         \u001b[0mnew_values\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mArrayLike\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Duncan.passey\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for year in periods:\n",
    "    financial_year = year\n",
    "    print(f\"Run as {financial_year} using {'NSGA3' if nsga3 else 'NSGA2'}\")\n",
    "    start_date, end_date = get_fin_year_dates(financial_year)\n",
    "    activities_with_solutions = activities.loc[(activities['CC_Activity_Date'] >= start_date) & (activities['CC_Activity_Date'] <= end_date)].copy().reset_index(drop=True)\n",
    "    filtered_activities, num_homes, num_sites, most_frequent_sites, home_lsoas, home_activities, home_populations = data_prep(activities, start_date, end_date)\n",
    "    nearby_sites = {home: get_nearby_sites(home) for home in home_lsoas}\n",
    "    # Check and register the appropriate mutation function based on your conditions\n",
    "    if restricted_mutation:\n",
    "        toolbox.register(\"mutate\", restricted_mutNearbyInt, indpb=individual_mutation_prob, nearby_sites=nearby_sites)\n",
    "    elif weighted_mutation:\n",
    "        # Ensure that cumulative_probs is calculated and passed correctly\n",
    "        toolbox.register(\"mutate\", weighted_mutation_function, indpb=individual_mutation_prob, cumulative_probs=cumulative_probs)\n",
    "    else:\n",
    "        toolbox.register(\"mutate\", restricted_mutUniformInt, low=0, up=num_sites-1, indpb=individual_mutation_prob)\n",
    "    toolbox.decorate(\"mate\",   history.decorator)\n",
    "    toolbox.decorate(\"mutate\", history.decorator)    \n",
    "    for t in tf:\n",
    "        nsga3 = t\n",
    "        for solution in range(1):\n",
    "            solution_number = solution\n",
    "            toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.random_site, num_homes)\n",
    "            num_elites = elite_pop\n",
    "            activity_focus = list()\n",
    "            activity_limits = set()\n",
    "            restricted_sites = {'E01006570'}\n",
    "            RESTRICTED_SITE_INDICES = {site_codes.index(code) for code in restricted_sites}\n",
    "            proposed_additions = list()\n",
    "            all_sites = site_codes + proposed_additions\n",
    "\n",
    "            history = tools.History()\n",
    "            logs_df = create_logs_df()\n",
    "            pop, log, hof, hof2, best = main()\n",
    "            # print(log.stream)\n",
    "            best_index = pop.index(best)\n",
    "            # print(f\"Index of best individual from Pareto front in the population: {best_index}\")\n",
    "            # print(f\"Individual (bestie): {best}\")\n",
    "            print(f\"Fitness: {pop[best_index].fitness.values}\")\n",
    "            # best_ind = pop[best_index]\n",
    "            \n",
    "            home_to_site_mapping = create_solution_list(best, home_lsoas, site_codes)\n",
    "            activities_with_solutions = add_solution(activities_with_solutions, home_to_site_mapping, solution_number, activity_focus)\n",
    "            export_log(solution_number)\n",
    "            results = aggregate_results(activities_with_solutions)\n",
    "            output_results(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualise our genealogy tree having implemented a decorator in the main script as per the documentation which has updated throught the evolution process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it takes a long time to run on a longer evolution so we can skip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_genealogy_vis = False\n",
    "\n",
    "if run_genealogy_vis == True:\n",
    "    # Add root nodes with no parents\n",
    "    for i in range(1, pop_num + 1):\n",
    "        if i not in history.genealogy_tree:\n",
    "            history.genealogy_tree[i] = ()\n",
    "\n",
    "    # Reconstruct the graph\n",
    "    graph = nx.DiGraph(history.genealogy_tree)\n",
    "    graph = graph.reverse()\n",
    "\n",
    "    evaluation_values  = [toolbox.evaluate(history.genealogy_history[i])[5] for i in graph]\n",
    "    min_val = min(evaluation_values)\n",
    "    max_val = max(evaluation_values)\n",
    "\n",
    "    normalized_values = [(val - min_val) / (max_val - min_val) if max_val != min_val else 0.5 for val in evaluation_values]\n",
    "\n",
    "    colour_map = plt.cm.get_cmap('turbo')\n",
    "    colours = [colour_map(val) for val in normalized_values]\n",
    "\n",
    "    # Now, roots should include the initial population\n",
    "    roots = [n for n in range(1, pop_num + 1)]\n",
    "                \n",
    "    # Initial population (roots) are at level 0\n",
    "    levels = {0: [i for i in range(1, pop_num + 1)]}\n",
    "\n",
    "    # Prepare for BFS\n",
    "    visited = set(levels[0])  # Mark initial population as visited\n",
    "    queue = [(node, 0) for node in levels[0]]  # Queue of (node, level)\n",
    "\n",
    "    # BFS to assign generations\n",
    "    while queue:\n",
    "        parent, parent_level = queue.pop(0)\n",
    "        for child in graph.successors(parent):\n",
    "            if child not in visited:\n",
    "                visited.add(child)\n",
    "                child_level = parent_level + 1\n",
    "                levels.setdefault(child_level, []).append(child)\n",
    "                queue.append((child, child_level))\n",
    "\n",
    "    generations = max(levels.keys()) + 1\n",
    "    # Check if the number of generations is as expected\n",
    "    print(f\"Calculated number of generations: {generations}\")\n",
    "\n",
    "    # Set the fixed width for each row\n",
    "    fixed_row_width = 2.0  # adjust this as needed\n",
    "\n",
    "    vertical_spacing = 0.3\n",
    "\n",
    "    # Manual layout: Place nodes by generation\n",
    "    pos = {}\n",
    "    for level, nodes in levels.items():\n",
    "        level_width = len(nodes)\n",
    "        # Calculate spacing between nodes\n",
    "        if level_width > 1:\n",
    "            dx = fixed_row_width / (level_width - 1)\n",
    "        else:\n",
    "            dx = 0\n",
    "\n",
    "        # Center nodes in each level\n",
    "        start_x = -fixed_row_width / 2 if level_width > 1 else 0\n",
    "\n",
    "        for i, node in enumerate(sorted(nodes)):\n",
    "            x_position = start_x + i * dx\n",
    "            pos[node] = (x_position, -level * vertical_spacing)\n",
    "\n",
    "    # Adjust figure size based on the tree size\n",
    "    fig_width = max(10, pop_num / 5)  # Example heuristic\n",
    "    fig_height = max(5, generations * vertical_spacing)  # Adjust height based on vertical spacing\n",
    "    plt.figure(figsize=(fig_width, fig_height))\n",
    "\n",
    "    # Adjust other parameters based on the size of the tree\n",
    "    node_size = max(30, 4000 / pop_num)  # Example scaling\n",
    "    font_size = min(8, 4000 / pop_num)  # Example scaling\n",
    "\n",
    "    print(\"Root nodes:\", roots)\n",
    "    print(\"Number of nodes in graph:\", len(graph.nodes()))\n",
    "    print(\"Sample of genealogy tree:\", list(history.genealogy_tree.items())[:5])\n",
    "\n",
    "    missing_nodes = [node for node in graph.nodes() if node not in pos]\n",
    "    if missing_nodes:\n",
    "        print(\"Missing nodes in layout:\", missing_nodes)\n",
    "        # assign a default position to these nodes\n",
    "        for node in missing_nodes:\n",
    "            pos[node] = (0, 0)  # Example default position\n",
    "            \n",
    "    print(\"Levels:\", levels)\n",
    "    print(\"Positions of first few nodes:\", list(pos.items())[:10])\n",
    "\n",
    "    # Draw the graph\n",
    "    nx.draw(graph, pos, node_color=colours, with_labels=True, node_size=node_size, font_size=font_size)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a copy of the input data so we can add the solutions as new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_with_solutions = activities.loc[(activities['CC_Activity_Date'] >= start_date) & (activities['CC_Activity_Date'] <= end_date)].copy().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_number = 1\n",
    "# Now we can create a list that maps each home to a site\n",
    "home_to_site_mapping = create_solution_list(best_ind, home_lsoas, site_codes)\n",
    "# And add the assignments back to the data frame\n",
    "activities_with_solutions = add_solution(activities_with_solutions, home_to_site_mapping, solution_number, activity_focus)\n",
    "\n",
    "export_log(solution_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful now we have competing priorities to visualise the Pareto Front, this is the edge along which in the solution space, a move which is positive for one objective is negative for the competing objective and where those limits are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectives_pareto = [ind.fitness.values for ind in hof2]\n",
    "objectives_all = [ind.fitness.values for ind in pop]\n",
    "\n",
    "plt.scatter([obj[0] for obj in objectives_all],\n",
    "            [obj[1] for obj in objectives_all],\n",
    "            c='grey', label='All Solutions')\n",
    "\n",
    "plt.scatter([obj[0] for obj in objectives_pareto],\n",
    "            [obj[1] for obj in objectives_pareto],\n",
    "            c='red', label='Pareto Front Solutions')\n",
    "\n",
    "plt.xlabel('Objective 1')\n",
    "plt.ylabel('Objective 2')\n",
    "plt.title('Pareto Front')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.xlim(plt.xlim()[1], plt.xlim()[0])  # Reverse X-axis\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectives = [ind.fitness.values for ind in hof2]\n",
    "objectives_all = [ind.fitness.values for ind in pop]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter([obj[0] for obj in objectives],\n",
    "           [obj[1] for obj in objectives],\n",
    "           [obj[2] for obj in objectives],\n",
    "           c='red', label='Pareto Front Solutions')\n",
    "ax.scatter([obj[0] for obj in objectives_all],\n",
    "           [obj[1] for obj in objectives_all],\n",
    "           [obj[2] for obj in objectives_all],\n",
    "           c='grey', label='All Solutions')\n",
    "ax.set_xlabel('Objective 1')\n",
    "ax.set_ylabel('Objective 2')\n",
    "ax.set_zlabel('Objective 3')\n",
    "plt.title('Pareto Front')\n",
    "\n",
    "plt.xlim(plt.xlim()[1], plt.xlim()[0])  # Reverse X-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could look at a pair plot to give us a sense of the optimisation journeys of each objective and \n",
    "# how they compare with each competing objective\n",
    "\n",
    "# Create DataFrame for Pareto front objectives\n",
    "objectives_df = pd.DataFrame(objectives, columns=['min_travel_time','max_in_30', 'min_max_distance', 'max_large_unit',\n",
    "                                                  'max_min_no', 'min_max_no', 'max_in_30_and_large', 'max_large_nicu'])\n",
    "objectives_df['Pareto Front'] = 'Yes'\n",
    "\n",
    "# Create DataFrame for all population objectives\n",
    "objectives_all_df = pd.DataFrame(objectives_all, columns=['min_travel_time','max_in_30', 'min_max_distance', 'max_large_unit',\n",
    "                                                  'max_min_no', 'min_max_no', 'max_in_30_and_large', 'max_large_nicu'])\n",
    "objectives_all_df['Pareto Front'] = 'No'\n",
    "\n",
    "# Concatenate both DataFrames\n",
    "all_data = pd.concat([objectives_df, objectives_all_df])\n",
    "\n",
    "# Plot using pairplot with hue based on the 'Pareto Front' column\n",
    "sns.pairplot(all_data, hue='Pareto Front')\n",
    "plt.suptitle('Pareto Front Pairwise Comparisons')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_sites = site_codes + proposed_additions\n",
    "\n",
    "# best_individual_solution = {home_lsoas[i]: all_sites[best_ind[i]] for i in range(len(best_ind))}\n",
    "\n",
    "# # Create a DataFrame from the best individual solution that maps home to site\n",
    "# home_to_site_df = pd.DataFrame(list(best_individual_solution.items()), columns=['HomeCode', 'SiteCode'])\n",
    "\n",
    "# # Initialize the total activity counts\n",
    "# total_activity_counts = defaultdict(lambda: [0, 0, 0])\n",
    "\n",
    "# # Iterate through the daily activities\n",
    "# for daily_df in daily_activities:\n",
    "#     # Merge the daily DataFrame with the home_to_site_df to add the site codes\n",
    "#     daily_with_site = daily_df.reset_index().merge(home_to_site_df, left_on='Der_Postcode_LSOA_Code', right_on='HomeCode')\n",
    "    \n",
    "#     # Drop the SiteCode_x column and rename SiteCode_y to SiteCode\n",
    "#     daily_with_site.drop(columns=['HomeCode'], inplace=True)\n",
    "    \n",
    "#     # Use pivot_table to compute the counts for each combination of site and activity level\n",
    "#     activity_counts_pivot = daily_with_site.pivot_table(index='SiteCode', columns='CC_Level', aggfunc='size', fill_value=0)\n",
    "\n",
    "#     # Update the total activity counts with the daily counts\n",
    "#     for site_code, counts in activity_counts_pivot.iterrows():\n",
    "#         total_activity_counts[site_code][0] += counts.get('NICU', 0)\n",
    "#         total_activity_counts[site_code][1] += counts.get('HDU', 0)\n",
    "#         total_activity_counts[site_code][2] += counts.get('SCBU', 0)\n",
    "\n",
    "# # Compute the average daily activity counts\n",
    "# average_daily_activity_counts = {site_code: [count / len(daily_activities) for count in counts] for site_code, counts in total_activity_counts.items()}\n",
    "\n",
    "# # Print the results\n",
    "# print(\"Average daily activity counts per site:\")\n",
    "# print(average_daily_activity_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the average_daily_activity_counts dictionary into a DataFrame\n",
    "# average_daily_activities_df = pd.DataFrame.from_dict(average_daily_activity_counts, orient='index', columns=['NICU', 'HDU', 'SCBU'])\n",
    "\n",
    "# # Reset the index to make 'SiteCode' a column\n",
    "# average_daily_activities_df.reset_index(inplace=True)\n",
    "# average_daily_activities_df.rename(columns={'index': 'SiteCode'}, inplace=True)\n",
    "# average_daily_activities_df['SiteCode'] = average_daily_activities_df['SiteCode'].map(site_mapping)\n",
    "\n",
    "# # Plot the DataFrame directly using pandas plot function\n",
    "# average_daily_activities_df.set_index('SiteCode').plot(kind='bar', stacked=False, figsize=(10,7))\n",
    "\n",
    "# plt.title('Average Daily Activities by Site and Activity Level')\n",
    "# plt.xlabel('Site')\n",
    "# plt.ylabel('Average Daily Activities')\n",
    "# plt.legend(title='Activity Level')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_assignments_folium(individual):\n",
    "\n",
    "    # Make a homes GeoDF\n",
    "    homes_geo_df = lsoas[lsoas['lsoa11cd'].isin(home_codes)]\n",
    "    homes_geo_df = homes_geo_df.set_index('lsoa11cd')\n",
    "    homes_geo_df['centroid'] = homes_geo_df.geometry.centroid\n",
    "\n",
    "    sites_geo_df = lsoas[lsoas['lsoa11cd'].isin(all_sites)]\n",
    "    sites_geo_df = sites_geo_df.set_index('lsoa11cd')\n",
    "    sites_geo_df['centroid'] = sites_geo_df.geometry.centroid\n",
    "\n",
    "    # Convert centroids to latitude and longitude\n",
    "    homes_centroids_ll = homes_geo_df['centroid'].to_crs(epsg=4326)\n",
    "    sites_centroids_ll = sites_geo_df['centroid'].to_crs(epsg=4326)\n",
    "    \n",
    "    # Calculate the mean latitude and longitude\n",
    "    center_latitude = homes_centroids_ll.apply(lambda p: p.y).mean()\n",
    "    center_longitude = homes_centroids_ll.apply(lambda p: p.x).mean()\n",
    "\n",
    "    # Create a map centered at the mean coordinates\n",
    "    m = folium.Map(location=[center_latitude, center_longitude], zoom_start=7)\n",
    "    \n",
    "    # Add the site locations as blue markers\n",
    "    for point in sites_centroids_ll:\n",
    "        folium.Marker([point.y, point.x], icon=folium.Icon(color=\"blue\")).add_to(m)\n",
    "\n",
    "    # Add the home locations as small red markers\n",
    "    for point in homes_centroids_ll:\n",
    "        folium.CircleMarker([point.y, point.x], radius=2, color=\"red\").add_to(m)\n",
    "\n",
    "    # Plot lines from home to site\n",
    "    for home_idx, site_idx in enumerate(individual):\n",
    "        home_code = home_lsoas[home_idx]\n",
    "        site_code = all_sites[site_idx]\n",
    "        \n",
    "        if home_code in homes_geo_df.index and site_code in sites_geo_df.index:\n",
    "            home_point = homes_centroids_ll.loc[home_code]\n",
    "            site_point = sites_centroids_ll.loc[site_code]\n",
    "            coords = [[home_point.y, home_point.x], [site_point.y, site_point.x]]\n",
    "            folium.PolyLine(coords, color=\"grey\", weight=1, opacity=0.8).add_to(m)\n",
    "            \n",
    "        # else: print(f\"Missing Combination: home code: {home_code} site code: {site_code}\")\n",
    "\n",
    "    # Display the map\n",
    "    return m\n",
    "\n",
    "# Use the function and store the map\n",
    "m = plot_assignments_folium(pop[best_index])\n",
    "\n",
    "# Show the map\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_activity_levels = set()\n",
    "for daily_df in daily_activities:\n",
    "    unique_activity_levels.update(daily_df['CC_Level'].unique())\n",
    "\n",
    "print(\"Unique activity levels across all daily DataFrames:\", unique_activity_levels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at it from a different starting point. We will create an extreme individual, as in our absolute travel time only solution and then use this as the starting point for our evolution, this should ensure that travel times are optimally accounted for at the beginning and we then begin to factor in our other priorities. \n",
    "\n",
    "Because we have built this in to our population creation functions we can simply set the flag to include the individual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then re run and re-visualise our new results, as we are starting with only one extreme individual, let's turn up our mutation chance as we will be relying on mutation for any variation in the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_number = 2\n",
    "# Now we can create a list that maps each home to a site\n",
    "home_to_site_mapping = create_solution_list(best_ind, home_lsoas, site_codes)\n",
    "# And add the assignments back to the data frame\n",
    "activities_with_solutions = add_solution(activities_with_solutions, home_to_site_mapping, solution_number, activity_focus)\n",
    "# Store the log data for some interogation if required\n",
    "export_log(solution_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look which units now meet the various standards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectives_pareto = [ind.fitness.values for ind in hof2]\n",
    "objectives_all = [ind.fitness.values for ind in pop]\n",
    "\n",
    "plt.scatter([obj[1] for obj in objectives_all],\n",
    "            [obj[4] for obj in objectives_all],\n",
    "            c='grey', label='All Solutions')\n",
    "\n",
    "plt.scatter([obj[1] for obj in objectives_pareto],\n",
    "            [obj[4] for obj in objectives_pareto],\n",
    "            c='red', label='Pareto Front Solutions')\n",
    "\n",
    "plt.xlabel('Objective 2')\n",
    "plt.ylabel('Objective 5')\n",
    "plt.title('Pareto Front')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,7), layout='constrained')  # Adjusted figure size\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Grey dots with reduced alpha\n",
    "ax.scatter([obj[0] for obj in objectives_all],\n",
    "           [obj[3] for obj in objectives_all],\n",
    "           [obj[1] for obj in objectives_all],\n",
    "           c='grey', label='All Solutions', alpha=0.5)\n",
    "\n",
    "# Red dots with increased size\n",
    "ax.scatter([obj[0] for obj in objectives],\n",
    "           [obj[3] for obj in objectives],\n",
    "           [obj[1] for obj in objectives],\n",
    "           c='red', label='Pareto Front Solutions', alpha=1, s=35)\n",
    "\n",
    "ax.set_xlabel('Objective 1')\n",
    "ax.set_ylabel('Objective 4')\n",
    "ax.set_zlabel('Objective 2')\n",
    "\n",
    "ax.xaxis.labelpad = -32\n",
    "ax.yaxis.labelpad = -30\n",
    "ax.zaxis.labelpad = -30\n",
    "\n",
    "ax.view_init(elev=20., azim=35)\n",
    "\n",
    "plt.title('Pareto Front')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for Pareto front objectives\n",
    "objectives_df = pd.DataFrame(objectives, columns=['min_travel_time','max_in_30', 'min_max_distance', 'max_large_unit',\n",
    "                                                  'max_min_no', 'min_max_no', 'max_in_30_and_large', 'max_large_nicu'])\n",
    "objectives_df['Pareto Front'] = 'Yes'\n",
    "\n",
    "# Create DataFrame for all population objectives\n",
    "objectives_all_df = pd.DataFrame(objectives_all, columns=['min_travel_time','max_in_30', 'min_max_distance', 'max_large_unit',\n",
    "                                                  'max_min_no', 'min_max_no', 'max_in_30_and_large', 'max_large_nicu'])\n",
    "objectives_all_df['Pareto Front'] = 'No'\n",
    "\n",
    "# Concatenate both DataFrames\n",
    "all_data = pd.concat([objectives_df, objectives_all_df])\n",
    "\n",
    "# Plot using pairplot with hue based on the 'Pareto Front' column\n",
    "sns.pairplot(all_data, hue='Pareto Front')\n",
    "plt.suptitle('Pareto Front Pairwise Comparisons')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the average_daily_activity_counts dictionary into a DataFrame\n",
    "average_daily_activities_df = pd.DataFrame.from_dict(average_daily_activity_counts, orient='index', columns=['NICU', 'HDU', 'SCBU'])\n",
    "\n",
    "# Reset the index to make 'SiteCode' a column\n",
    "average_daily_activities_df.reset_index(inplace=True)\n",
    "average_daily_activities_df.rename(columns={'index': 'SiteCode'}, inplace=True)\n",
    "average_daily_activities_df['SiteCode'] = average_daily_activities_df['SiteCode'].map(site_mapping)\n",
    "\n",
    "# Plot the DataFrame directly using pandas plot function\n",
    "average_daily_activities_df.set_index('SiteCode').plot(kind='bar', stacked=False, figsize=(10,7))\n",
    "\n",
    "plt.title('Average Daily Activities by Site and Activity Level')\n",
    "plt.xlabel('Site')\n",
    "plt.ylabel('Average Daily Activities')\n",
    "plt.legend(title='Activity Level')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = plot_assignments_folium(pop[best_index])\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add some additional restrictions in to run various different scenarios:\n",
    "\n",
    "Firstly lets try to limit NICU activity at a particular site\n",
    "\n",
    "It is important to understand that our population assignments will allow activity to be assigned to this site but that our evaluation should predjudice against solutions where this limit is breached. This does mean that activity will be assigned here but hopefully with the right configuration in the evolutionary algorithm we should be able to model a realistic solution that minimises this activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 3: Here we limit 'RXR10' (Burnley Hospital) to a minimum NICU activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_focus = list()\n",
    "activity_limits = {\n",
    "    'E01024897': {'NICU': {'min': 0, 'max': 500}}\n",
    "    }\n",
    "restricted_sites = list()\n",
    "RESTRICTED_SITE_INDICES = {site_codes.index(code) for code in restricted_sites}\n",
    "proposed_additions = list()\n",
    "all_sites = site_codes + proposed_additions\n",
    "\n",
    "# number of solutions in a population\n",
    "pop_num = 100\n",
    "# percentage chance to cross breed one solution with another\n",
    "cross_chance = 0.5\n",
    "# percentage chance to introduce random mutations into the solutions\n",
    "initial_mutation_prob = 0.05\n",
    "\n",
    "# Let us re-state our weightings in case we want to tweak them\n",
    "min_travel_time         = -1.0\n",
    "max_in_30               = 1.0\n",
    "min_max_distance        = -1.0\n",
    "max_large_unit          = 1.0\n",
    "max_min_no              = 1.0\n",
    "min_max_no              = -1.0\n",
    "# constraint_adherence    = -2.0\n",
    "max_in_30_and_large     = 1.0\n",
    "max_large_nicu          = 1.0\n",
    "\n",
    "nicu_activities_threshold = 2000 \n",
    "\n",
    "include_original_sites = False\n",
    "\n",
    "num_elites = elite_pop\n",
    "\n",
    "history = tools.History()\n",
    "logs_df = create_logs_df()\n",
    "pop, log, hof, hof2 = main()\n",
    "best_index = get_best_from_pareto_index(hof2, pop)\n",
    "print(f\"Index of best individual from Pareto front in the population: {best_index}\")\n",
    "print(f\"Individual: {pop[best_index]}\")\n",
    "print(f\"Fitness: {pop[best_index].fitness.values}\")\n",
    "best_ind = pop[best_index]\n",
    "\n",
    "solution_number = 3\n",
    "# Now we can create a list that maps each home to a site\n",
    "home_to_site_mapping = create_solution_list(best_ind, home_lsoas, site_codes)\n",
    "# And add the assignments back to the data frame\n",
    "activities_with_solutions = add_solution(activities_with_solutions, home_to_site_mapping, solution_number, activity_focus)\n",
    "# Store the log data for some interogation if required\n",
    "export_log(solution_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = plot_assignments_folium(pop[best_index])\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 4: Here we limit 'RXR10' (Burnley Hospital) to a minimum NICU activity, and include the original individual to influence the evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_focus = list()\n",
    "activity_limits = {\n",
    "    'E01024897': {'NICU': {'min': 0, 'max': 500}}\n",
    "    }\n",
    "restricted_sites = list()\n",
    "RESTRICTED_SITE_INDICES = {site_codes.index(code) for code in restricted_sites}\n",
    "proposed_additions = list()\n",
    "all_sites = site_codes + proposed_additions\n",
    "\n",
    "# number of solutions in a population\n",
    "pop_num = 100\n",
    "# percentage chance to cross breed one solution with another\n",
    "cross_chance = 0.5\n",
    "# percentage chance to introduce random mutations into the solutions\n",
    "initial_mutation_prob = 0.05\n",
    "\n",
    "# Let us re-state our weightings in case we want to tweak them\n",
    "min_travel_time         = -1.0\n",
    "max_in_30               = 1.0\n",
    "min_max_distance        = -1.0\n",
    "max_large_unit          = 1.0\n",
    "max_min_no              = 1.0\n",
    "min_max_no              = -1.0\n",
    "# constraint_adherence    = -2.0\n",
    "max_in_30_and_large     = 1.0\n",
    "max_large_nicu          = 1.0\n",
    "\n",
    "nicu_activities_threshold = 2000 \n",
    "\n",
    "include_original_sites = True\n",
    "\n",
    "num_elites = elite_pop\n",
    "\n",
    "history = tools.History()\n",
    "logs_df = create_logs_df()\n",
    "pop, log, hof, hof2 = main()\n",
    "best_index = get_best_from_pareto_index(hof2, pop)\n",
    "print(f\"Index of best individual from Pareto front in the population: {best_index}\")\n",
    "print(f\"Individual: {pop[best_index]}\")\n",
    "print(f\"Fitness: {pop[best_index].fitness.values}\")\n",
    "best_ind = pop[best_index]\n",
    "\n",
    "solution_number = 4\n",
    "# Now we can create a list that maps each home to a site\n",
    "home_to_site_mapping = create_solution_list(best_ind, home_lsoas, site_codes)\n",
    "# And add the assignments back to the data frame\n",
    "activities_with_solutions = add_solution(activities_with_solutions, home_to_site_mapping, solution_number, activity_focus)\n",
    "# Store the log data for some interogation if required\n",
    "export_log(solution_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = plot_assignments_folium(pop[best_index])\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 5: Here we limit 'RXR10' (Burnley Hospital) to a minimum NICU activity, with the original individual and reduced elitism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_focus = list()\n",
    "activity_limits = {\n",
    "    'E01024897': {'NICU': {'min': 0, 'max': 500}}\n",
    "    }\n",
    "restricted_sites = list()\n",
    "RESTRICTED_SITE_INDICES = {site_codes.index(code) for code in restricted_sites}\n",
    "proposed_additions = list()\n",
    "all_sites = site_codes + proposed_additions\n",
    "\n",
    "# number of solutions in a population\n",
    "pop_num = 100\n",
    "# percentage chance to cross breed one solution with another\n",
    "cross_chance = 0.5\n",
    "# percentage chance to introduce random mutations into the solutions\n",
    "initial_mutation_prob = 0.05\n",
    "\n",
    "# Let us re-state our weightings in case we want to tweak them\n",
    "min_travel_time         = -1.0\n",
    "max_in_30               = 1.0\n",
    "min_max_distance        = -1.0\n",
    "max_large_unit          = 1.0\n",
    "max_min_no              = 1.0\n",
    "min_max_no              = -1.0\n",
    "# constraint_adherence    = -2.0\n",
    "max_in_30_and_large     = 1.0\n",
    "max_large_nicu          = 1.0\n",
    "\n",
    "nicu_activities_threshold = 2000 \n",
    "\n",
    "include_original_sites = True\n",
    "\n",
    "num_elites = 1\n",
    "\n",
    "history = tools.History()\n",
    "logs_df = create_logs_df()\n",
    "pop, log, hof, hof2 = main()\n",
    "best_index = get_best_from_pareto_index(hof2, pop)\n",
    "print(f\"Index of best individual from Pareto front in the population: {best_index}\")\n",
    "print(f\"Individual: {pop[best_index]}\")\n",
    "print(f\"Fitness: {pop[best_index].fitness.values}\")\n",
    "best_ind = pop[best_index]\n",
    "\n",
    "\n",
    "solution_number = 5\n",
    "# Now we can create a list that maps each home to a site\n",
    "home_to_site_mapping = create_solution_list(best_ind, home_lsoas, site_codes)\n",
    "# And add the assignments back to the data frame\n",
    "activities_with_solutions = add_solution(activities_with_solutions, home_to_site_mapping, solution_number, activity_focus)\n",
    "# Store the log data for some interogation if required\n",
    "export_log(solution_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = plot_assignments_folium(pop[best_index])\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 6: Here we limit 'RXN02' (Royal Preston) to a minimum activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_focus = list()\n",
    "activity_limits = {\n",
    "    'E01025300': {'NICU': {'min': 0, 'max': 500}}\n",
    "    }\n",
    "restricted_sites = list()\n",
    "RESTRICTED_SITE_INDICES = {site_codes.index(code) for code in restricted_sites}\n",
    "proposed_additions = list()\n",
    "all_sites = site_codes + proposed_additions\n",
    "\n",
    "# number of solutions in a population\n",
    "pop_num = 100\n",
    "# percentage chance to cross breed one solution with another\n",
    "cross_chance = 0.5\n",
    "# percentage chance to introduce random mutations into the solutions\n",
    "initial_mutation_prob = 0.05\n",
    "\n",
    "# Let us re-state our weightings in case we want to tweak them\n",
    "min_travel_time         = -1.0\n",
    "max_in_30               = 1.0\n",
    "min_max_distance        = -1.0\n",
    "max_large_unit          = 1.0\n",
    "max_min_no              = 1.0\n",
    "min_max_no              = -1.0\n",
    "# constraint_adherence    = -2.0\n",
    "max_in_30_and_large     = 1.0\n",
    "max_large_nicu          = 1.0\n",
    "\n",
    "nicu_activities_threshold = 2000 \n",
    "\n",
    "include_original_sites = False\n",
    "\n",
    "num_elites = elite_pop\n",
    "\n",
    "history = tools.History()\n",
    "logs_df = create_logs_df()\n",
    "pop, log, hof, hof2 = main()\n",
    "best_index = get_best_from_pareto_index(hof2, pop)\n",
    "print(f\"Index of best individual from Pareto front in the population: {best_index}\")\n",
    "print(f\"Individual: {pop[best_index]}\")\n",
    "print(f\"Fitness: {pop[best_index].fitness.values}\")\n",
    "best_ind = pop[best_index]\n",
    "\n",
    "solution_number = 6\n",
    "# Now we can create a list that maps each home to a site\n",
    "home_to_site_mapping = create_solution_list(best_ind, home_lsoas, site_codes)\n",
    "# And add the assignments back to the data frame\n",
    "activities_with_solutions = add_solution(activities_with_solutions, home_to_site_mapping, solution_number, activity_focus)\n",
    "# Store the log data for some interogation if required\n",
    "export_log(solution_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = plot_assignments_folium(pop[best_index])\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 7: Here we limit 'RXN02' (Royal Preston) to a minimum activity and include the original individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_focus = list()\n",
    "activity_limits = {\n",
    "    'E01025300': {'NICU': {'min': 0, 'max': 500}}\n",
    "    }\n",
    "restricted_sites = list()\n",
    "RESTRICTED_SITE_INDICES = {site_codes.index(code) for code in restricted_sites}\n",
    "proposed_additions = list()\n",
    "all_sites = site_codes + proposed_additions\n",
    "\n",
    "# number of solutions in a population\n",
    "pop_num = 100\n",
    "# percentage chance to cross breed one solution with another\n",
    "cross_chance = 0.5\n",
    "# percentage chance to introduce random mutations into the solutions\n",
    "initial_mutation_prob = 0.05\n",
    "\n",
    "# Let us re-state our weightings in case we want to tweak them\n",
    "min_travel_time         = -1.0\n",
    "max_in_30               = 1.0\n",
    "min_max_distance        = -1.0\n",
    "max_large_unit          = 1.0\n",
    "max_min_no              = 1.0\n",
    "min_max_no              = -1.0\n",
    "# constraint_adherence    = -2.0\n",
    "max_in_30_and_large     = 1.0\n",
    "max_large_nicu          = 1.0\n",
    "\n",
    "nicu_activities_threshold = 2000 \n",
    "\n",
    "include_original_sites = True\n",
    "\n",
    "num_elites = elite_pop\n",
    "\n",
    "history = tools.History()\n",
    "logs_df = create_logs_df()\n",
    "pop, log, hof, hof2 = main()\n",
    "best_index = get_best_from_pareto_index(hof2, pop)\n",
    "print(f\"Index of best individual from Pareto front in the population: {best_index}\")\n",
    "print(f\"Individual: {pop[best_index]}\")\n",
    "print(f\"Fitness: {pop[best_index].fitness.values}\")\n",
    "best_ind = pop[best_index]\n",
    "\n",
    "solution_number = 7\n",
    "# Now we can create a list that maps each home to a site\n",
    "home_to_site_mapping = create_solution_list(best_ind, home_lsoas, site_codes)\n",
    "# And add the assignments back to the data frame\n",
    "activities_with_solutions = add_solution(activities_with_solutions, home_to_site_mapping, solution_number, activity_focus)\n",
    "# Store the log data for some interogation if required\n",
    "export_log(solution_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = plot_assignments_folium(pop[best_index])\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 8: Here we limit 'RXN02' (Royal Preston) to a minimum activity and include the original individual with reduced elitism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_focus = list()\n",
    "activity_limits = {\n",
    "    'E01025300': {'NICU': {'min': 0, 'max': 500}}\n",
    "    }\n",
    "restricted_sites = list()\n",
    "RESTRICTED_SITE_INDICES = {site_codes.index(code) for code in restricted_sites}\n",
    "proposed_additions = list()\n",
    "all_sites = site_codes + proposed_additions\n",
    "\n",
    "# number of solutions in a population\n",
    "pop_num = 100\n",
    "# percentage chance to cross breed one solution with another\n",
    "cross_chance = 0.5\n",
    "# percentage chance to introduce random mutations into the solutions\n",
    "initial_mutation_prob = 0.05\n",
    "\n",
    "# Let us re-state our weightings in case we want to tweak them\n",
    "min_travel_time         = -1.0\n",
    "max_in_30               = 1.0\n",
    "min_max_distance        = -1.0\n",
    "max_large_unit          = 1.0\n",
    "max_min_no              = 1.0\n",
    "min_max_no              = -1.0\n",
    "# constraint_adherence    = -2.0\n",
    "max_in_30_and_large     = 1.0\n",
    "max_large_nicu          = 1.0\n",
    "\n",
    "nicu_activities_threshold = 2000 \n",
    "\n",
    "include_original_sites = True\n",
    "\n",
    "num_elites = 1\n",
    "\n",
    "history = tools.History()\n",
    "logs_df = create_logs_df()\n",
    "pop, log, hof, hof2 = main()\n",
    "best_index = get_best_from_pareto_index(hof2, pop)\n",
    "print(f\"Index of best individual from Pareto front in the population: {best_index}\")\n",
    "print(f\"Individual: {pop[best_index]}\")\n",
    "print(f\"Fitness: {pop[best_index].fitness.values}\")\n",
    "best_ind = pop[best_index]\n",
    "\n",
    "solution_number = 8\n",
    "# Now we can create a list that maps each home to a site\n",
    "home_to_site_mapping = create_solution_list(best_ind, home_lsoas, site_codes)\n",
    "# And add the assignments back to the data frame\n",
    "activities_with_solutions = add_solution(activities_with_solutions, home_to_site_mapping, solution_number, activity_focus)\n",
    "# Store the log data for some interogation if required\n",
    "export_log(solution_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = plot_assignments_folium(pop[best_index])\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then average per day usage which is a good proxy for bed requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique CC_Level values\n",
    "level_order = ['NICU', 'HDU', 'SCBU']\n",
    "\n",
    "# Get solution columns\n",
    "solution_columns = [col for col in activities_with_solutions.columns if col.startswith('solution_') and not col.endswith('_unit')]\n",
    "\n",
    "# Loop over each unique CC_Level value\n",
    "for level in level_order:\n",
    "    # Filter data for the current CC_Level\n",
    "    level_data = activities_with_solutions[activities_with_solutions['CC_Level'] == level]\n",
    "\n",
    "    # Group by SiteLSOA and CC_Activity_Date, then count the data\n",
    "    site_daily_group = level_data.groupby(['SiteLSOA', 'CC_Activity_Date']).size().reset_index(name='Daily_Count')\n",
    "\n",
    "    # Group again by SiteLSOA to calculate the average count per day\n",
    "    site_avg_group = site_daily_group.groupby('SiteLSOA')['Daily_Count'].mean().reset_index(name='Average_Daily_Count')\n",
    "\n",
    "    # Initialize an empty list to store the grouped dataframes\n",
    "    grouped_dfs = [site_avg_group.rename(columns={'SiteLSOA': 'Site', 'Average_Daily_Count': 'Count'})]\n",
    "    grouped_dfs[0]['solution'] = 'site'\n",
    "\n",
    "    # Loop over each solution column\n",
    "    for solution_col in solution_columns:\n",
    "        # Group and calculate average per day for each solution column\n",
    "        solution_daily_group = level_data.groupby([solution_col, 'CC_Activity_Date']).size().reset_index(name='Daily_Count')\n",
    "        solution_avg_group = solution_daily_group.groupby(solution_col)['Daily_Count'].mean().reset_index(name='Average_Daily_Count')\n",
    "\n",
    "        # Rename columns, assign solution label, and append to the list\n",
    "        grouped_dfs.append(solution_avg_group.rename(columns={solution_col: 'Site', 'Average_Daily_Count': 'Count'}))\n",
    "        grouped_dfs[-1]['solution'] = solution_col\n",
    "\n",
    "    # Concatenate all grouped dataframes and plot\n",
    "    merged_avg_solutions = pd.concat(grouped_dfs)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12,6))\n",
    "    ax = sns.barplot(data=merged_avg_solutions, x='Site', y='Count', hue='solution')\n",
    "    plt.title(f'Average Daily Activity Counts per Site for CC_Level: {level}')\n",
    "    plt.xlabel('Site')\n",
    "    plt.ylabel('Average Daily Count of Activities')\n",
    "    plt.xticks(rotation=90)  # Add rotation for better readability of x labels\n",
    "    \n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', fontsize=9, color='black', xytext=(1.3, 13), rotation=90,\n",
    "                    textcoords='offset points')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "solution_columns = [col for col in activities_with_solutions.columns if col.startswith('solution_') and not col.endswith('_unit')]\n",
    "\n",
    "activities_with_solutions['CC_Activity_Date'] = pd.to_datetime(activities_with_solutions['CC_Activity_Date'], format='%Y-%m-%d')\n",
    "\n",
    "# Loop over the unique values of SiteCode to plot each line\n",
    "for site in activities_with_solutions['SiteLSOA'].unique():\n",
    "    # Set up the matplotlib figure\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Filter the data for the current SiteCode\n",
    "    site_data = activities_with_solutions[activities_with_solutions['SiteLSOA'] == site]\n",
    "    \n",
    "    # Resample the data by day and count the number of occurrences per day\n",
    "    daily_summary = site_data.resample('D', on='CC_Activity_Date').size().reset_index(name='Count')\n",
    "    \n",
    "    # Plot the time series for the current SiteCode\n",
    "    sns.lineplot(data=daily_summary, x='CC_Activity_Date', y='Count', label=f'SiteLSOA: {site}', color='b')\n",
    "\n",
    "    # Loop over each solution column\n",
    "    for idx, solution_col in enumerate(solution_columns, start=1):\n",
    "        # Filter data for each solution and resample by date\n",
    "        solution_data = activities_with_solutions[activities_with_solutions[solution_col] == site]\n",
    "        \n",
    "        # Check if solution_data is empty before attempting to resample\n",
    "        if not solution_data.empty:\n",
    "            solution_summary = solution_data.resample('D', on='CC_Activity_Date').size().reset_index(name='Count')\n",
    "            \n",
    "            # Check if solution_summary is empty\n",
    "            if not solution_summary.empty:\n",
    "                sns.lineplot(data=solution_summary, x='CC_Activity_Date', y='Count', label=f'{solution_col} for SiteCode: {site}', color=sns.color_palette(\"husl\", len(solution_columns))[idx - 1])\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title(f'Time Series of Activity Counts for SiteCode: {site}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Count of Activities')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Uncomment the line below to save each plot as a separate file\n",
    "    # plt.savefig(f'time_series_for_SiteLSOA_{site}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file name\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "startstamp = start_date.strftime(\"%Y\")\n",
    "endstamp = end_date.strftime(\"%Y\")\n",
    "file_name = f\"./Data_Output/activities_output_{startstamp}_to_{endstamp}_AT_{timestamp}.csv\"\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "activities_with_solutions.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities.loc[activities['Der_Postcode_LSOA_Code'] == 'E01000874']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_with_solutions.loc[activities_with_solutions['Der_Postcode_LSOA_Code'] == 'E01000874']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_entries(data_dict, value_to_find):\n",
    "    matching_entries = {}\n",
    "    for key, value in data_dict.items():\n",
    "        if value_to_find in key:\n",
    "            matching_entries[key] = value\n",
    "    return matching_entries\n",
    "\n",
    "# Example usage\n",
    "result = find_matching_entries(travel_times_dict, 'E01000874')\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def aggregate_results(df):\n",
    "      \n",
    "      solution_columns = [col for col in df.columns if 'solution_' in col and '_unit' not in col]\n",
    "\n",
    "      df_melted = df.melt(id_vars=[col for col in df.columns if col not in solution_columns],\n",
    "                        value_vars=solution_columns, \n",
    "                        var_name='SolutionColumn', \n",
    "                        value_name='Solution')\n",
    "\n",
    "      df_melted['SolutionNumber'] = df_melted['SolutionColumn'].apply(lambda x: x.split('_')[1])\n",
    "\n",
    "      df_melted['CC_Activity_Date'] = pd.to_datetime(df_melted['CC_Activity_Date'])\n",
    "      df_melted['Fin_Year'] = pd.cut(df_melted['CC_Activity_Date'], \n",
    "                                    bins=[pd.Timestamp('2018-04-01'), pd.Timestamp('2019-04-01'),\n",
    "                                          pd.Timestamp('2020-04-01'), pd.Timestamp('2021-04-01'),\n",
    "                                          pd.Timestamp('2022-04-01')],\n",
    "                                    labels=['18/19', '19/20', '20/21', '21/22'])\n",
    "\n",
    "      grouped = df_melted.groupby(['Solution', 'SolutionNumber', \n",
    "                                    'CC_Level', 'Fin_Year']).size().reset_index(name='Activity_Count')\n",
    "\n",
    "      sorted_df = grouped.sort_values(by=['SolutionNumber', 'Solution', 'CC_Level', 'Fin_Year'])\n",
    "\n",
    "      final_df = sorted_df.loc[sorted_df['Fin_Year'] == financial_year]\n",
    "\n",
    "      return final_df\n",
    "\n",
    "results = aggregate_results(activities_with_solutions)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def output_results(results):\n",
    "    file_parts = [\"./Data_Output/activities_output_grouped\", financial_year.replace('/', ''), f\"AT_{timestamp}\"]\n",
    "\n",
    "    if nsga3:\n",
    "        file_parts.append(\"NSGA3\")\n",
    "    if not nsga3:\n",
    "        file_parts.append(\"NSGA2\")\n",
    "    if restricted_mutation:\n",
    "        file_parts.append(f\"Site_Limit_{restricted_mutation_depth}\")\n",
    "    if include_extreme_individual:\n",
    "        file_parts.append(\"EI_Inc\")\n",
    "    if include_original_sites:\n",
    "        file_parts.append(\"OI_Inc\")\n",
    "file_name = '_'.join(file_parts) + \".csv\"\n",
    "\n",
    "    print(file_name) \n",
    "\n",
    "    results.to_csv(file_name, index=False)\n",
    "    \n",
    "output_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
