{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "azdata_cell_guid": "2696d9d4-76d6-4d08-8db1-b1ae507e0023",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "'numpy' is already installed.\n",
                        "'pandas' is already installed.\n",
                        "'matplotlib' is already installed.\n",
                        "'seaborn' is already installed.\n",
                        "'folium' is already installed.\n",
                        "'geopandas' is already installed.\n",
                        "'networkx' is already installed.\n",
                        "'deap' is already installed.\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import importlib\n",
                "import subprocess\n",
                "\n",
                "def install_and_check_libraries(lib_list):\n",
                "    for lib in lib_list:\n",
                "        try:\n",
                "            # Try to import library\n",
                "            importlib.import_module(lib)\n",
                "            print(f\"'{lib}' is already installed.\")\n",
                "        except ImportError:\n",
                "            # If not installed, install library\n",
                "            try:\n",
                "                print(f\">>>> Installing {lib}.\")\n",
                "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", lib])\n",
                "            except Exception as e:\n",
                "                print(f\"Error installing {lib}:\", str(e))\n",
                "\n",
                "libraries = [\"numpy\", \"pandas\", \"matplotlib\", \"seaborn\", \"folium\",\"geopandas\", \"networkx\", \"deap\"]\n",
                "install_and_check_libraries(libraries)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "azdata_cell_guid": "5bf896b3-fd2f-458c-a943-f0ae671530f0",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from deap import base, creator, tools, algorithms\n",
                "from deap.benchmarks.tools import igd\n",
                "from math import factorial\n",
                "import warnings\n",
                "from collections import defaultdict\n",
                "import seaborn as sns\n",
                "import folium\n",
                "import random\n",
                "import geopandas as gpd\n",
                "from geopandas import GeoSeries\n",
                "from functools import partial\n",
                "from statistics import mean\n",
                "from datetime import datetime\n",
                "import requests\n",
                "import itertools\n",
                "from itertools import product\n",
                "import time\n",
                "import os\n",
                "import networkx as nx\n",
                "# from IPython.display import IFrame, HTML\n",
                "from branca.element import Figure\n",
                "\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "55165ff9-1679-4df6-9b53-7d0076ee561c"
            },
            "source": [
                "Lets set up some initial parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "azdata_cell_guid": "c6d16bbd-56f7-40fa-acb1-3133ed26f019",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "nsga3 = False\n",
                "weighted_mutation = False\n",
                "restricted_mutation = True\n",
                "restricted_mutation_depth = 10 # nearest x number of sites by travel times\n",
                "elite_pop = 10\n",
                "include_extreme_individual = False\n",
                "include_original_sites = False"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "01648c2c-656a-4e70-bb5c-437f09e2ac04"
            },
            "source": [
                "We'll use a function to turn our financial year into useable dates"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "azdata_cell_guid": "6ff314f0-4412-4503-a91d-da4285360b2e",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "def get_fin_year_dates(financial_year):\n",
                "    start_year_part, end_year_part = financial_year.split('/')\n",
                "\n",
                "    start_year = int(\"20\" + start_year_part)  \n",
                "    end_year = int(\"20\" + end_year_part)\n",
                "\n",
                "    start_date = pd.Timestamp(f\"{start_year}-04-01\")\n",
                "    end_date = pd.Timestamp(f\"{end_year}-03-31\")\n",
                "\n",
                "    return start_date, end_date"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "55e21c36-ce7b-4c2b-b942-3ec03bf1d47f"
            },
            "source": [
                "We need to load our data, firstly our travel times which we have in a pre-calculated table"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "azdata_cell_guid": "f8102f3b-5522-49e7-9add-5020a4443550",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Adding file Missing_travel_times_20231117_153542.csv\n",
                        "Adding file Missing_travel_times_20231117_163729.csv\n",
                        "Adding file Missing_travel_times_20231120_002542.csv\n",
                        "Adding file Missing_travel_times_20231120_083042.csv\n",
                        "Adding file Missing_travel_times_20231120_083043.csv\n",
                        "Adding file Missing_travel_times_20231120_143912.csv\n",
                        "Adding file Missing_travel_times_20231121_025324.csv\n",
                        "Adding file Missing_travel_times_20231121_075202.csv\n",
                        "Adding file Missing_travel_times_20231121_075206.csv\n",
                        "Adding file Missing_travel_times_20231122_132507.csv\n",
                        "Adding file Missing_travel_times_20231122_151104.csv\n",
                        "Adding file Missing_travel_times_20240116_161859.csv\n",
                        "Adding file Missing_travel_times_20240117_094839.csv\n",
                        "Adding file Missing_travel_times_20240117_102403.csv\n",
                        "Adding file Missing_travel_times_20240117_104236.csv\n",
                        "Adding file Missing_travel_times_20240117_124704.csv\n",
                        "Adding file Missing_travel_times_20240118_111610.csv\n"
                    ]
                }
            ],
            "source": [
                "# Read in the travel times data\n",
                "travel_times = pd.read_csv('./LSOA_Travel_Times.csv')\n",
                "travel_times = travel_times.dropna()\n",
                "\n",
                "# Initialize an empty DataFrame to hold the new travel times from CSV files\n",
                "new_travel_times_df = pd.DataFrame()\n",
                "\n",
                "directory = \"./\"\n",
                "\n",
                "# Loop through the files in the directory\n",
                "for filename in os.listdir(directory):\n",
                "    if filename.startswith(\"Missing_travel_times\") and filename.endswith(\".csv\"):\n",
                "        # Construct the full file path\n",
                "        file_path = os.path.join(directory, filename)\n",
                "        # Read the CSV file into a DataFrame\n",
                "        current_df = pd.read_csv(file_path)\n",
                "        \n",
                "        print(f\"Adding file {filename}\")\n",
                "        # Append the current DataFrame to the new travel times DataFrame\n",
                "        new_travel_times_df = pd.concat([new_travel_times_df, current_df], ignore_index=True)\n",
                "\n",
                "\n",
                "# Drop any rows with NaN values that may have appeared in the new DataFrame\n",
                "new_travel_times_df = new_travel_times_df.dropna()\n",
                "\n",
                "new_travel_times_df = new_travel_times_df.rename(columns={'Travel_Time': 'TT', 'home_LSOA': 'Home_LSOA'})\n",
                "\n",
                "# Concatenate the existing and new travel times DataFrames\n",
                "combined_travel_times_df = pd.concat([travel_times, new_travel_times_df], ignore_index=True)\n",
                "\n",
                "# Drop duplicates in case some entries are in both DataFrames\n",
                "combined_travel_times_df = combined_travel_times_df.drop_duplicates(subset=['Home_LSOA', 'Site_LSOA'])\n",
                "\n",
                "# Convert the combined DataFrame into a dictionary\n",
                "travel_times_dict = {(row[\"Home_LSOA\"], row[\"Site_LSOA\"]): row[\"TT\"] for _, row in combined_travel_times_df.iterrows()}\n",
                "\n",
                "# Now combined_travel_times_dict contains all the travel times from both sources\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "8a0ff233-e64a-4c82-a532-07170ec4ec61"
            },
            "source": [
                "Let us load and process our data about our sites"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "azdata_cell_guid": "1122bf65-6cb5-4f89-8e8b-bebf9087191c",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "#load to a data frame\n",
                "sites = pd.read_csv('./Sites.csv', encoding='ISO-8859-1')\n",
                "#remove unnecessary columns\n",
                "sites = sites.loc[:, ['UnitCode', 'LSOA','NICU','LCU','SCBU']]\n",
                "\n",
                "#Apply data cleansing\n",
                "sites = sites.replace('', np.nan)\n",
                "sites = sites.dropna()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "256896f8-b975-4e3f-a923-17a3dce94c57"
            },
            "source": [
                "And our activities data "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "azdata_cell_guid": "0957a236-138a-49ae-a819-d26a2a3e2963",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Data ranges from 2020-11-14 00:00:00 to 2023-03-31 00:00:00\n"
                    ]
                }
            ],
            "source": [
                "#Load to a data frame\n",
                "activities = pd.read_csv('./Badgernet_Activity_2 - Copy.csv', encoding='ISO-8859-1')\n",
                "\n",
                "#Remove unecessary columns\n",
                "activities_orig = activities.loc[:, ['Der_Postcode_LSOA_Code','CC_Activity_Date','SiteLSOA', 'CC_Level']]\n",
                "activities = activities.loc[:, ['Der_Postcode_LSOA_Code','CC_Activity_Date','SiteLSOA', 'CC_Level']]\n",
                "\n",
                "#Apply data cleansing\n",
                "activities = activities.replace('', np.nan)\n",
                "activities = activities.dropna()\n",
                "\n",
                "# Ensure the date is a date\n",
                "activities['CC_Activity_Date'] = pd.to_datetime(activities['CC_Activity_Date'], format='%d/%m/%Y')\n",
                "activities_indexed = activities.set_index('Der_Postcode_LSOA_Code')\n",
                "\n",
                "# time_periods = pd.date_range(start_date, end_date, freq='D')\n",
                "\n",
                "int_to_activity = {i: activity for i, activity in enumerate(activities['CC_Level'].unique())}\n",
                "\n",
                "home_lsoas = []\n",
                "most_frequent_sites = []\n",
                "home_activities = []\n",
                "home_populations = []\n",
                "all_sites = []\n",
                "\n",
                "def data_prep(activities, start_date, end_date):\n",
                "    filtered_activities = activities.loc[(activities['CC_Activity_Date'] >= start_date) & (activities['CC_Activity_Date'] <= end_date)]\n",
                "    filtered_activities = filtered_activities.set_index('Der_Postcode_LSOA_Code')\n",
                "    home_lsoas = sorted(filtered_activities.index.unique().tolist())\n",
                "    num_homes = len(home_lsoas)\n",
                "    num_sites = len(site_codes)# Group by DER_Postcode_LSOA_Code and count the occurrences\n",
                "    home_populations_dict = filtered_activities.groupby('Der_Postcode_LSOA_Code').size().to_dict()\n",
                "    home_activities_dict = filtered_activities.groupby('Der_Postcode_LSOA_Code')['CC_Level'].value_counts().unstack(fill_value=0).to_dict(orient='index')\n",
                "    home_activities = [[home_activities_dict[home][int_to_activity[i]] for i in range(3)] for home in home_lsoas]\n",
                "    # Convert it to list matching the order of home_lsoas\n",
                "    home_populations = [home_populations_dict.get(home, 0) for home in home_lsoas]\n",
                "    site_frequencies = filtered_activities.groupby(['Der_Postcode_LSOA_Code', 'SiteLSOA']).size().reset_index(name='counts')\n",
                "    most_frequent_sites = site_frequencies.loc[site_frequencies.groupby('Der_Postcode_LSOA_Code')['counts'].idxmax()]\n",
                "    return filtered_activities, num_homes, num_sites, most_frequent_sites, home_lsoas, home_activities, home_populations\n",
                "\n",
                "#Add site code to our df\n",
                "activities = pd.merge(activities, sites[['LSOA','UnitCode']], left_on='SiteLSOA', right_on='LSOA', how='left')\n",
                "activities = activities.drop('LSOA', axis=1)\n",
                "activities.rename(columns={'UnitCode': 'SiteCode'}, inplace=True)\n",
                "\n",
                "\n",
                "# Make a list of all our homes and sites\n",
                "site_codes = sites['LSOA'].unique().tolist()\n",
                "home_codes =  activities_indexed.index.unique().tolist()\n",
                "\n",
                "# print (f\"filtered_activities row count: {len(filtered_activities)}\")\n",
                "endrange = activities['CC_Activity_Date'].max()\n",
                "startrange = activities['CC_Activity_Date'].min()\n",
                "\n",
                "print(f\"Data ranges from {startrange} to {endrange}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "80cc050c-c9d4-411b-9a8e-384273b06833"
            },
            "source": [
                "We also want to look up any travel times that might be missing from our data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "azdata_cell_guid": "b3c2944d-3cd3-4f05-8423-c366c80a4303",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "class OutOfAPICallsException(Exception):\n",
                "    \"\"\"Exception raised when the API returns a 403 status code indicating the quota has been exceeded.\"\"\"\n",
                "    pass\n",
                "\n",
                "class NoSuchLocationException(Exception):\n",
                "    \"\"\"Exception raised when the API returns a 404 status code indicating the location hasnt been found.\"\"\"\n",
                "    pass \n",
                "\n",
                "class RateLimitException(Exception):\n",
                "    \"\"\"Exception raised when the API returns a 429 status code indicating too many requests.\"\"\"\n",
                "    pass\n",
                "\n",
                "\n",
                "def calculate_travel_time_openrouteservice(api_key, start_coords, end_coords, api_request_no, transport_mode='driving-car'):\n",
                "    \"\"\" Calculate travel time using the Openrouteservice API. \"\"\"\n",
                "    \n",
                "    url = \"https://api.openrouteservice.org/v2/directions/{}/geojson\".format(transport_mode)\n",
                "    \n",
                "    # Set up the headers with the API key\n",
                "    headers = {\n",
                "        'Authorization': api_key,\n",
                "        'Content-Type': 'application/json'\n",
                "    }\n",
                "    \n",
                "    # Set up the parameters with the start and end coordinates\n",
                "    body = {\n",
                "        'coordinates': [start_coords, end_coords]\n",
                "    }\n",
                "    \n",
                "    # Make the request \n",
                "    response = requests.post(url, headers=headers, json=body)\n",
                "    \n",
                "    # Check response\n",
                "    if response.status_code == 200:\n",
                "        # Parse the response\n",
                "        directions = response.json()\n",
                "        try:\n",
                "            # Travel time in seconds is nested in the 'features' list, under the 'properties' dictionary\n",
                "            duration_seconds = directions['features'][0]['properties']['segments'][0]['duration']\n",
                "            return duration_seconds\n",
                "        except (IndexError, KeyError):\n",
                "            print(\"Error parsing the response.\")\n",
                "            return None\n",
                "    elif response.status_code == 403:  # Out of API calls\n",
                "        print(f\"API request {api_request_no} failed with status code {response.status_code}\")\n",
                "        raise OutOfAPICallsException(\"API quota exceeded\")\n",
                "    elif response.status_code == 404:  # Out of API calls\n",
                "        print(f\"API request {api_request_no} failed with location not found {response.status_code}\")\n",
                "        raise NoSuchLocationException(\"No location found\")\n",
                "    elif response.status_code == 429:  # Rate limited by the API\n",
                "        print(f\"API request {api_request_no} has been rate-limited with status code {response.status_code}\")\n",
                "        raise RateLimitException(\"Rate limit exceeded\")\n",
                "    else:\n",
                "        print(f\"API request {api_request_no} failed with status code {response.status_code}\")\n",
                "        return None\n",
                "\n",
                "# Example usage\n",
                "api_key = '5b3ce3597851110001cf62486f4bed53db4c47b7a841e3da98655493'\n",
                "start_coordinates = (8.681495, 49.41461)  # Example coordinates (longitude, latitude)\n",
                "end_coordinates = (8.687872, 49.420318)  # Example coordinates (longitude, latitude)\n",
                "transport_mode = 'driving-car'  # Mode of transportation\n",
                "\n",
                "# Calculate travel time\n",
                "# travel_time_seconds = calculate_travel_time_openrouteservice(api_key, start_coordinates, end_coordinates, transport_mode)\n",
                "# print(f\"Estimated travel time: {travel_time_seconds / 60:.2f} minutes\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "azdata_cell_guid": "3bc1f1a6-d6d6-46c0-8866-c63cf0fcb32c",
                "language": "python"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "LSOA_LL_df = pd.read_csv('./LSOA_to_LL.csv')\n",
                "\n",
                "# Create the Cartesian product of home_codes and site_codes\n",
                "combination_product = list(product(home_codes, site_codes + ['E01012632']))\n",
                "\n",
                "# List to store lat/lng details\n",
                "lat_lng_details = list()\n",
                "\n",
                "LSOA_LL_df\n",
                "\n",
                "# Loop through each combination\n",
                "for home, site in combination_product:\n",
                "    # Check if we have the travel time for this home and site\n",
                "    if (home, site) not in travel_times_dict and home != 'M99999999':\n",
                "        # Filter the DataFrame for the home and check if it's empty\n",
                "        home_rows = LSOA_LL_df[LSOA_LL_df['LSOA'] == home][['Latitude_1m', 'Longitude_1m']]\n",
                "        if not home_rows.empty:\n",
                "            home_lat_lng = home_rows.iloc[0]\n",
                "        else:\n",
                "            # Handle the case where no match is found, for example by continuing to the next iteration\n",
                "            continue\n",
                "\n",
                "        # Filter the DataFrame for the site and check if it's empty\n",
                "        site_rows = LSOA_LL_df[LSOA_LL_df['LSOA'] == site][['Latitude_1m', 'Longitude_1m']]\n",
                "        if not site_rows.empty:\n",
                "            site_lat_lng = site_rows.iloc[0]\n",
                "        else:\n",
                "            # Handle the case where no match is found, for example by continuing to the next iteration\n",
                "            continue\n",
                "        \n",
                "        # Store the details in a dictionary\n",
                "        lat_lng_detail = {\n",
                "            'home_code': home,\n",
                "            'home_latitude': home_lat_lng['Latitude_1m'],\n",
                "            'home_longitude': home_lat_lng['Longitude_1m'],\n",
                "            'site_code': site,\n",
                "            'site_latitude': site_lat_lng['Latitude_1m'],\n",
                "            'site_longitude': site_lat_lng['Longitude_1m']\n",
                "        }\n",
                "        \n",
                "        # Add the dictionary to our list\n",
                "        lat_lng_details.append(lat_lng_detail)\n",
                "        \n",
                "len(lat_lng_details)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "azdata_cell_guid": "820ca576-5028-41b6-903b-c3ab7a1cf4c8",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "# Initialize an empty DataFrame to store home_LSOA, Site_LSOA, and Travel Time\n",
                "missing_travel_times_df = pd.DataFrame(columns=['home_LSOA', 'Site_LSOA', 'Travel_Time'])\n",
                "\n",
                "# API rate limiting parameters\n",
                "api_request_count = 0\n",
                "api_limit = 2000\n",
                "api_per_minute_limit = 40  # Adjust to per-minute limit\n",
                "delay_between_requests = 60 / api_per_minute_limit  # Delay to adhere to per-minute limit\n",
                "\n",
                "not_found_details = []\n",
                "\n",
                "max_retries = 5\n",
                "\n",
                "# Function to handle API requests and retries\n",
                "def fetch_travel_time(detail, max_retries):\n",
                "    retries = 0\n",
                "    while retries < max_retries:\n",
                "        try:\n",
                "            travel_time_seconds = calculate_travel_time_openrouteservice(\n",
                "                api_key, \n",
                "                (detail['home_longitude'], detail['home_latitude']), \n",
                "                (detail['site_longitude'], detail['site_latitude']), \n",
                "                api_request_count, \n",
                "                transport_mode\n",
                "            )\n",
                "\n",
                "            if travel_time_seconds is not None:\n",
                "                return round(travel_time_seconds / 60, 1)  # Convert to minutes\n",
                "\n",
                "        except OutOfAPICallsException:\n",
                "            # If out of API calls, halt further processing\n",
                "            print(\"API quota exceeded. Halting process.\")\n",
                "            return \"quota_exceeded\"\n",
                "        except NoSuchLocationException:\n",
                "            # Log and break for locations not found\n",
                "            not_found_details.append((detail['home_code'], detail['site_code']))\n",
                "            print(f\"Location not found for {detail['home_code']} - {detail['site_code']}\")\n",
                "            break\n",
                "        except RateLimitException:\n",
                "            # If rate limit is hit, wait and retry\n",
                "            time.sleep(delay_between_requests)\n",
                "        except Exception as e:\n",
                "            # Log unexpected exceptions and retry\n",
                "            print(f\"An unexpected exception occurred: {e}\")\n",
                "\n",
                "        retries += 1\n",
                "        if retries < max_retries:\n",
                "            print(f\"Retrying request for {detail['home_code']} to {detail['site_code']}. Attempt {retries}/{max_retries}\")\n",
                "            time.sleep(delay_between_requests)\n",
                "\n",
                "    # Return None if unsuccessful after all retries\n",
                "    return None\n",
                "\n",
                "# List to collect results\n",
                "results = []\n",
                "\n",
                "# Loop through each home-site pair\n",
                "for detail in lat_lng_details:\n",
                "    if api_request_count >= api_limit:\n",
                "        print(\"Stopped due to API quota being exceeded.\")\n",
                "        break\n",
                "\n",
                "    # Fetch travel time with retries\n",
                "    travel_time_minutes = fetch_travel_time(detail, max_retries)\n",
                "\n",
                "    if travel_time_minutes is not None and travel_time_minutes != \"quota_exceeded\":\n",
                "        # Append successful results\n",
                "        results.append({\n",
                "            'home_LSOA': detail['home_code'],\n",
                "            'Site_LSOA': detail['site_code'],\n",
                "            'Travel_Time': travel_time_minutes\n",
                "        })\n",
                "        api_request_count += 1\n",
                "\n",
                "    if travel_time_minutes == \"quota_exceeded\":\n",
                "        # Break the loop if API quota is exceeded\n",
                "        break\n",
                "\n",
                "    if travel_time_minutes is None and isinstance(travel_time_minutes, OutOfAPICallsException):\n",
                "        # If out of API calls, break from the loop\n",
                "        break\n",
                "    \n",
                "    print(f\"Home: {detail['home_code']} Site: {detail['site_code']} >>> {travel_time_minutes} minutes\")\n",
                "    \n",
                "    time.sleep(delay_between_requests)\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "azdata_cell_guid": "4007c1df-336b-4b1a-a63d-24cda74868b4",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Empty DataFrame\n",
                        "Columns: []\n",
                        "Index: []\n"
                    ]
                }
            ],
            "source": [
                "# Convert results to DataFrame\n",
                "missing_travel_times_df = pd.DataFrame(results)\n",
                "\n",
                "print(missing_travel_times_df)\n",
                "\n",
                "def export_travel_times(df):\n",
                "    if len(df) > 0:\n",
                "        now = datetime.now()\n",
                "        timestamp = now.strftime(\"%Y%m%d_%H%M%S\") \n",
                "        # Specify the file name\n",
                "        log_name = f\"./Missing_travel_times_{timestamp}.csv\"\n",
                "        # Save the DataFrame to CSV\n",
                "        df.to_csv(log_name, index=False)\n",
                "        print(f\"Exported file {log_name}\")\n",
                "    \n",
                "# Call the export function\n",
                "export_travel_times(missing_travel_times_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "f427e402-bff6-4990-bdad-7e5aecfb2d9c"
            },
            "source": [
                "Load and organise our geographic information "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "azdata_cell_guid": "1f9613b3-cc6d-4dfe-8bd1-c160cf8dd419",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "# Load the LSOA shape file\n",
                "lsoas = gpd.read_file('./LSOA_Dec_2011_PWC_in_England_and_Wales/LSOA_Dec_2011_PWC_in_England_and_Wales.shp')\n",
                "\n",
                "# Make a sites GeoDF\n",
                "sites_geo_df = lsoas[lsoas['lsoa11cd'].isin(site_codes)]\n",
                "sites_geo_df = sites_geo_df.set_index('lsoa11cd')\n",
                "sites_geo_df['centroid'] = sites_geo_df.geometry.centroid\n",
                "\n",
                "# Make a homes GeoDF\n",
                "homes_geo_df = lsoas[lsoas['lsoa11cd'].isin(home_codes)]\n",
                "homes_geo_df = homes_geo_df.set_index('lsoa11cd')\n",
                "homes_geo_df['centroid'] = homes_geo_df.geometry.centroid\n",
                "\n",
                "# Extract the centroids as a GeoSeries\n",
                "homes_centroids = GeoSeries(homes_geo_df['centroid'])\n",
                "sites_centroids = GeoSeries(sites_geo_df['centroid'])\n",
                "\n",
                "# Set the CRS of the centroids to EPSG:27700\n",
                "homes_centroids.crs = \"EPSG:27700\"\n",
                "sites_centroids.crs = \"EPSG:27700\"\n",
                "\n",
                "# Convert the centroids to EPSG:4326 (latitude and longitude)\n",
                "homes_centroids_ll = homes_centroids.to_crs(epsg=4326)\n",
                "sites_centroids_ll = sites_centroids.to_crs(epsg=4326)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "8bcd0047-aa03-483a-873b-29a6e310dd5d"
            },
            "source": [
                "And we can then plot the assignments on a map"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "azdata_cell_guid": "0f93c526-b945-4ec7-98e5-39ec37f9cfa3",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "current_site_list = []\n",
                "\n",
                "def plot_assignments_folium(individual):\n",
                "\n",
                "    # Make a homes GeoDF\n",
                "    homes_geo_df = lsoas[lsoas['lsoa11cd'].isin(home_codes)]\n",
                "    homes_geo_df = homes_geo_df.set_index('lsoa11cd')\n",
                "    homes_geo_df['centroid'] = homes_geo_df.geometry.centroid\n",
                "\n",
                "    sites_geo_df = lsoas[lsoas['lsoa11cd'].isin(current_site_list)]\n",
                "    sites_geo_df = sites_geo_df.set_index('lsoa11cd')\n",
                "    sites_geo_df['centroid'] = sites_geo_df.geometry.centroid\n",
                "\n",
                "    # Convert centroids to latitude and longitude\n",
                "    homes_centroids_ll = homes_geo_df['centroid'].to_crs(epsg=4326)\n",
                "    sites_centroids_ll = sites_geo_df['centroid'].to_crs(epsg=4326)\n",
                "    \n",
                "    # Calculate the mean latitude and longitude\n",
                "    center_latitude = homes_centroids_ll.apply(lambda p: p.y).mean()\n",
                "    center_longitude = homes_centroids_ll.apply(lambda p: p.x).mean()\n",
                "\n",
                "    # Create a map centered at the mean coordinates\n",
                "\n",
                "    m = folium.Map(location=[center_latitude, center_longitude], zoom_start=7, width='100%', height='100%')\n",
                "\n",
                "    \n",
                "    # Add the site locations as blue markers\n",
                "    for point in sites_centroids_ll:\n",
                "        folium.Marker([point.y, point.x], icon=folium.Icon(color=\"blue\")).add_to(m)\n",
                "\n",
                "    # Add the home locations as small red markers\n",
                "    for point in homes_centroids_ll:\n",
                "        folium.CircleMarker([point.y, point.x], radius=2, color=\"red\").add_to(m)\n",
                "\n",
                "    # Plot lines from home to site\n",
                "    for home_idx, site_idx in enumerate(individual):\n",
                "        home_code = home_lsoas[home_idx]\n",
                "        site_code = current_site_list[site_idx]\n",
                "        \n",
                "        if home_code in homes_geo_df.index and site_code in sites_geo_df.index:\n",
                "            home_point = homes_centroids_ll.loc[home_code]\n",
                "            site_point = sites_centroids_ll.loc[site_code]\n",
                "            coords = [[home_point.y, home_point.x], [site_point.y, site_point.x]]\n",
                "            folium.PolyLine(coords, color=\"grey\", weight=1, opacity=0.8).add_to(m)\n",
                "            \n",
                "    fig = Figure(width=600, height=400)\n",
                "    fig.add_child(m)  \n",
                "         \n",
                "    return fig\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "azdata_cell_guid": "7a2a9c14-7da0-4ce1-855d-2207f9721964",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "calc_percs = True \n",
                "\n",
                "filtered_activities = pd.DataFrame()\n",
                "\n",
                "if calc_percs:\n",
                "    activities_to_rank = filtered_activities.copy()\n",
                "    home_lsoa_codes = sorted(activities_to_rank.index.unique().tolist())\n",
                "\n",
                "    def get_sites(home, num_sites=None):\n",
                "        # If num_sites is None, return all sites\n",
                "        num_sites = num_sites if num_sites is not None else len(site_codes)\n",
                "        # Returns a list of Site_LSOA codes sorted by distance (nearest first)\n",
                "        sorted_sites = sorted(site_codes, key=lambda site: travel_times_dict.get((home, site), float('inf')))\n",
                "        return sorted_sites[:num_sites]\n",
                "\n",
                "    nearby_sites = {home: get_sites(home) for home in home_lsoa_codes}\n",
                "\n",
                "    def determine_site_ranking(data, nearby_sites):\n",
                "        # Add a new column to the data for ranking\n",
                "        data['Ranking'] = None\n",
                "        for index, row in data.iterrows():\n",
                "            home_lsoa = index  # Accessing the index\n",
                "            site_lsoa = row['SiteLSOA']\n",
                "            if home_lsoa in nearby_sites:\n",
                "                try:\n",
                "                    rank = nearby_sites[home_lsoa].index(site_lsoa) + 1  # Adding 1 to start ranking from 1\n",
                "                    data.at[index, 'Ranking'] = rank\n",
                "                except ValueError:\n",
                "                    # Site LSOA not in the list for this home LSOA\n",
                "                    data.at[index, 'Ranking'] = None\n",
                "            else:\n",
                "                data.at[index, 'Ranking'] = None\n",
                "\n",
                "    determine_site_ranking(activities_to_rank, nearby_sites)\n",
                "\n",
                "    def calculate_percentages(data):\n",
                "        ranking_counts = data['Ranking'].value_counts()\n",
                "        total_counts = data['Ranking'].count()  # Count only non-null rankings\n",
                "        percentages = (ranking_counts / total_counts) * 100\n",
                "        return percentages.sort_index()\n",
                "\n",
                "    percentages = calculate_percentages(activities_to_rank)\n",
                "\n",
                "    percentages \n",
                "    \n",
                "    def calculate_cumulative_probabilities(probabilities):\n",
                "        total = sum(probabilities)\n",
                "        normalized_probs = [p / total for p in probabilities]\n",
                "        cumulative_probs = []\n",
                "        cumulative_sum = 0\n",
                "        for p in normalized_probs:\n",
                "            cumulative_sum += p\n",
                "            cumulative_probs.append(cumulative_sum)\n",
                "        return cumulative_probs\n",
                "\n",
                "    cumulative_probs = calculate_cumulative_probabilities(percentages)\n",
                "\n",
                "    cumulative_probs\n",
                "\n",
                "# 1     65.824697\n",
                "# 2     14.283368\n",
                "# 3      6.887122\n",
                "# 4      3.835668\n",
                "# 5      2.270525\n",
                "# 6      2.157204\n",
                "# 7      0.976367\n",
                "# 8      0.979651\n",
                "# 9      0.283302\n",
                "# 10     0.194616\n",
                "# 11     0.463138\n",
                "# 12     0.156843\n",
                "# 13     0.068978\n",
                "# 14     0.394981\n",
                "# 15     0.918885\n",
                "# 16     0.128923\n",
                "# 17     0.135492\n",
                "# 18     0.000821\n",
                "# 19     0.003285\n",
                "# 20     0.026277\n",
                "# 22     0.009854"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "8e0c380f-b2d3-44a0-aecc-9787a72e575b"
            },
            "source": [
                "So our simple nearest assignment has distinct limitations and does not allow us to balance any other competing priorities. \n",
                "\n",
                "This kind of problem is a variant of the travelling salesman problem (https://en.wikipedia.org/wiki/Travelling_salesman_problem) which is NP-Hard meaning that it is too computationally expensive to compute all possible solutions and find the best one. Therefore needs to be approached and solved using a heuristic approach.\n",
                "\n",
                "For this purpose we are going to use a genetic algorithm to enable us to balance competing priorities and come up with a balanced good solution. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "0de44856-939a-4918-94ce-0d71bd197c55"
            },
            "source": [
                "We will need to define the parameters for our genetic algorithm\n",
                "\n",
                "    * Population Size\n",
                "    * Chance to cross breed\n",
                "    * Mutation Probabilities\n",
                "    * Max number of generations to breed\n",
                "\n",
                "The base mutation rate probability is adaptive and increased based on the stagnation of both the size and diversity of the pareto front, but we can also set the probability that elements of an individual will be mutated once the individual has been selected for mutation.\n",
                "\n",
                "Cessation of the process is also controlled by stagnation in the pareto front, once the mutation rate has been increased and yet still no improvements have been realised in both size and diversity of the pareto front for a number of generations then the evolution is stopped "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "azdata_cell_guid": "8f160298-8950-4f30-8a05-ceccf525f15b",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "# Let us set up sone of the parameters for the evolution of our solution\n",
                "# number of solutions in a population\n",
                "pop_num = 200\n",
                "# percentage chance to cross breed one solution with another\n",
                "cross_chance = 0.3\n",
                "# percentage chance to introduce random mutations into the solutions, % of selected individuals\n",
                "initial_mutation_prob = 0.05\n",
                "# maximum percentage chance to introduce random mutations into the solutions, % of selected individuals\n",
                "max_mutation_prob = 0.5\n",
                "# percentage chance to introduce random mutations into the individuals selected for mutation\n",
                "individual_mutation_prob = 0.5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "5cd8bbf2-765a-43bb-a395-49ab003a8dfb"
            },
            "source": [
                "Now we will set up and run our evolutionary algorithm. The most important part is the custom evaluation function. Most of the population, generations, breeding and mutating is handled by the DEAP library, but we need to define our own custom function to assess the fitness of each solution. These scores are then used to find the best individual solutions in each generation to breed off and mutate in later generations to evelove the population towards a 'good' solution to our problem with competing priorities."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "95636d72-1499-44be-8232-9f86bde3dff8"
            },
            "source": [
                "Lets add all our competing priorities in to our evaluation function as per the source paper: https://www.journalslibrary.nihr.ac.uk/hsdr/hsdr06350/#/abstract\n",
                "\n",
                "We want to:\n",
                "\n",
                "    * Minimise the average travel time\n",
                "    * Maximise the proportion within 30 minutes\n",
                "    * Minimise the maximum distance for any assignment\n",
                "    * Maximise the number taking place in units with more than x admissions per year\n",
                "    * Maximise the smallest number of admissions per year  \n",
                "    * Minimise the largest number of admissions per year \n",
                "    * Maximise the proportion within 30 minutes and in units with more than x admissions per year\n",
                "\n",
                "The fourth and final of these are different in this approach as we are not working with admissions data but with critical care information, what we will model instead here is whether a NICU, LNU, and SCBU site meets the minimum required number of days as set out in the BAPM standards https://hubble-live-assets.s3.amazonaws.com/bapm/file_asset/file/1494/BAPM_Service_Quality_Standards_FINAL.pdf and we will look at the proiportion of activities taking place in the nicu sites as a general positive given these sites are the most specialised.\n",
                "\n",
                "So we have:\n",
                "\n",
                "    * Minimise the average travel time\n",
                "    * Maximise the proportion within 30 minutes\n",
                "    * Minimise the maximum distance for any assignment\n",
                "    * Maximise the number taking place in level 3 nicu units\n",
                "    * Maximise the smallest number of admissions per year  \n",
                "    * Minimise the largest number of admissions per year \n",
                "    * Maximise the proportion within 30 minutes and in in level 3 nicu units\n",
                "\n",
                "We can also adjust the weightings that we give to each of these should we want to."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "azdata_cell_guid": "19c2f4fd-7669-4b2a-be7e-50913684d62c",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "# Let us set up variables for the weightings\n",
                "min_travel_time         = -1.0\n",
                "max_in_30               = 1.0\n",
                "min_max_distance        = -1.0\n",
                "max_large_unit          = 1.0\n",
                "max_min_no              = 1.0\n",
                "min_max_no              = -1.0\n",
                "max_in_30_and_large     = 1.0\n",
                "max_large_nicu          = 1.0\n",
                "\n",
                "# Define the threshold for minimum admissions\n",
                "nicu_activities_threshold = 2000  # set to 1000 to make the algorithm reach the threshold of over lnu range and insentivise those solutions\n",
                "lnu_activities_threshold = 1000  \n",
                "scbu_activities_threshold = 500\n",
                "\n",
                "# Using this we can provide particular objectives to our evolutionary process\n",
                "# must be structured like this {\n",
                "#     'E01024897': {'NICU': {'min': 0, 'max': 500}}\n",
                "#     ,'E01005062': {'NICU': {'min': 4000}}\n",
                "#     }\n",
                "# can provide both minimums, maximums to any existing site and any activity level\n",
                "activity_limits = set()\n",
                "\n",
                "# Sites that should not be assigned to any home, for modelling full site closures\n",
                "restricted_sites = set()\n",
                "\n",
                "# Do we want to propose a new site, we can add the LSOA of the proposed site and run our process against it\n",
                "# E01012632 would be blackburn hospital\n",
                "proposed_additions = list()\n",
                "\n",
                "# Activity to focus on in the evolutionary assignment\n",
                "activity_focus = list()\n",
                "\n",
                "# We can also add an extreme individual to the population this is to ensure that the population space contains \n",
                "# the most optimal fitness for one of our evaluation metrics.. in this case the minimisation of travel time\n",
                "include_original_sites = False\n",
                "\n",
                "# Number of elite individuals to carry to the next generation\n",
                "num_elites = elite_pop\n",
                "\n",
                "# normalisation boundaries, these are bnased an pragmatic known results, these could need further evaluation\n",
                "min_avg_time = 10\n",
                "max_avg_time = 200\n",
                "min_prop_within_30_mins = 0.0\n",
                "max_prop_within_30_mins = 1\n",
                "min_min_max_distance = 280\n",
                "max_min_max_distance = 440\n",
                "min_number_of_sites_over_nicu_threshold = 0.2\n",
                "max_number_of_sites_over_nicu_threshold = 0.8\n",
                "min_smallest_site = 4000 \n",
                "max_smallest_site = 14000\n",
                "min_largest_site = 18000 \n",
                "max_largest_site = 36000\n",
                "min_constraint_adherence = 0 \n",
                "max_constraint_adherence = 3000\n",
                "min_prop_within_30_mins_and_large_NICU = 0.05 \n",
                "max_prop_within_30_mins_and_large_NICU = 0.20\n",
                "min_max_large_nicu = 0\n",
                "max_max_large_nicu = 4000"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "412297cc-e16c-4f8d-bd0a-4cc98d3de544"
            },
            "source": [
                "Let us add these priorities in to our evaluation function algorithm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {
                "azdata_cell_guid": "8f63c30e-3d13-40cb-a990-a14891860aa0",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "creator.create(\"FitnessMulti\", base.Fitness, weights=(min_travel_time\n",
                "                                                      , max_in_30\n",
                "                                                      , min_max_distance\n",
                "                                                      , max_large_unit\n",
                "                                                      , max_min_no\n",
                "                                                      , min_max_no\n",
                "                                                    #   , constraint_adherence\n",
                "                                                      , max_in_30_and_large\n",
                "                                                      , max_large_nicu\n",
                "                                                      ))\n",
                "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
                "\n",
                "toolbox = base.Toolbox()\n",
                "\n",
                "num_sites = 0\n",
                "\n",
                "current_site_list = []\n",
                "\n",
                "def prepare_site_list(site_codes, proposed_additions, restricted_sites):\n",
                "    combined_sites = site_codes + proposed_additions\n",
                "    filtered_sites = [site for site in combined_sites if site not in restricted_sites]\n",
                "    return filtered_sites\n",
                "\n",
                "# Function to asign a random site to each individual in the population but allow us to add or remove sites\n",
                "def restricted_random_site():\n",
                "    # global proposed_additions\n",
                "    # # Inside restricted_random_site function\n",
                "    # filtered_additions = [site for site in proposed_additions if site not in restricted_sites]\n",
                "    # working_site_list = num_sites + len(filtered_additions)\n",
                "    # valid_sites = set(range(working_site_list)) - restricted_site_indeces\n",
                "    # return random.choice(list(valid_sites))\n",
                "    return random.choice(range(len(current_site_list)))\n",
                "\n",
                "\n",
                "# # function to re-index the sites in the individual based on the adjusted list\n",
                "# def index_of_site_code(individual):\n",
                "#     used_sites = site_codes + proposed_additions\n",
                "#     return [used_sites.index(site) for site in individual]\n",
                "\n",
                "restricted_site_indeces = {site_codes.index(code) for code in restricted_sites}\n",
                "\n",
                "toolbox.register(\"random_site\", restricted_random_site)\n",
                "\n",
                "# Create an extreme individual based on the sites in the data using most frequent where more than one site\n",
                "\n",
                "def nearest_allowed_site(home, restricted_site_indeces, passed_sites, travel_times_dict):\n",
                "    # Find the nearest non-restricted site\n",
                "    valid_sites_indices = [i for i in range(len(passed_sites)) if i not in restricted_site_indeces]\n",
                "    nearest_site_idx = min(valid_sites_indices, key=lambda site_idx: travel_times_dict.get((home, passed_sites[site_idx]), float('inf')))\n",
                "    return nearest_site_idx\n",
                "\n",
                "def create_individual_based_on_data(most_frequent_sites, home_lsoas, passed_sites, restricted_site_indeces):\n",
                "    site_code_indices = {code: idx for idx, code in enumerate(passed_sites)}\n",
                "    \n",
                "    site_index_map = {}\n",
                "    for _, row in most_frequent_sites.iterrows():\n",
                "        home_code = row['Der_Postcode_LSOA_Code']\n",
                "        site_code = row['SiteLSOA']\n",
                "        site_idx = site_code_indices.get(site_code)\n",
                "        \n",
                "        # Check if site is not restricted. Find nearest non restricted site if it is\n",
                "        if site_idx is not None and site_idx not in restricted_site_indeces:\n",
                "            site_index_map[home_code] = site_idx\n",
                "        else:\n",
                "            # Assign nearest non-restricted site index\n",
                "            site_index_map[home_code] = nearest_allowed_site(home_code, restricted_site_indeces, passed_sites, travel_times_dict)\n",
                "\n",
                "    # Build the individual based on the most frequented site index or nearest allowed site\n",
                "    individual = [site_index_map.get(home, nearest_allowed_site(home, restricted_site_indeces, passed_sites, travel_times_dict)) for home in home_lsoas]\n",
                "\n",
                "    return creator.Individual(individual)\n",
                "\n",
                "# def create_extreme_individual():\n",
                "#     individual = []\n",
                "#     for home_idx, home in enumerate(home_lsoas):\n",
                "#         nearest_site_idx = min(range(num_sites), key=lambda site_idx: travel_times_dict.get((home, all_sites[site_idx]), float('inf')))\n",
                "#         individual.append(nearest_site_idx)\n",
                "#     return creator.Individual(individual)\n",
                "\n",
                "def create_extreme_individual():\n",
                "    individual = []\n",
                "    for home_idx, home in enumerate(home_lsoas):\n",
                "        nearest_site_idx = nearest_allowed_site(home, restricted_site_indeces, current_site_list, travel_times_dict)\n",
                "        individual.append(nearest_site_idx)\n",
                "    return creator.Individual(individual)\n",
                "\n",
                "\n",
                "def init_population(n):\n",
                "    population = []\n",
                "    z = 0\n",
                "    # Add the extreme individual if flagged to\n",
                "    if include_extreme_individual:\n",
                "        population.append(create_extreme_individual())\n",
                "        z += 1\n",
                "    # Add the individual based on the actual data if flagged to\n",
                "    if include_original_sites:\n",
                "        population.append(create_individual_based_on_data(most_frequent_sites, home_lsoas, current_site_list, restricted_site_indeces))\n",
                "        z += 1\n",
                "    # Fill the rest of the population with individuals with randomly assigned sites\n",
                "    for _ in range(n - z):\n",
                "        population.append(toolbox.individual())\n",
                "    return population\n",
                "\n",
                "toolbox.register(\"population\", init_population)\n",
                "\n",
                "def create_logs_df():\n",
                "    column_types = {'individual': 'str',\n",
                "                    'avg_time': 'float64'\n",
                "                    ,'prop_within_30_mins': 'float64'\n",
                "                    ,'max_distance': 'float64'\n",
                "                    ,'units_over_x': 'float64'\n",
                "                    ,'smallest_site': 'float64'\n",
                "                    ,'largest_site': 'float64'\n",
                "                    ,'max_in_30_and_large': 'float64'\n",
                "                    ,'totals': 'float64'\n",
                "                    ,'large_nicu': 'float64'\n",
                "                    }\n",
                "\n",
                "    # Create a DataFrame with the specified columns and data types\n",
                "    logs_df = pd.DataFrame(columns=column_types.keys()).astype(column_types)\n",
                "    return logs_df\n",
                "\n",
                "inner_log_df = pd.DataFrame(columns=['site',\n",
                "                                    'home',\n",
                "                                    'activity_type',\n",
                "                                    'activity_counts'])\n",
                "\n",
                "activity_log_df = pd.DataFrame(columns=['Generation', 'Site', 'HDU', 'SCBU', 'NICU'])\n",
                "\n",
                "def calculate_activity_counts(individual):\n",
                "    activity_counts = defaultdict(lambda: [0, 0, 0])  # Initialize counts for each activity at each site\n",
                "    # used_sites = site_codes + proposed_additions  # Combine existing and proposed sites\n",
                "    # Iterate over each home-site pair\n",
                "    for home_idx, site_idx in enumerate(individual):\n",
                "        site = current_site_list[site_idx]  # Get the site assigned to this home\n",
                "        home_activity_counts = home_activities[home_idx]  # Get the activity counts for this home\n",
                "        # Aggregate activities at the assigned site\n",
                "        for i in range(len(home_activity_counts)):\n",
                "            activity_counts[site][i] += home_activity_counts[i]\n",
                "    return activity_counts\n",
                "\n",
                "\n",
                "def is_feasible(individual):\n",
                "    activity_counts = calculate_activity_counts(individual)\n",
                "    for site, counts in activity_counts.items():\n",
                "        if site in activity_limits:\n",
                "            for i, activity in enumerate(['HDU', 'SCBU', 'NICU']):\n",
                "                limits = activity_limits[site].get(activity)\n",
                "                if limits:\n",
                "                    if counts[i] < limits.get('min', 0) or counts[i] > limits.get('max', float('inf')):\n",
                "                        return False\n",
                "    return True\n",
                "\n",
                "def distance_to_feasibility(individual):\n",
                "    distance = 0\n",
                "    activity_counts = calculate_activity_counts(individual)\n",
                "    for site, counts in activity_counts.items():\n",
                "        if site in activity_limits:\n",
                "            for i, activity in enumerate(['HDU', 'SCBU', 'NICU']):\n",
                "                limits = activity_limits[site].get(activity)\n",
                "                if limits:\n",
                "                    excess = max(0, counts[i] - limits.get('max', float('inf')))\n",
                "                    shortfall = max(0, limits.get('min', 0) - counts[i])\n",
                "                    distance += excess + shortfall\n",
                "    return distance\n",
                "\n",
                "base_penalty = 1.0  # Base penalty\n",
                "penalty_factor = 1.1  # Exponential factor\n",
                "\n",
                "# def exponential_penalty(individual):\n",
                "#     distance = distance_to_feasibility(individual)\n",
                "#     penalty_value = base_penalty * (penalty_factor ** distance)\n",
                "\n",
                "#     weights = creator.FitnessMulti.weights\n",
                "\n",
                "#     print(f\"Distance: {distance}, Penalty Value: {penalty_value}\")\n",
                "\n",
                "#     penalties = []\n",
                "#     for weight in weights:\n",
                "#         if weight > 0:  # Penalise maximisation\n",
                "#             penalties.append(-penalty_value)\n",
                "#         else:           # Penalise minimisation\n",
                "#             penalties.append(penalty_value)\n",
                "\n",
                "#     print(f\"Penalties: {penalties}\")\n",
                "#     return tuple(penalties)\n",
                "\n",
                "# def simple_penalty(individual):\n",
                "#     # Just return a fixed penalty for testing\n",
                "#     return (-10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0)\n",
                "\n",
                "# Normalization function in order that no parameter dominations the evolutionary process simply due to its scale\n",
                "def normalize(raw_value, min_value, max_value):\n",
                "    return (raw_value - min_value) / (max_value - min_value)\n",
                "\n",
                "def eval_func(individual, activity_focus=None):\n",
                "    global inner_log_df, logs_df \n",
                "    \n",
                "    # Initialize accumulators and counters\n",
                "    total_time = 0\n",
                "    total_population = 0\n",
                "    within_30_mins = 0\n",
                "    # constraint_adherence = 0\n",
                "    total_time_activity_weighted = 0\n",
                "    total_activity_count = 0\n",
                "\n",
                "    # Combine existing and proposed sites\n",
                "    # used_sites = site_codes + proposed_additions\n",
                "\n",
                "    # Calculate activity counts for each site\n",
                "    activity_counts = calculate_activity_counts(individual)\n",
                "\n",
                "    missing_combinations = [] \n",
                "    \n",
                "    # Loop over each home-site pair in the individual\n",
                "    for home_idx, site_idx in enumerate(individual):\n",
                "        home = home_lsoas[home_idx]\n",
                "        site = current_site_list[site_idx]\n",
                "        key = (home, site)\n",
                "        if key not in travel_times_dict:\n",
                "            missing_combinations.append(key)\n",
                "\n",
                "        if (home, site) in travel_times_dict:\n",
                "            travel_time = travel_times_dict[(home, site)]\n",
                "            total_time += travel_time * home_populations[home_idx]\n",
                "            total_population += home_populations[home_idx]\n",
                "\n",
                "            if travel_time <= 30:\n",
                "                within_30_mins += home_populations[home_idx]\n",
                "\n",
                "            activity_counts_per_home = home_activities[home_idx]\n",
                "            for activity_count in activity_counts_per_home:\n",
                "                total_time_activity_weighted += travel_time * home_populations[home_idx] * activity_count\n",
                "                total_activity_count += activity_count\n",
                "\n",
                "    # # Check for activity count violations (constraint adherence)\n",
                "    # for site, counts in activity_counts.items():\n",
                "    #     if site in activity_limits:\n",
                "    #         for i, activity in enumerate(['HDU', 'SCBU', 'NICU']):\n",
                "    #             limit = activity_limits[site].get(activity, float('inf'))\n",
                "    #             constraint_adherence += max(0, counts[i] - limit)\n",
                "\n",
                "    # Calculations for average time, max distance, etc.\n",
                "    avg_time = total_time / total_population if total_population else 0\n",
                "    avg_time_activity_weighted = total_time_activity_weighted / total_activity_count if total_activity_count else 0\n",
                "    prop_within_30_mins = within_30_mins / total_population if total_population else 0\n",
                "    travel_times = [travel_times_dict[(home_lsoas[home_idx], current_site_list[site_idx])]\n",
                "                for home_idx, site_idx in enumerate(individual)\n",
                "                if (home_lsoas[home_idx], current_site_list[site_idx]) in travel_times_dict]\n",
                "\n",
                "    max_distance = max(travel_times, default=0)  # default=0 handles empty list\n",
                "\n",
                "\n",
                "    site_activities = {site: sum(counts) for site, counts in activity_counts.items()}\n",
                "    \n",
                "    #print(site_activities)\n",
                "    \n",
                "    smallest_site = min(site_activities.values())\n",
                "    largest_site = max(site_activities.values())\n",
                "    \n",
                "    min_max_values = [\n",
                "        (min_avg_time, max_avg_time), \n",
                "        (min_prop_within_30_mins, max_prop_within_30_mins),\n",
                "        (min_min_max_distance, max_min_max_distance),\n",
                "        (min_number_of_sites_over_nicu_threshold, max_number_of_sites_over_nicu_threshold ),\n",
                "        (min_smallest_site, max_smallest_site),\n",
                "        (min_largest_site, max_largest_site),\n",
                "        (min_prop_within_30_mins_and_large_NICU, max_prop_within_30_mins_and_large_NICU),\n",
                "        (min_max_large_nicu, max_max_large_nicu)\n",
                "    ]\n",
                "    if not site_activities:\n",
                "        return [0] * len(min_max_values)  # Return a list of zeroes for each objective, or handle as appropriate\n",
                "    \n",
                "    # Count the number of sites that meet or exceed the threshold for NICU activities\n",
                "    NICU_INDEX = 2\n",
                "    HDU_INDEX = 0\n",
                "    \n",
                "    # Find the sites that meet the NICU threshold\n",
                "    nicu_sites = [site for site, counts in activity_counts.items() if counts[NICU_INDEX] >= nicu_activities_threshold]\n",
                "    # number_of_sites_over_nicu_threshold = len(nicu_sites)\n",
                "    large_nicu = [counts[NICU_INDEX] for site, counts in activity_counts.items()]\n",
                "    large_nicu_count = max(large_nicu)\n",
                "    \n",
                "    # Calculate the total NICU activity count across all sites\n",
                "    total_nicu_activities = sum(counts[NICU_INDEX] for site, counts in activity_counts.items())\n",
                "    # Calculate the NICU activity count at sites that exceed the threshold\n",
                "    over_threshold_nicu_activities = sum(counts[NICU_INDEX] for site, counts in activity_counts.items() if counts[NICU_INDEX] >= nicu_activities_threshold)\n",
                "    # Calculate the proportion of NICU activities that are at sites over the threshold\n",
                "    proportion_over_threshold_nicu_activities = (over_threshold_nicu_activities / total_nicu_activities \n",
                "                                                if total_nicu_activities != 0 else 0)\n",
                "\n",
                "    # grand_total = sum(sum(values) for key, values in activity_counts.items())\n",
                "    # print(f\"Total Activities Assigned: {grand_total}\")\n",
                "    \n",
                "    # print(individual.id)\n",
                "    # print(activity_counts)\n",
                "    # print(f\"number_of_sites_over_nicu_threshold: {number_of_sites_over_nicu_threshold}\")\n",
                "    # print(f\"nicu_sites: {nicu_sites}\")\n",
                "    \n",
                "    # lnu_sites = [site for site, counts in activity_counts.items() if counts[NICU_INDEX]+counts[HDU_INDEX] >= 1000]\n",
                "\n",
                "    # Calculate the population within 30 minutes and going to a large NICU site\n",
                "    within_30_mins_and_large_NICU = 0\n",
                "    for home_idx, site_idx in enumerate(individual):\n",
                "        home = home_lsoas[home_idx]\n",
                "        site = current_site_list[site_idx]\n",
                "        travel_time = travel_times_dict.get((home, site), float('inf'))\n",
                "        if travel_time <= 30 and site in nicu_sites:\n",
                "            within_30_mins_and_large_NICU += home_populations[home_idx]\n",
                "            \n",
                "    # Calculate the proportion (or 0 if total_population is 0)\n",
                "    prop_within_30_mins_and_large_NICU = within_30_mins_and_large_NICU / total_population if total_population != 0 else 0\n",
                "\n",
                "    # Create a new DataFrame from the dictionary\n",
                "    new_row = pd.DataFrame([{\n",
                "        'individual': individual.index,  \n",
                "        'avg_time': avg_time,\n",
                "        'prop_within_30_mins': prop_within_30_mins,\n",
                "        'max_distance': max_distance,\n",
                "        'units_over_x': proportion_over_threshold_nicu_activities,\n",
                "        'smallest_site': smallest_site,\n",
                "        'largest_site': largest_site,\n",
                "        'totals' : total_population,\n",
                "        'activity_counts': activity_counts,\n",
                "        'large_nicu': large_nicu_count\n",
                "    }])\n",
                "\n",
                "    # Concatenate the new DataFrame with the existing one\n",
                "    logs_df = pd.concat([logs_df, new_row], ignore_index=True)\n",
                "\n",
                "\n",
                "    # Raw objective values\n",
                "    raw_objectives = [\n",
                "        avg_time, \n",
                "        prop_within_30_mins, \n",
                "        max_distance, \n",
                "        proportion_over_threshold_nicu_activities,\n",
                "        smallest_site, \n",
                "        largest_site, \n",
                "        prop_within_30_mins_and_large_NICU,\n",
                "        large_nicu_count\n",
                "    ]\n",
                "    \n",
                "    # Normalize objectives\n",
                "    normalized_objectives = [\n",
                "        normalize(raw, min_val, max_val) \n",
                "        for raw, (min_val, max_val) in zip(raw_objectives, min_max_values)\n",
                "    ]\n",
                "\n",
                "    # return (avg_time,\n",
                "    #         prop_within_30_mins,\n",
                "    #         max_distance,\n",
                "    #         proportion_over_threshold_nicu_activities,\n",
                "    #         smallest_site,\n",
                "    #         largest_site,\n",
                "    #         prop_within_30_mins_and_large_NICU,\n",
                "    #         large_nicu_count)\n",
                "            \n",
                "    return normalized_objectives\n",
                "\n",
                "# Random mutation function\n",
                "def restricted_mutUniformInt(individual, low, up, indpb):\n",
                "    for i, site_index in enumerate(individual):\n",
                "        if random.random() < indpb:\n",
                "            individual[i] = restricted_random_site()\n",
                "    return individual,\n",
                "\n",
                "# Let us also create an alternative mutation function which limits the choice of site to one of the 3 nearest rather than any\n",
                "# This should reflect the more realistic real world scenario whereby travel is more limited to nearer sites\n",
                "def get_nearby_sites(home, mutation_depth_no=restricted_mutation_depth):\n",
                "    # Returns a list of site indices sorted by distance (nearest first)\n",
                "    sorted_sites = sorted(range(len(current_site_list)), key=lambda site_idx: travel_times_dict.get((home, current_site_list[site_idx]), float('inf')))\n",
                "    return sorted_sites[:mutation_depth_no]\n",
                "\n",
                "def restricted_mutNearbyInt(individual, indpb, nearby_sites):\n",
                "    for i, site_index in enumerate(individual):\n",
                "        if random.random() < indpb:\n",
                "            home = home_lsoas[i]\n",
                "            if home in nearby_sites:\n",
                "                # Choose from nearby random site\n",
                "                individual[i] = random.choice(nearby_sites[home])\n",
                "            else:\n",
                "                # Fallback to random if nearby info is not available\n",
                "                individual[i] = restricted_random_site()\n",
                "    return individual,\n",
                "\n",
                "# # Following, there is another alternative mutation assigning nearby sites \n",
                "# # based on the real data distribution of sites based on travel times\n",
                "\n",
                "def weighted_random_choice(cumulative_probs):\n",
                "    rnd = random.random()\n",
                "    for i, prob in enumerate(cumulative_probs):\n",
                "        if rnd <= prob:\n",
                "            return i\n",
                "    return len(cumulative_probs) - 1  # Fallback in case of rounding errors\n",
                "\n",
                "def weighted_mutation_function(individual, indpb, cumulative_probs):\n",
                "    for i, _ in enumerate(individual):\n",
                "        if random.random() < indpb:\n",
                "            new_site_index = weighted_random_choice(cumulative_probs)\n",
                "            individual[i] = new_site_index\n",
                "    return individual,\n",
                "\n",
                "# Create a partial function that has activity_focus pre-specified\n",
                "eval_func_focused = partial(eval_func, activity_focus=activity_focus)\n",
                "\n",
                "toolbox.register(\"evaluate\", eval_func_focused)\n",
                "toolbox.decorate(\"evaluate\", tools.DeltaPenalty(is_feasible, 7.0, distance_to_feasibility))\n",
                "\n",
                "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
                "\n",
                "# Generate reference points for NSGA3\n",
                "# Parameters\n",
                "NOBJ = 8\n",
                "P = [2, 1]\n",
                "SCALES = [1, 0.5]\n",
                "\n",
                "# Create, combine and removed duplicates\n",
                "ref_points = [tools.uniform_reference_points(NOBJ, p, s) for p, s in zip(P, SCALES)]\n",
                "ref_points = np.concatenate(ref_points, axis=0)\n",
                "_, uniques = np.unique(ref_points, axis=0, return_index=True)\n",
                "ref_points = ref_points[uniques] \n",
                "\n",
                "history = tools.History()\n",
                "\n",
                "# ADAPTIVE STRATEGY FOR MUTATION RATE\n",
                "\n",
                "def adapt_mutation_rate_based_on_stagnation(generations_since_improvement, threshold, initial_mutation_prob, max_mutation_prob):\n",
                "    if generations_since_improvement > threshold:\n",
                "        # Increase mutation probability up to a maximum\n",
                "        return min(initial_mutation_prob * (1 + generations_since_improvement / threshold), max_mutation_prob)\n",
                "    else:\n",
                "        return initial_mutation_prob\n",
                "    \n",
                "def calculate_diversity(front):\n",
                "    if len(front) < 2:\n",
                "        return 0\n",
                "\n",
                "    distances = []\n",
                "    for i in range(len(front) - 1):\n",
                "        dist = np.linalg.norm(np.array(front[i].fitness.values) - np.array(front[i+1].fitness.values))\n",
                "        distances.append(dist)\n",
                "\n",
                "    return np.mean(distances)\n",
                "\n",
                "def has_pareto_front_improved(current_front, previous_front, diversity_threshold):\n",
                "    if previous_front is None:\n",
                "        return True\n",
                "\n",
                "    current_size = len(current_front)\n",
                "    previous_size = len(previous_front)\n",
                "\n",
                "    if current_size > previous_size:\n",
                "        return True\n",
                "\n",
                "    if current_size > diversity_threshold:\n",
                "        current_diversity = calculate_diversity(current_front)\n",
                "        previous_diversity = calculate_diversity(previous_front)\n",
                "        # print(f\"Current diversity: {current_diversity} > Previous diversity: {previous_diversity}?\")\n",
                "        if current_diversity > previous_diversity:\n",
                "            return True\n",
                "    \n",
                "    return False\n",
                "\n",
                "initial_diversity_threshold = 0.10\n",
                "stagnation_limit = 20\n",
                "max_number_generations = 10000\n",
                "stagnation_threshold = 10\n",
                "\n",
                "def main():\n",
                "    \n",
                "    # restricted_site_indeces = {site_codes.index(code) for code in restricted_sites}\n",
                "    \n",
                "    if nsga3:\n",
                "        toolbox.register(\"select\", tools.selNSGA3, ref_points=ref_points)\n",
                "    else:\n",
                "        toolbox.register(\"select\", tools.selNSGA2)\n",
                "\n",
                "    # Define statistics for each objective\n",
                "    stats_time = tools.Statistics(key=lambda ind: ind.fitness.values[0])\n",
                "    stats_time.register(\"avg_time\", np.mean)\n",
                "\n",
                "    stats_prop = tools.Statistics(key=lambda ind: ind.fitness.values[1])\n",
                "    stats_prop.register(\"prop_within_30_mins\", np.max)\n",
                "    \n",
                "    stats_max_distance = tools.Statistics(key=lambda ind: ind.fitness.values[2])\n",
                "    stats_max_distance.register(\"max_distance\", np.mean)\n",
                "    \n",
                "    stats_large_sites = tools.Statistics(key=lambda ind: ind.fitness.values[3])\n",
                "    stats_large_sites.register(\"large_sites\", np.max)\n",
                "    \n",
                "    smallest_site_stats = tools.Statistics(key=lambda ind: ind.fitness.values[4])\n",
                "    smallest_site_stats.register(\"smallest_site\", np.max)\n",
                "    \n",
                "    largest_site_stats = tools.Statistics(key=lambda ind: ind.fitness.values[5])\n",
                "    largest_site_stats.register(\"largest_site\", np.max)\n",
                "    \n",
                "    thirty_and_large_stats = tools.Statistics(key=lambda ind: ind.fitness.values[6])\n",
                "    thirty_and_large_stats.register(\"30_and_large\", np.max)\n",
                "    \n",
                "    large_nicu_stats = tools.Statistics(key=lambda ind: ind.fitness.values[7])\n",
                "    large_nicu_stats.register(\"large_nicu\", np.max)\n",
                "    \n",
                "    # Combine statistics into MultiStatistics\n",
                "    mstats = tools.MultiStatistics(time=stats_time\n",
                "                                   , prop=stats_prop\n",
                "                                   , max_dist=stats_max_distance\n",
                "                                    , large_sites=stats_large_sites\n",
                "                                    ,smallest_site=smallest_site_stats\n",
                "                                    , largest_site=largest_site_stats,\n",
                "                                #    constraint_adherence = constraint_adherence_stats,\n",
                "                                   thirty_and_large = thirty_and_large_stats\n",
                "                                , large_nicu = large_nicu_stats\n",
                "                                   )\n",
                "\n",
                "    # Initialize and evaluate the population\n",
                "    pop = toolbox.population(n=pop_num)\n",
                "    history.update(pop)\n",
                "    hof = tools.HallOfFame(1)\n",
                "    paretofront = tools.ParetoFront()\n",
                "    fitnesses = map(toolbox.evaluate, pop)\n",
                "    for ind, fit in zip(pop, fitnesses):\n",
                "        ind.fitness.values = fit\n",
                "    bestie = ()\n",
                "\n",
                "    # Create a logbook and record initial statistics\n",
                "    logbook = tools.Logbook()\n",
                "    logbook.header = ['gen', 'nevals'] + (mstats.fields if mstats else [])\n",
                "    record = mstats.compile(pop) if mstats else {}\n",
                "    logbook.record(gen=0, nevals=len(pop), **record)\n",
                "    \n",
                "    # Function to select elite individuals for crossover\n",
                "    # def ranked_selection(population, k):\n",
                "    #     # Rank the population by fitness\n",
                "    #     sorted_pop = sorted(population, key=lambda ind: ind.fitness, reverse=True)\n",
                "    #     # Select the top k individuals\n",
                "    #     return sorted_pop[:k]\n",
                "\n",
                "    # # Number of individuals to select for crossover\n",
                "    # k = len(pop) // 2\n",
                "\n",
                "    generations_since_improvement = 0\n",
                "    previous_pareto_front = None\n",
                "    gen = 0\n",
                "\n",
                "    while generations_since_improvement < stagnation_limit and gen < max_number_generations:\n",
                "    \n",
                "        gen += 1\n",
                "        \n",
                "        # Update hall of fame and Pareto front (paretofront)\n",
                "        hof.update(pop)\n",
                "        paretofront.update(pop)\n",
                "        \n",
                "        current_pop = len(pop) * gen\n",
                "        diversity_threshold = initial_diversity_threshold * current_pop\n",
                "    \n",
                "        # Check if the Pareto front has improved\n",
                "        if has_pareto_front_improved(paretofront, previous_pareto_front, diversity_threshold):\n",
                "            generations_since_improvement = 0\n",
                "            # Store the current Pareto front as the previous front for the next generation\n",
                "            previous_pareto_front = list(paretofront)\n",
                "        else:\n",
                "            generations_since_improvement += 1\n",
                "            \n",
                "        mutation_prob = adapt_mutation_rate_based_on_stagnation(generations_since_improvement,stagnation_threshold,initial_mutation_prob,max_mutation_prob)\n",
                "        \n",
                "        # print(f\"Generation: {gen} / Pareto Front Size:{len(paretofront)} / Diversity threshold: {diversity_threshold}\")     \n",
                "        # print(f\"Mutation probability {mutation_prob}, at {generations_since_improvement} generations since improvement\")\n",
                "\n",
                "        # Select the next generation individuals\n",
                "        offspring = toolbox.select(pop, len(pop) - num_elites)\n",
                "        # Clone the selected individuals\n",
                "        offspring = list(map(toolbox.clone, offspring))\n",
                "        \n",
                "        # # Select individuals for crossover\n",
                "        # selected_for_crossover = ranked_selection(pop, k)\n",
                "\n",
                "        # # Apply crossover to elite selected individuals\n",
                "        # for child1, child2 in zip(selected_for_crossover[::2], selected_for_crossover[1::2]):\n",
                "        #     if np.random.rand() < cross_chance:\n",
                "        #         toolbox.mate(child1, child2)\n",
                "        #         del child1.fitness.values\n",
                "        #         del child2.fitness.values\n",
                "\n",
                "        # Apply crossover and mutation on the offspring\n",
                "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
                "            if np.random.rand() < cross_chance:\n",
                "                toolbox.mate(child1, child2)\n",
                "                del child1.fitness.values\n",
                "                del child2.fitness.values\n",
                "\n",
                "        for mutant in offspring:\n",
                "            if np.random.rand() < mutation_prob:\n",
                "                toolbox.mutate(mutant)\n",
                "                del mutant.fitness.values\n",
                "\n",
                "        # Evaluate the individuals with an invalid fitness\n",
                "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
                "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
                "        for ind, fit in zip(invalid_ind, fitnesses):\n",
                "            ind.fitness.values = fit\n",
                "                \n",
                "        # Select the elite individuals\n",
                "        elites = tools.selBest(pop, num_elites)\n",
                "        offspring.extend(elites)\n",
                "        pop[:] = offspring\n",
                "        \n",
                "        # Record statistics for this generation\n",
                "        record = mstats.compile(pop) if mstats else {}\n",
                "        logbook.record(gen=gen+1, nevals=len(invalid_ind), **record)\n",
                "        \n",
                "        sys.stdout.write(\"\\r        Generation: {}, Generations Since Improvement: {}  \".format(gen, generations_since_improvement))\n",
                "        sys.stdout.flush()\n",
                "            \n",
                "    bestie = tools.selBest(pop, 1)[0]\n",
                "    print(\" \")\n",
                "    \n",
                "    return pop, logbook, hof, paretofront, bestie"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "7618e7b1-44a5-45ef-bc71-17b3dffea54d"
            },
            "source": [
                "We can use DEAPs built in selBest tool to select the best individual from the population "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "azdata_cell_guid": "6d906ff1-aeef-4dbd-9fff-3c672f20cc1e",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "# Here we translate the best individual (which is a list of site indices) into a list of (home_code, site_code) pairs\n",
                "def create_solution_list(bestind, home_lsoas, current_site_list):\n",
                "    solution = []\n",
                "    # used_sites = site_codes + proposed_additions\n",
                "    for i, site_index in enumerate(bestind):\n",
                "        home_code = home_lsoas[i]\n",
                "        site_code = current_site_list[site_index]\n",
                "        solution.append((home_code, site_code))\n",
                "    return solution  # return the solution list\n",
                "\n",
                "def add_solution(activities, solution, solution_number, activity_focus):\n",
                "    \n",
                "    solution_column_name = f'solution_{solution_number}'\n",
                "    solution_unit_name = f'solution_{solution_number}_unit'\n",
                "    \n",
                "    # Ensure the solution column exists\n",
                "    if solution_column_name not in activities.columns:\n",
                "        activities[solution_column_name] = np.nan\n",
                "    \n",
                "    # Convert the solution list to a dictionary for faster lookup\n",
                "    solution_dict = dict(solution)\n",
                "    \n",
                "    # Iterate over the activities DataFrame and update where conditions match\n",
                "    for idx, row in activities.iterrows():\n",
                "        if (not activity_focus or row['CC_Level'] in activity_focus) and row['Der_Postcode_LSOA_Code'] in solution_dict:\n",
                "            activities.at[idx, solution_column_name] = solution_dict[row['Der_Postcode_LSOA_Code']]\n",
                "            \n",
                "    # Drop the solution_unit_name column if it exists\n",
                "    if solution_unit_name in activities.columns:\n",
                "        activities = activities.drop(solution_unit_name, axis=1)\n",
                "    \n",
                "    # Merge and then drop the LSOA column, ensuring the merged column name is correct\n",
                "    merged_df = pd.merge(activities, sites[['LSOA', 'UnitCode']], left_on=solution_column_name, right_on='LSOA', how='left')\n",
                "    merged_df = merged_df.drop('LSOA', axis=1)\n",
                "    merged_df.rename(columns={'UnitCode': solution_unit_name}, inplace=True)\n",
                "    \n",
                "    return merged_df\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "azdata_cell_guid": "b6cb1194-c905-482b-8017-acfe38445990",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "def export_log(solution_id, timestamp):\n",
                "    # now = datetime.now()\n",
                "    # timestamp = now.strftime(\"%Y%m%d_%H%M%S\") \n",
                "    ## Specify the file name\n",
                "    if solution_id:\n",
                "        log_name = f\"./Logs/activities_output_{timestamp}_solution_{solution_id}.csv.gz\"\n",
                "    else:\n",
                "        log_name = f\"./Logs/activities_output_{timestamp}.csv.gz\"\n",
                "    # Save the DataFrame to CSV\n",
                "    logs_df.to_csv(log_name, index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {
                "azdata_cell_guid": "46613e4e-764e-4ab8-b860-086d48629c7a",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "def export_solutions(activities_with_solutions, financial_year ,timestamp):\n",
                "    \n",
                "    file_name = f\"./Data_Output/activities_output_{financial_year.replace('/', '')}_AT_{timestamp}.csv\"\n",
                "    activities_with_solutions.to_csv(file_name, index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {
                "azdata_cell_guid": "8145c9ad-0ad8-470c-b0a4-5379076195c1",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "financial_year = ''\n",
                "\n",
                "def aggregate_results(df):\n",
                "      \n",
                "      solution_columns = [col for col in df.columns if 'solution_' in col and '_unit' not in col]\n",
                "\n",
                "      df_melted = df.melt(id_vars=[col for col in df.columns if col not in solution_columns],\n",
                "                        value_vars=solution_columns, \n",
                "                        var_name='SolutionColumn', \n",
                "                        value_name='Solution')\n",
                "\n",
                "      df_melted['SolutionNumber'] = df_melted['SolutionColumn'].apply(lambda x: x.split('_')[1])\n",
                "\n",
                "      df_melted['CC_Activity_Date'] = pd.to_datetime(df_melted['CC_Activity_Date'])\n",
                "      df_melted['Fin_Year'] = pd.cut(df_melted['CC_Activity_Date'], \n",
                "                                    bins=[pd.Timestamp('2018-04-01'), pd.Timestamp('2019-04-01'),\n",
                "                                          pd.Timestamp('2020-04-01'), pd.Timestamp('2021-04-01'),\n",
                "                                          pd.Timestamp('2022-04-01')],\n",
                "                                    labels=['18/19', '19/20', '20/21', '21/22'])\n",
                "\n",
                "      grouped = df_melted.groupby(['Solution', 'SolutionNumber', \n",
                "                                    'CC_Level', 'Fin_Year']).size().reset_index(name='Activity_Count')\n",
                "\n",
                "      sorted_df = grouped.sort_values(by=['SolutionNumber', 'Solution', 'CC_Level', 'Fin_Year'])\n",
                "\n",
                "      final_df = sorted_df.loc[sorted_df['Fin_Year'] == financial_year]\n",
                "\n",
                "      return final_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {
                "azdata_cell_guid": "428a94be-a748-4509-b0ff-fbf290c2b079",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "def generate_detail_string(nsga3 = nsga3, restricted_mutation = restricted_mutation\n",
                "                           , restricted_mutation_depth = restricted_mutation_depth,\n",
                "                           include_extreme_individual = include_extreme_individual,\n",
                "                           include_original_sites = include_original_sites):\n",
                "    detail_string_parts = []\n",
                "    if nsga3:\n",
                "        detail_string_parts.append(\"NSGA3\")\n",
                "    if not nsga3:\n",
                "        detail_string_parts.append(\"NSGA2\")\n",
                "    if restricted_mutation:\n",
                "        detail_string_parts.append(f\"Site_Limit_{restricted_mutation_depth}\")\n",
                "    if include_extreme_individual:\n",
                "        detail_string_parts.append(\"EI_Inc\")\n",
                "    if include_original_sites:\n",
                "        detail_string_parts.append(\"OI_Inc\")\n",
                "    detail_string_parts.append(f\"Num_Elites_{elite_pop}\")\n",
                "    detail_string = '_'.join(detail_string_parts)\n",
                "    return detail_string"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {
                "azdata_cell_guid": "c708b3c6-a55d-4573-acb2-739915072f57",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "def output_results(results,timestamp,run_detail_string):\n",
                "\n",
                "    file_parts = [\"./Data_Output/activities_output_grouped\", financial_year.replace('/', ''), f\"AT_{timestamp}\"]\n",
                "    file_parts.append(run_detail_string)\n",
                "    file_name = '_'.join(file_parts) + \".csv\"\n",
                "\n",
                "    results.to_csv(file_name, index=False)\n",
                "    \n",
                "    return print(f\"File output: {file_name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {
                "azdata_cell_guid": "9a1de160-608b-493f-990c-ac506a5717ba",
                "language": "python"
            },
            "outputs": [],
            "source": [
                "def output_map(m,timestamp,solution_number,run_detail_string):\n",
                "\n",
                "    file_parts = [\"./Data_Output/map\", financial_year.replace('/', ''), f\"AT_{timestamp}\"]\n",
                "    file_parts.append(f\"Solution_{solution_number}\")\n",
                "    file_parts.append(run_detail_string)\n",
                "    file_name = '_'.join(file_parts) + \".html\"\n",
                "\n",
                "    m.save(file_name)\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "1af6dd8c-1bdd-4e15-b151-3bd0ceea44d9"
            },
            "source": [
                "Then lets run our algorithm and evolve our solutions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {
                "azdata_cell_guid": "909ff712-a246-4d73-9d55-8abe8e01667f",
                "language": "python"
            },
            "outputs": [],
            "source": [
                " # https://hubble-live-assets.s3.eu-west-1.amazonaws.com/bapm/file_asset/file/64/LNU_doc_Nov_2018.pdf\n",
                " # LNUs over 600 advised to have dedicated tier 3 resource so ideally under\n",
                " # LNUs over 400 IC days dedicated resident Tier 2 resource \n",
                " # so surmising that the idealised tier 2 should have between 600 and 400 IC days\n",
                "scenarios = [\n",
                "            [# 1: burnley tier 2 IC Days Limit\n",
                "                 # objectives\n",
                "                {'E01024897': {'NICU': {'min': 400, 'max': 600}} # Burnley\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                []\n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]\n",
                "            ,[# 2: preston tier 2 IC Days Limit\n",
                "                 # objectives\n",
                "                {'E01025300': {'NICU': {'min': 400, 'max': 600}} # Preston\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                []\n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]\n",
                "            ,[# 3: burnley tier 1 ICU Limit\n",
                "                 # objectives\n",
                "                {'E01024897': {'NICU': {'min': 0, 'max': 400}} # Burnley\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                []\n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]\n",
                "            ,[# 4: preston tier 1 ICU Limit\n",
                "                 # objectives\n",
                "                {'E01025300': {'NICU': {'min': 0, 'max': 400}} # Preston\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                []\n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]\n",
                "            ,[# 5: blackburn added as tier 3\n",
                "                 # objectives\n",
                "                {'E01012632': {'NICU': {'min': 2000}} # Blackburn\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                ['E01012632'] # Blackburn\n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]            \n",
                "            ,[# 6: blackburn added as tier 2\n",
                "                 # objectives\n",
                "                {'E01012632': {'NICU': {'min': 400, 'max': 600}} # Blackburn\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                ['E01012632'] # Blackburn\n",
                "                ,# proposed restrictions\n",
                "                ['E01006570'] # Remove Alder Hey\n",
                "                ]\n",
                "            ,[# 7: blackburn added as tier 3 Burnley removed\n",
                "                 # objectives\n",
                "                {'E01012632': {'NICU': {'min': 2000}} # Blackburn\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                ['E01012632'] # Blackburn\n",
                "                ,# proposed restrictions\n",
                "                ['E01006570','E01024897'] # Remove Alder Hey and Burnley\n",
                "                ]\n",
                "            ,[# 8: blackburn added as tier 3 Preston Removed\n",
                "                 # objectives\n",
                "                {'E01012632': {'NICU': {'min': 2000}} # Blackburn\n",
                "                }\n",
                "                ,# proposed additions\n",
                "                ['E01012632'] # Blackburn\n",
                "                ,# proposed restrictions\n",
                "                ['E01006570','E01025300'] # Remove Alder Hey and Preston\n",
                "                ]\n",
                "]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {
                "azdata_cell_guid": "1c451954-8a4a-439c-a82f-9b379a31e69a",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Start time: 2024-01-19 22:55:00.767994\n",
                        "No data present for 2019-04-01 00:00:00 to 2020-03-31 00:00:00\n",
                        "No data present for 2020-04-01 00:00:00 to 2021-03-31 00:00:00\n",
                        "Registered restricted mutation function\n",
                        "Run as 21/22 using NSGA2 and mutation distance limit 3 and 20 Elites\n",
                        "    Solution number: 1\n",
                        "        Current site list: ['E01007251', 'E01018377', 'E01018480', 'E01006512', 'E01018616', 'E01025488', 'E01012457', 'E01006499', 'E01005062', 'E01005164', 'E01005070', 'E01006370', 'E01004880', 'E01005354', 'E01005801', 'E01005944', 'E01019155', 'E01033071', 'E01025300', 'E01024897', 'E01012722']\n",
                        "        Generation: 28, Generations Since Improvement: 10  "
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn [32], line 80\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Solution number: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msolution_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        Current site list: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_site_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 80\u001b[0m pop, log, hof, paretofront, best \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m best_index \u001b[38;5;241m=\u001b[39m pop\u001b[38;5;241m.\u001b[39mindex(best)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        Index of the best individual: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
                        "Cell \u001b[1;32mIn [31], line 588\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child1, child2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(offspring[::\u001b[38;5;241m2\u001b[39m], offspring[\u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m]):\n\u001b[0;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m<\u001b[39m cross_chance:\n\u001b[1;32m--> 588\u001b[0m         \u001b[43mtoolbox\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    589\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m child1\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m    590\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m child2\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m.\u001b[39mvalues\n",
                        "File \u001b[1;32mc:\\Users\\Duncan.Passey\\azuredatastudio-python\\lib\\site-packages\\deap\\tools\\support.py:112\u001b[0m, in \u001b[0;36mHistory.decorator.<locals>.decFunc.<locals>.wrapFunc\u001b[1;34m(*args, **kargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapFunc\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkargs):\n\u001b[0;32m    111\u001b[0m     individuals \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkargs)\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindividuals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m individuals\n",
                        "File \u001b[1;32mc:\\Users\\Duncan.Passey\\azuredatastudio-python\\lib\\site-packages\\deap\\tools\\support.py:96\u001b[0m, in \u001b[0;36mHistory.update\u001b[1;34m(self, individuals)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenealogy_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     95\u001b[0m ind\u001b[38;5;241m.\u001b[39mhistory_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenealogy_index\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenealogy_history[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenealogy_index] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenealogy_tree[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenealogy_index] \u001b[38;5;241m=\u001b[39m parent_indices\n",
                        "File \u001b[1;32mc:\\Users\\Duncan.Passey\\azuredatastudio-python\\lib\\copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
                        "File \u001b[1;32mc:\\Users\\Duncan.Passey\\azuredatastudio-python\\lib\\copy.py:288\u001b[0m, in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m listiter:\n\u001b[0;32m    287\u001b[0m         item \u001b[38;5;241m=\u001b[39m deepcopy(item, memo)\n\u001b[1;32m--> 288\u001b[0m         y\u001b[38;5;241m.\u001b[39mappend(item)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m listiter:\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "num_switches = 2\n",
                "periods = ['19/20','20/21','21/22','22/23','23/24']\n",
                "mutation_limits = [3,5]\n",
                "restricted_sites = [] # remove Alder Hey as undertaking surgical only\n",
                "include_extreme_individual = False\n",
                "elite_pop = 20\n",
                "\n",
                "# the maximum number of generations to run the evolution for\n",
                "max_number_generations = 1000\n",
                "# number of generations that the pareto front is stagnant before stopping\n",
                "stagnation_limit = 50 # stagnation before stopping\n",
                "stagnation_threshold = 10 # to increase mutaion rate after x generations\n",
                "\n",
                "start = datetime.now()\n",
                "print(f'Start time: {start}')\n",
                "\n",
                "for year in periods:\n",
                "    financial_year = year\n",
                "    start_date, end_date = get_fin_year_dates(financial_year)\n",
                "    \n",
                "    # check the dataset covers the period\n",
                "    if start_date >= startrange and end_date <= endrange:\n",
                "        activities_with_solutions = activities.loc[(activities['CC_Activity_Date'] >= start_date) & (activities['CC_Activity_Date'] <= end_date)].copy().reset_index(drop=True)\n",
                "        \n",
                "        # data_prep\n",
                "        filtered_activities, num_homes, num_sites, most_frequent_sites, home_lsoas, home_activities, home_populations = data_prep(activities, start_date, end_date)\n",
                "        \n",
                "        toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.random_site, num_homes)\n",
                "        \n",
                "        for mutation_depth, switch_tuple in itertools.product(mutation_limits, itertools.product([False, True], repeat=num_switches)):\n",
                "            restricted_mutation_depth = mutation_depth\n",
                "            include_original_sites = switch_tuple[0] \n",
                "            include_extreme_individual = switch_tuple[1] \n",
                "            \n",
                "            nearby_sites = {}\n",
                "        \n",
                "            # Check and register the appropriate mutation function based on the conditions\n",
                "            if restricted_mutation:\n",
                "                toolbox.register(\"mutate\", restricted_mutNearbyInt, indpb=individual_mutation_prob, nearby_sites=nearby_sites)\n",
                "                print(\"Registered restricted mutation function\")\n",
                "            elif weighted_mutation:\n",
                "                toolbox.register(\"mutate\", weighted_mutation_function, indpb=individual_mutation_prob, cumulative_probs = cumulative_probs)\n",
                "                print(\"Registered weighted mutation function\")\n",
                "            else:\n",
                "                toolbox.register(\"mutate\", restricted_mutUniformInt, low=0, up=num_sites-1, indpb=individual_mutation_prob)\n",
                "                \n",
                "            toolbox.decorate(\"mate\",   history.decorator)\n",
                "            toolbox.decorate(\"mutate\", history.decorator)  \n",
                "            \n",
                "            print(\n",
                "                f\"Run as {financial_year} using \"\n",
                "                f\"{'NSGA3' if nsga3 else 'NSGA2'}\"\n",
                "                f\"{' and EI included' if include_extreme_individual else ''}\"\n",
                "                f\"{' and OI included' if include_original_sites else ''}\"\n",
                "                f\"{f' and mutation distance limit {restricted_mutation_depth}' if restricted_mutation else ''}\"\n",
                "                f\"{f' and {elite_pop} Elites'}\"\n",
                "                )\n",
                "            \n",
                "            now = datetime.now()\n",
                "            timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
                "\n",
                "            for solution, objectives in enumerate(scenarios, 1):\n",
                "                solution_number = solution\n",
                "                activity_limits = objectives [0]\n",
                "                proposed_additions = objectives [1]\n",
                "                restricted_sites = objectives [2]\n",
                "                \n",
                "                num_elites = elite_pop\n",
                "                activity_focus = list()\n",
                "                \n",
                "                restricted_site_indeces = {site_codes.index(code) for code in restricted_sites}\n",
                "                # all_sites = site_codes + proposed_additions \n",
                "                current_site_list = prepare_site_list(site_codes, proposed_additions, restricted_sites)\n",
                "                nearby_sites = {home: get_nearby_sites(home) for home in home_lsoas}\n",
                "\n",
                "                history = tools.History()\n",
                "                logs_df = create_logs_df()\n",
                "                print(f\"    Solution number: {solution_number}\")\n",
                "                print(f\"        Current site list: {current_site_list}\")\n",
                "                pop, log, hof, paretofront, best = main()\n",
                "                best_index = pop.index(best)\n",
                "                print(f\"        Index of the best individual: {best_index}\")\n",
                "                print(f\"        Fitness: {pop[best_index].fitness.values}\")\n",
                "                \n",
                "                deets = generate_detail_string()\n",
                "                \n",
                "                # Use the function and store the map\n",
                "                m = plot_assignments_folium(pop[best_index])\n",
                "\n",
                "                output_map(m,timestamp,solution_number,deets)\n",
                "                \n",
                "                display(m)\n",
                "                \n",
                "                home_to_site_mapping = create_solution_list(best, home_lsoas, site_codes)\n",
                "                activities_with_solutions = add_solution(activities_with_solutions, home_to_site_mapping, solution_number, activity_focus)\n",
                "                export_log(solution_number, timestamp)\n",
                "                \n",
                "            export_solutions(activities_with_solutions, financial_year ,timestamp)\n",
                "            results = aggregate_results(activities_with_solutions)\n",
                "            output_results(results, timestamp, deets)\n",
                "    else:\n",
                "        print(f\"No data present for {start_date} to {end_date}\")      \n",
                "end = datetime.now()\n",
                "duration = end - start\n",
                "print(f'End time: {end}')\n",
                "print(f'Total duration: {duration}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "a0dc57cc-ad93-4635-b722-edfcbb814d76",
                "language": "python"
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
